{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM of Problem 1.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "import numpy.core.defchararray as npch\n",
    "\n",
    "# read characters\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "def char_text_to_ngram_text(text, ngram=2):\n",
    "    assert(ngram>=2)\n",
    "    ngram_component=[]\n",
    "    for n in range(ngram):\n",
    "        # shift n positions the original list\n",
    "        ngram_component.append(text[n::ngram])\n",
    "    ngram_list = np.asarray(list(ngram_component[0]))\n",
    "    for n in range(ngram)[1:]:\n",
    "        ngram_array = npch.add(ngram_list, \n",
    "                               np.asarray(list(ngram_component[n])))\n",
    "        ngram_list=ngram_array\n",
    "        del ngram_array\n",
    "    del ngram_component\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngrams_text size 50000000\n",
      "[' a' 'na' 'rc' 'hi' 'sm' ' o' 'ri' 'gi' 'na' 'te']\n",
      "' a'\n",
      "'na'\n",
      "'rc'\n",
      "'hi'\n",
      "'sm'\n",
      "' o'\n",
      "'ri'\n",
      "'gi'\n",
      "'na'\n",
      "'te'\n"
     ]
    }
   ],
   "source": [
    "ngrams = 2\n",
    "ngrams_text = char_text_to_ngram_text(text, ngrams)\n",
    "print('Ngrams_text size %d' % len(ngrams_text))\n",
    "print(\"{}\".format(ngrams_text[:10]))\n",
    "for k in range(10):\n",
    "    print(\"'{}'\".format(ngrams_text[k]))\n",
    "del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999500 ['on' 's ' 'an' 'ar' 'ch' 'is' 'ts' ' a' 'dv' 'oc' 'at' 'e ' 'so' 'ci' 'al'\n",
      " ' r' 'el' 'at' 'io' 'ns' ' b' 'as' 'ed' ' u' 'po' 'n ' 'vo' 'lu' 'nt' 'ar'\n",
      " 'y ' 'as' 'so' 'ci' 'at' 'io' 'n ' 'of' ' a' 'ut' 'on' 'om' 'ou' 's ' 'in'\n",
      " 'di' 'vi' 'du' 'al' 's ' 'mu' 'tu' 'al' ' a' 'id' ' a' 'nd' ' s' 'el' 'f '\n",
      " 'go' 've' 'rn' 'an']\n",
      "500 [' a' 'na' 'rc' 'hi' 'sm' ' o' 'ri' 'gi' 'na' 'te' 'd ' 'as' ' a' ' t' 'er'\n",
      " 'm ' 'of' ' a' 'bu' 'se' ' f' 'ir' 'st' ' u' 'se' 'd ' 'ag' 'ai' 'ns' 't '\n",
      " 'ea' 'rl' 'y ' 'wo' 'rk' 'in' 'g ' 'cl' 'as' 's ' 'ra' 'di' 'ca' 'ls' ' i'\n",
      " 'nc' 'lu' 'di' 'ng' ' t' 'he' ' d' 'ig' 'ge' 'rs' ' o' 'f ' 'th' 'e ' 'en'\n",
      " 'gl' 'is' 'h ' 're']\n"
     ]
    }
   ],
   "source": [
    "valid_size = 500\n",
    "valid_text = ngrams_text[:valid_size]\n",
    "train_text = ngrams_text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we construct a bigram dataset and dictionary of bigrams? (like the word dictionary in word2vec assignment...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abecedary_length = 27\n",
      "ascii_lowercase=\"abcdefghijklmnopqrstuvwxyz \"\n",
      "Last bigram is '  '\n",
      "Final Bigram list length (729)\n",
      "Sample data  [702, 351, 461, 197, 498, 716, 467, 170, 351, 517]\n",
      "Dictionary entry 'tg': 519\n",
      "Dictionary entry 'ci': 62\n",
      "Dictionary entry 'db': 82\n",
      "Dictionary entry 'rc': 461\n",
      "Dictionary entry 'kz': 295\n",
      "Dictionary entry 'nc': 353\n",
      "Dictionary entry 'kx': 293\n",
      "Dictionary entry 'iu': 236\n",
      "Dictionary entry 'xy': 645\n",
      "Dictionary entry 'bq': 43\n",
      "Dictionary entry 'oe': 382\n",
      "rev Dictionary entry 0: aa\n",
      "rev Dictionary entry 1: ab\n",
      "rev Dictionary entry 2: ac\n",
      "rev Dictionary entry 3: ad\n",
      "rev Dictionary entry 4: ae\n",
      "rev Dictionary entry 5: af\n",
      "rev Dictionary entry 6: ag\n",
      "rev Dictionary entry 7: ah\n",
      "rev Dictionary entry 8: ai\n",
      "rev Dictionary entry 9: aj\n",
      "rev Dictionary entry 10: ak\n"
     ]
    }
   ],
   "source": [
    "abecedary_length = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "print(\"abecedary_length = {}\".format(abecedary_length))\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "print(\"ascii_lowercase=\\\"{}\\\"\".format(string.ascii_lowercase+' '))\n",
    "\n",
    "bigram_list = []\n",
    "for first_char in string.ascii_lowercase+' ':\n",
    "    for second_char in string.ascii_lowercase+' ':\n",
    "        bigram_list.append(first_char+second_char)\n",
    "# print(\"Bigram list ({}) = \\n{}\".format(len(bigram_list),bigram_list))\n",
    "\n",
    "# remove bigram formed with two spaces '  '\n",
    "# bigram_list = [ x for x in bigram_list if x != '  ']\n",
    "print(\"Last bigram is '{}'\".format(bigram_list[-1]))\n",
    "print(\"Final Bigram list length ({})\".format(len(bigram_list)))\n",
    "vocabulary_size = len(bigram_list)\n",
    "\n",
    "def build_bigram_dict(bigrams_text, bigram_vocab):\n",
    "  dictionary = dict()\n",
    "  for bigram in bigram_vocab:\n",
    "    # len acts as index since it increases in each iteration\n",
    "    dictionary[bigram] = len(dictionary)\n",
    "  data_idx = list()\n",
    "  for word in bigrams_text:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    data_idx.append(index)\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data_idx, dictionary, reverse_dictionary\n",
    "\n",
    "# data_idx holds the dictionary index of each bigram in vocabulary\n",
    "# dictionary holds a list of bigrams, with their index within the dictionary\n",
    "# reverse dictionary has indices as key and bigrams as values\n",
    "data_idx, dictionary, reverse_dictionary = build_bigram_dict(ngrams_text, bigram_list)\n",
    "\n",
    "print(\"Sample data \", data_idx[:10])\n",
    "for (n, (k,v)) in enumerate(dictionary.items()):\n",
    "    print(\"Dictionary entry '{}': {}\".format(k, v))\n",
    "    if n >= 10:\n",
    "        break\n",
    "for (n, (k,v)) in enumerate(reverse_dictionary.items()):\n",
    "    print(\"rev Dictionary entry {}: {}\".format(k, v))\n",
    "    if n >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram2Vec Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['na', 'na', 'rc', 'rc', 'hi', 'hi', 'sm', 'sm']\n",
      "    labels: [' a', 'rc', 'na', 'hi', 'rc', 'sm', ' o', 'hi']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2:\n",
      "    batch: ['rc', 'rc', 'rc', 'rc', 'hi', 'hi', 'hi', 'hi']\n",
      "    labels: ['hi', 'sm', ' a', 'na', 'sm', 'na', 'rc', ' o']\n",
      "\n",
      "with num_skips = 8 and skip_window = 4:\n",
      "    batch: ['sm', 'sm', 'sm', 'sm', 'sm', 'sm', 'sm', 'sm']\n",
      "    labels: ['na', 'hi', 'gi', ' a', 'rc', 'ri', ' o', 'na']\n",
      "\n",
      "with num_skips = 2 and skip_window = 2:\n",
      "    batch: ['rc', 'rc', 'hi', 'hi', 'sm', 'sm', ' o', ' o']\n",
      "    labels: ['sm', ' a', ' o', 'na', 'hi', 'ri', 'ri', 'gi']\n"
     ]
    }
   ],
   "source": [
    "# bigram2vec batch generator\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data_idx[data_index])\n",
    "    data_index = (data_index + 1) % len(data_idx)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data_idx[data_index])\n",
    "    data_index = (data_index + 1) % len(data_idx)\n",
    "  return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data_idx[:8]])\n",
    "\n",
    "# skip_window = How many words to consider left and right.\n",
    "# num_skips = How many times to reuse an input to generate a label.\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2), (8, 4), (2, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.127357\n",
      "Nearest to ca:\n",
      "  n (0.428), iv (0.413), xv (0.375), lk (0.368), wy (0.345), of (0.343), lg (0.304), lw (0.302),\n",
      "Nearest to aj:\n",
      " yl (0.381), qc (0.368), ir (0.334), cw (0.314), hq (0.301), yh (0.291), ka (0.288), gj (0.286),\n",
      "Nearest to df:\n",
      " dj (0.363), bc (0.355), qq (0.344), nr (0.331), fn (0.311), xy (0.303), ud (0.288), q  (0.286),\n",
      "Nearest to cj:\n",
      " xk (0.478), oa (0.320), n  (0.316), er (0.316), fq (0.310), pw (0.301), nj (0.293), zp (0.279),\n",
      "Nearest to b :\n",
      " pz (0.396), pd (0.343), ju (0.343), fy (0.337), eg (0.301), cn (0.294), em (0.289), ns (0.280),\n",
      "Nearest to cv:\n",
      " ej (0.388), pk (0.340), nc (0.336), pr (0.325), mp (0.323), ru (0.323), ga (0.314), rs (0.314),\n",
      "Nearest to cr:\n",
      " cz (0.359), cy (0.355), gs (0.349), jp (0.346), ci (0.338), he (0.309), r  (0.301), rz (0.298),\n",
      "Nearest to do:\n",
      " ge (0.328), ou (0.309), na (0.307), oj (0.303), jp (0.300), ef (0.292), sl (0.291), re (0.290),\n",
      "Nearest to ae:\n",
      " kj (0.343), iy (0.337), ah (0.306), mh (0.303), o  (0.301), ke (0.283), c  (0.269), xx (0.269),\n",
      "Nearest to db:\n",
      " rm (0.443), er (0.330), pr (0.324), pi (0.324), yu (0.317), zt (0.311), nf (0.302), iz (0.297),\n",
      "Nearest to dh:\n",
      " yn (0.346), qp (0.332), cm (0.318), be (0.308), gq (0.306), ed (0.303), yk (0.300), xb (0.288),\n",
      "Nearest to cs:\n",
      " li (0.321), kg (0.318), ln (0.311), sk (0.301), ii (0.296), iq (0.288), ks (0.285), fg (0.284),\n",
      "Nearest to ah:\n",
      " oq (0.438), en (0.398), px (0.338), nt (0.328), cu (0.319), kr (0.314), ae (0.306), nq (0.292),\n",
      "Nearest to cz:\n",
      " cr (0.359), zw (0.327), cd (0.305), dp (0.301), jd (0.295), oq (0.293), y  (0.289), bh (0.287),\n",
      "Nearest to de:\n",
      " at (0.380), lz (0.299), xm (0.297), bh (0.291), ax (0.288), rm (0.282), dl (0.277), hn (0.274),\n",
      "Nearest to cy:\n",
      " wd (0.377), cr (0.355), rz (0.346), ex (0.324), nk (0.316), tm (0.310), mx (0.306), us (0.291),\n",
      "Average loss at step 2000: 1.380411\n",
      "Average loss at step 4000: 1.170608\n",
      "Average loss at step 6000: 1.144732\n",
      "Average loss at step 8000: 1.122906\n",
      "Average loss at step 10000: 1.161610\n",
      "Nearest to ca:\n",
      " xv (0.388), c  (0.386), cu (0.337), qh (0.332), wy (0.323), ga (0.321), co (0.319), ua (0.287),\n",
      "Nearest to aj:\n",
      " qc (0.389), yl (0.359), et (0.351), yh (0.338), ut (0.328), tt (0.319), to (0.291), gg (0.289),\n",
      "Nearest to df:\n",
      " qq (0.380), th (0.356), dj (0.347), xy (0.333), bc (0.320), fn (0.319), nr (0.312), q  (0.297),\n",
      "Nearest to cj:\n",
      " xk (0.478),  p (0.372), nj (0.322), ph (0.315),  g (0.312), fq (0.310), pw (0.302), bm (0.298),\n",
      "Nearest to b :\n",
      " pd (0.358), wc (0.328), t  (0.325), cn (0.321), d  (0.297), ds (0.293), pl (0.291), pz (0.283),\n",
      "Nearest to cv:\n",
      " ej (0.389), pk (0.339), mp (0.332), z  (0.328), dn (0.321), cq (0.309), oh (0.308), ef (0.307),\n",
      "Nearest to cr:\n",
      " tm (0.461), jp (0.397), tr (0.368), gr (0.339), mr (0.328), cz (0.325), fa (0.312), fe (0.310),\n",
      "Nearest to do:\n",
      " ge (0.402), to (0.398), wg (0.393), je (0.357), zt (0.341), ka (0.339), wm (0.328), ga (0.327),\n",
      "Nearest to ae:\n",
      " kj (0.324), ec (0.323), fq (0.322), zq (0.312), ww (0.301), mh (0.295), ma (0.295), qi (0.291),\n",
      "Nearest to db:\n",
      " ch (0.407), iz (0.368), hw (0.352), tw (0.343), nf (0.329), rm (0.317),  n (0.311), zt (0.310),\n",
      "Nearest to dh:\n",
      " yn (0.377), yk (0.344), qp (0.324), cm (0.318), gq (0.310), es (0.301), gh (0.290), cy (0.286),\n",
      "Nearest to cs:\n",
      " sh (0.402), ii (0.380), on (0.372), sm (0.328), ln (0.326), ng (0.325), sk (0.316), gb (0.283),\n",
      "Nearest to ah:\n",
      " oq (0.410), en (0.374), cu (0.328), kr (0.325),  u (0.323), ke (0.316), nq (0.283), px (0.281),\n",
      "Nearest to cz:\n",
      " zw (0.326), cr (0.325), io (0.321), cd (0.301), oq (0.296), dp (0.294), jd (0.291), bh (0.288),\n",
      "Nearest to de:\n",
      " lz (0.373), gm (0.350), ce (0.333), aq (0.331), nk (0.300), ge (0.298), kl (0.296), wu (0.293),\n",
      "Nearest to cy:\n",
      " tm (0.339), nk (0.336), ex (0.312), cr (0.309), hm (0.303), mx (0.295), jp (0.292), dh (0.286),\n",
      "Average loss at step 12000: 1.119393\n",
      "Average loss at step 14000: 1.049646\n",
      "Average loss at step 16000: 1.143233\n",
      "Average loss at step 18000: 1.109967\n",
      "Average loss at step 20000: 1.110361\n",
      "Nearest to ca:\n",
      " ga (0.427), cu (0.386), co (0.349), we (0.336), qh (0.326), za (0.302), wy (0.302), ha (0.284),\n",
      "Nearest to aj:\n",
      " yl (0.383), qc (0.373), ut (0.366), yh (0.354), rf (0.329), ck (0.319), gg (0.306), nn (0.300),\n",
      "Nearest to df:\n",
      " qq (0.409),  k (0.389), dj (0.362), xy (0.336), th (0.331), fn (0.317), nr (0.312), do (0.297),\n",
      "Nearest to cj:\n",
      " xk (0.478), nj (0.350), bm (0.339), fq (0.310),  p (0.306), pw (0.302), df (0.288), tf (0.280),\n",
      "Nearest to b :\n",
      " wc (0.371), cn (0.324), gi (0.322), bs (0.312), kr (0.306), bl (0.299), ju (0.293), vo (0.291),\n",
      "Nearest to cv:\n",
      " ej (0.373), pk (0.339), z  (0.325), dn (0.313), wd (0.311), af (0.305), ef (0.294), wk (0.290),\n",
      "Nearest to cr:\n",
      " tm (0.493), tr (0.413), qu (0.382), po (0.340), jp (0.333), dc (0.331), mr (0.322), fe (0.319),\n",
      "Nearest to do:\n",
      " ga (0.386), qj (0.363), je (0.362), ka (0.361), gl (0.354), jp (0.353), dy (0.345), oq (0.315),\n",
      "Nearest to ae:\n",
      " fq (0.359), iu (0.326), pa (0.323), fj (0.303), qi (0.302), ma (0.301), xf (0.298), ww (0.298),\n",
      "Nearest to db:\n",
      " ch (0.393), tw (0.386), nf (0.366), hw (0.356), ef (0.315), zt (0.309), yu (0.290), iz (0.280),\n",
      "Nearest to dh:\n",
      " yn (0.366), yk (0.327), gh (0.312), cm (0.312), cs (0.303), jy (0.298), lg (0.296), qp (0.291),\n",
      "Nearest to cs:\n",
      " ng (0.427), sk (0.412), c  (0.381), gh (0.307), ln (0.306), gb (0.305), dh (0.303), sm (0.302),\n",
      "Nearest to ah:\n",
      " ke (0.396), rq (0.368),  v (0.338), ge (0.322), gy (0.302), yo (0.300), bv (0.299), ch (0.276),\n",
      "Nearest to cz:\n",
      " io (0.334), zw (0.326), dp (0.308), oq (0.298), jd (0.292), cd (0.292), bh (0.290), jx (0.283),\n",
      "Nearest to de:\n",
      " ty (0.374), lz (0.353), ds (0.340), gm (0.294), vm (0.288), di (0.286), re (0.286), mh (0.285),\n",
      "Nearest to cy:\n",
      " nk (0.459), hm (0.398), og (0.342), ft (0.333), vg (0.316), gt (0.304), mx (0.302), tm (0.287),\n",
      "Average loss at step 22000: 1.033595\n",
      "Average loss at step 24000: 1.116355\n",
      "Average loss at step 26000: 1.071857\n",
      "Average loss at step 28000: 1.071760\n",
      "Average loss at step 30000: 1.086328\n",
      "Nearest to ca:\n",
      " cu (0.427), qa (0.333), ga (0.325), qh (0.313), ua (0.308), va (0.300), sa (0.299), c  (0.293),\n",
      "Nearest to aj:\n",
      " qc (0.364), yl (0.350), yh (0.347), cc (0.326), at (0.319), hn (0.314), ck (0.312), nn (0.310),\n",
      "Nearest to df:\n",
      " qq (0.435), xy (0.375), dj (0.361), di (0.344), fn (0.320),  k (0.312), th (0.310), zs (0.294),\n",
      "Nearest to cj:\n",
      " xk (0.478),  p (0.370), bm (0.338), fq (0.310), nj (0.305), pw (0.303), df (0.292), f  (0.291),\n",
      "Nearest to b :\n",
      " wc (0.403), ph (0.348), g  (0.319), cn (0.314), fr (0.303), pd (0.303), sp (0.298), vo (0.295),\n",
      "Nearest to cv:\n",
      " ej (0.353), z  (0.351), pk (0.336), vo (0.328), dn (0.317), ys (0.314), ef (0.307), wd (0.304),\n",
      "Nearest to cr:\n",
      " tm (0.451), tr (0.444), gr (0.423), pr (0.405), kn (0.391), qu (0.374), dr (0.368), wh (0.363),\n",
      "Nearest to do:\n",
      " je (0.461), qj (0.430), di (0.355), dy (0.347), zi (0.345), sq (0.331), ne (0.314), da (0.288),\n",
      "Nearest to ae:\n",
      " iu (0.374), fq (0.370), aa (0.310), oa (0.297), ua (0.294), ra (0.293), pa (0.286), bi (0.279),\n",
      "Nearest to db:\n",
      " hw (0.382), nf (0.380), tw (0.365), ch (0.362), tn (0.311), ef (0.310), ib (0.301), vw (0.274),\n",
      "Nearest to dh:\n",
      " yn (0.386), yk (0.326), gh (0.324), ab (0.314), cm (0.314), sm (0.313), nj (0.289), xb (0.285),\n",
      "Nearest to cs:\n",
      " sm (0.411), ng (0.375), sk (0.333), py (0.329), ln (0.328), gh (0.306), pv (0.301), gb (0.300),\n",
      "Nearest to ah:\n",
      " rq (0.374), yo (0.317), ou (0.308), eh (0.297), tv (0.290), il (0.287), ge (0.286), ee (0.284),\n",
      "Nearest to cz:\n",
      " to (0.362), io (0.310), dp (0.299), oq (0.295), zw (0.289), jx (0.284), cd (0.272), fp (0.272),\n",
      "Nearest to de:\n",
      " lz (0.362), qt (0.349), zp (0.329), je (0.308), re (0.305), xo (0.300), di (0.294), vm (0.287),\n",
      "Nearest to cy:\n",
      " nk (0.446), hm (0.406), ft (0.368), gt (0.348), mr (0.334), ay (0.307), og (0.306), mx (0.301),\n",
      "Average loss at step 32000: 1.061400\n",
      "Average loss at step 34000: 1.059491\n",
      "Average loss at step 36000: 1.085376\n",
      "Average loss at step 38000: 1.102484\n",
      "Average loss at step 40000: 1.096530\n",
      "Nearest to ca:\n",
      " ga (0.443), co (0.392), cu (0.348), qa (0.339), da (0.336), tu (0.315), ba (0.315), zg (0.310),\n",
      "Nearest to aj:\n",
      " qc (0.349), cc (0.346), to (0.337), rf (0.326), yh (0.320), ck (0.311), hn (0.308), gg (0.290),\n",
      "Nearest to df:\n",
      " qq (0.434), xy (0.368), dj (0.358), th (0.322), fn (0.320), zs (0.313), gr (0.297), lg (0.294),\n",
      "Nearest to cj:\n",
      " xk (0.478), bm (0.352), fq (0.310), pw (0.296), pp (0.296), tf (0.293), dn (0.290), df (0.288),\n",
      "Nearest to b :\n",
      " x  (0.374), wc (0.372), pd (0.358), ap (0.346), kr (0.326), wu (0.316), fr (0.312), mu (0.312),\n",
      "Nearest to cv:\n",
      " z  (0.348), ej (0.339), ef (0.334), pk (0.331), dn (0.321), ys (0.314), wd (0.305), vo (0.300),\n",
      "Nearest to cr:\n",
      " tr (0.441), tm (0.425), po (0.402), pr (0.392), gr (0.386), cl (0.377), kn (0.352), qu (0.351),\n",
      "Nearest to do:\n",
      " je (0.453), ko (0.362), sq (0.362), dy (0.355), qj (0.337), zi (0.329), wr (0.324), jp (0.310),\n",
      "Nearest to ae:\n",
      " iu (0.401), fq (0.337), bs (0.302), ua (0.298), zz (0.298), ih (0.287), ww (0.283), eu (0.282),\n",
      "Nearest to db:\n",
      " ch (0.442), hw (0.432), nf (0.379), ef (0.370), tn (0.331), tw (0.324), gl (0.302), cc (0.282),\n",
      "Nearest to dh:\n",
      " gh (0.340), ab (0.332), sm (0.332), yk (0.308), cm (0.297), ms (0.296), jy (0.294), jx (0.284),\n",
      "Nearest to cs:\n",
      " ng (0.447), sm (0.389), on (0.387), sk (0.325), ts (0.323), sh (0.322), ne (0.315), gb (0.309),\n",
      "Nearest to ah:\n",
      " ew (0.353), eh (0.337), rq (0.325), ch (0.316), ee (0.310), ou (0.307), wg (0.304), il (0.302),\n",
      "Nearest to cz:\n",
      " to (0.385), io (0.335), dp (0.292), oq (0.288), zw (0.285), jx (0.283), cd (0.274), fp (0.274),\n",
      "Nearest to de:\n",
      " ke (0.409), ty (0.393), di (0.363), qt (0.324), ds (0.319), vz (0.315), da (0.311), vg (0.309),\n",
      "Nearest to cy:\n",
      " nk (0.469), ft (0.388), og (0.337), gt (0.333), hm (0.333), ce (0.325), vg (0.325), mr (0.304),\n",
      "Average loss at step 42000: 1.041445\n",
      "Average loss at step 44000: 1.112679\n",
      "Average loss at step 46000: 1.025664\n",
      "Average loss at step 48000: 1.078906\n",
      "Average loss at step 50000: 1.069394\n",
      "Nearest to ca:\n",
      " ga (0.434), cu (0.340), qa (0.329), yw (0.326), na (0.326), co (0.323), c  (0.299), ba (0.291),\n",
      "Nearest to aj:\n",
      " hn (0.380), qc (0.371), rf (0.362), cc (0.334), at (0.333), ot (0.296), ld (0.293), ut (0.291),\n",
      "Nearest to df:\n",
      " qq (0.451), xy (0.382), dj (0.342), zs (0.321), fn (0.314), th (0.308), do (0.295), cj (0.295),\n",
      "Nearest to cj:\n",
      " xk (0.478), bm (0.356), fq (0.310), dn (0.296), df (0.295), pw (0.293), ep (0.290), tf (0.289),\n",
      "Nearest to b :\n",
      " wc (0.421), kr (0.395), gd (0.365), pd (0.348), x  (0.339), p  (0.326), bi (0.309), a  (0.309),\n",
      "Nearest to cv:\n",
      " z  (0.365), ej (0.333), dn (0.321), pk (0.321), ys (0.311), ef (0.298), wd (0.297), af (0.292),\n",
      "Nearest to cr:\n",
      " gr (0.503), tr (0.488), br (0.438), pr (0.401), kn (0.393), po (0.383), dr (0.377), pl (0.369),\n",
      "Nearest to do:\n",
      " je (0.370), ko (0.362), da (0.346), zi (0.327), wr (0.304), jp (0.298), to (0.298), df (0.295),\n",
      "Nearest to ae:\n",
      " ua (0.378), oa (0.369), iu (0.355), fq (0.347), cy (0.305), es (0.303), bs (0.301), fj (0.290),\n",
      "Nearest to db:\n",
      " hw (0.449), ef (0.400), ch (0.378), nf (0.339), cc (0.331), tn (0.326), tw (0.326),  n (0.288),\n",
      "Nearest to dh:\n",
      " ab (0.349), sm (0.344), jx (0.321), gh (0.300), lg (0.289), nj (0.285), jy (0.281), ml (0.278),\n",
      "Nearest to cs:\n",
      " ng (0.428), sh (0.368), sm (0.359), sk (0.357), on (0.350), tv (0.319), ps (0.311), ze (0.304),\n",
      "Nearest to ah:\n",
      " rq (0.332), ew (0.330), et (0.301), wg (0.301), aq (0.299), ii (0.299), ee (0.299), jh (0.288),\n",
      "Nearest to cz:\n",
      " to (0.420), io (0.346), oq (0.282), al (0.280), el (0.280), zd (0.272), tr (0.266), fp (0.266),\n",
      "Nearest to de:\n",
      " da (0.349), ty (0.343), qt (0.323), fr (0.314), ve (0.311), va (0.309), di (0.303), ge (0.303),\n",
      "Nearest to cy:\n",
      " nk (0.385), ft (0.364), ay (0.348), yu (0.331), fx (0.327), mx (0.326), ce (0.316), hm (0.311),\n",
      "Average loss at step 52000: 1.066660\n",
      "Average loss at step 54000: 1.107341\n",
      "Average loss at step 56000: 1.060713\n",
      "Average loss at step 58000: 1.014732\n",
      "Average loss at step 60000: 1.034481\n",
      "Nearest to ca:\n",
      " ga (0.377), cu (0.365), co (0.356), tu (0.313), ya (0.291), yw (0.275), ba (0.266), qa (0.251),\n",
      "Nearest to aj:\n",
      " hn (0.420), rf (0.357), qc (0.356), ot (0.342), zy (0.312), ck (0.308), cc (0.308), ke (0.300),\n",
      "Nearest to df:\n",
      " qq (0.441), xy (0.382), do (0.345), dj (0.331), th (0.327), ps (0.327), zs (0.318), fn (0.310),\n",
      "Nearest to cj:\n",
      " xk (0.478), bm (0.363), fq (0.310), df (0.309), tf (0.296), pw (0.295), ed (0.286), zp (0.282),\n",
      "Nearest to b :\n",
      " a  (0.384), wc (0.378), g  (0.373), x  (0.362), pd (0.332), v  (0.331), w  (0.325), u  (0.322),\n",
      "Nearest to cv:\n",
      " z  (0.354), ej (0.341), af (0.321), pk (0.320), k  (0.314), vo (0.311), al (0.311), ef (0.301),\n",
      "Nearest to cr:\n",
      " gr (0.483), po (0.444), tr (0.443), kn (0.427), cl (0.390), fl (0.377), dr (0.377), qu (0.374),\n",
      "Nearest to do:\n",
      " je (0.387), to (0.373), zi (0.356), df (0.345), qj (0.330), jp (0.325), wr (0.318), vq (0.314),\n",
      "Nearest to ae:\n",
      " oa (0.433), fj (0.299), mu (0.295), hm (0.294), fq (0.293), dy (0.292), ua (0.290), pi (0.282),\n",
      "Nearest to db:\n",
      " hw (0.477), ch (0.411), ef (0.379), nf (0.373), tw (0.322), tn (0.322), ye (0.312), by (0.311),\n",
      "Nearest to dh:\n",
      " ch (0.352), jy (0.307), ml (0.302), ab (0.292), sm (0.287), jx (0.286), gh (0.284), nj (0.275),\n",
      "Nearest to cs:\n",
      " ng (0.411), ps (0.360), sm (0.352), sh (0.339), ee (0.338), tv (0.336), on (0.315), gh (0.315),\n",
      "Nearest to ah:\n",
      " rq (0.364), tz (0.321), wg (0.306), ee (0.305), tn (0.292), ke (0.286), ew (0.282), yb (0.280),\n",
      "Nearest to cz:\n",
      " io (0.334), al (0.315), oq (0.280), zd (0.278), fp (0.270), jv (0.268), sl (0.268), tr (0.264),\n",
      "Nearest to de:\n",
      " qt (0.300), ke (0.300), wz (0.291), ty (0.285), vg (0.278), te (0.275), da (0.274), hu (0.267),\n",
      "Nearest to cy:\n",
      " ft (0.379), yu (0.377), ce (0.353), ay (0.348), mr (0.338), nk (0.323), vg (0.313), hm (0.309),\n",
      "Average loss at step 62000: 1.037164\n",
      "Average loss at step 64000: 1.028939\n",
      "Average loss at step 66000: 1.058927\n",
      "Average loss at step 68000: 1.113511\n",
      "Average loss at step 70000: 1.086453\n",
      "Nearest to ca:\n",
      " ga (0.452), co (0.426), cu (0.403), na (0.391), cl (0.351), xe (0.316), sa (0.291), km (0.290),\n",
      "Nearest to aj:\n",
      " hn (0.414), rf (0.350), pr (0.325), jv (0.320), qc (0.317), zy (0.310), ld (0.305), tq (0.300),\n",
      "Nearest to df:\n",
      " qq (0.443), lg (0.348), xy (0.345), dj (0.342), vh (0.314), ps (0.313), zs (0.309), cj (0.309),\n",
      "Nearest to cj:\n",
      " xk (0.478), bm (0.364), fq (0.310), df (0.309),  k (0.302), dn (0.297), pw (0.297), tf (0.296),\n",
      "Nearest to b :\n",
      " m  (0.386), pd (0.376), vo (0.376), p  (0.367), x  (0.360), yb (0.319), wu (0.318), k  (0.315),\n",
      "Nearest to cv:\n",
      " al (0.369), ej (0.341), z  (0.338), pk (0.327), af (0.317), vo (0.314), ef (0.313), ad (0.309),\n",
      "Nearest to cr:\n",
      " dr (0.475), po (0.456), gr (0.455), tr (0.449), kn (0.431), cl (0.416), pr (0.390), tm (0.370),\n",
      "Nearest to do:\n",
      " je (0.448), by (0.417), qj (0.350), da (0.343), vq (0.324), di (0.323), wr (0.315), zi (0.311),\n",
      "Nearest to ae:\n",
      " oa (0.429), fq (0.326), oh (0.318), dy (0.316), ah (0.278), ai (0.278), ua (0.274), oo (0.265),\n",
      "Nearest to db:\n",
      " hw (0.463), nf (0.377), ef (0.348), tn (0.341), tw (0.332), ye (0.312), ch (0.312), kt (0.305),\n",
      "Nearest to dh:\n",
      " ab (0.370), nb (0.315), jy (0.307), ml (0.296), ch (0.293), gh (0.289), sk (0.283), jx (0.274),\n",
      "Nearest to cs:\n",
      " ng (0.372), tv (0.365), sh (0.355), ps (0.347), gb (0.332), ze (0.331), sm (0.329), gh (0.326),\n",
      "Nearest to ah:\n",
      " yb (0.320), eh (0.318), wg (0.315), tz (0.310), kh (0.304), ew (0.299), vn (0.292), qd (0.292),\n",
      "Nearest to cz:\n",
      " io (0.357), to (0.319), oq (0.288), fp (0.277), bh (0.276), jx (0.268), mm (0.260), hd (0.258),\n",
      "Nearest to de:\n",
      " ke (0.371), di (0.352), da (0.336), qt (0.325), va (0.298), gm (0.276), vg (0.274), xu (0.270),\n",
      "Nearest to cy:\n",
      " ft (0.414), yu (0.408), tf (0.352), um (0.329), uq (0.326), td (0.317), mx (0.309), hm (0.305),\n",
      "Average loss at step 72000: 1.047321\n",
      "Average loss at step 74000: 1.067880\n",
      "Average loss at step 76000: 1.080480\n",
      "Average loss at step 78000: 1.080761\n",
      "Average loss at step 80000: 1.081021\n",
      "Nearest to ca:\n",
      " co (0.479), ga (0.418), cu (0.410), sa (0.400), na (0.365), tu (0.311), ko (0.310), yw (0.294),\n",
      "Nearest to aj:\n",
      " zy (0.386), hn (0.366), rf (0.360), k  (0.353), kl (0.347), tq (0.344), pl (0.331), cc (0.309),\n",
      "Nearest to df:\n",
      " qq (0.429), lg (0.361), ds (0.340), dj (0.333), xy (0.326), vh (0.319), zs (0.313), cj (0.312),\n",
      "Nearest to cj:\n",
      " xk (0.478), bm (0.355), ae (0.328), dn (0.320), df (0.312), fq (0.310), tf (0.302), iy (0.293),\n",
      "Nearest to b :\n",
      " x  (0.486), v  (0.406), gd (0.384), mh (0.364), jf (0.361), pd (0.352), sq (0.349), m  (0.340),\n",
      "Nearest to cv:\n",
      " af (0.387), ej (0.351), of (0.333), pk (0.326), vo (0.325), al (0.321), gp (0.302), z  (0.298),\n",
      "Nearest to cr:\n",
      " kn (0.488), gr (0.481), dr (0.451), pr (0.442), tr (0.436), cl (0.423), fr (0.374), br (0.369),\n",
      "Nearest to do:\n",
      " je (0.439), di (0.378), da (0.374), dy (0.372), to (0.359), wr (0.317), qj (0.315), by (0.311),\n",
      "Nearest to ae:\n",
      " cy (0.358), an (0.342), oa (0.329), cj (0.328), bi (0.299), ua (0.299), fq (0.289), jc (0.287),\n",
      "Nearest to db:\n",
      " hw (0.441), ch (0.434), nf (0.415), tn (0.383), ef (0.371), kt (0.353), tw (0.339), by (0.337),\n",
      "Nearest to dh:\n",
      " ch (0.365), ml (0.348), nb (0.342), vx (0.319), jy (0.317), ab (0.299), jx (0.296), sw (0.293),\n",
      "Nearest to cs:\n",
      " ng (0.398), gh (0.395), ls (0.377), ee (0.367), sh (0.353), tv (0.348), md (0.346), py (0.329),\n",
      "Nearest to ah:\n",
      " ke (0.376), dy (0.350), ew (0.340), rq (0.332), tz (0.327), jh (0.326), yz (0.322), kh (0.317),\n",
      "Nearest to cz:\n",
      " to (0.306), fp (0.305), io (0.299), oq (0.280), al (0.279), bh (0.264), hd (0.263), bn (0.262),\n",
      "Nearest to de:\n",
      " qt (0.417), da (0.358), mh (0.334), ke (0.332), rp (0.317), ds (0.314), qw (0.305), vg (0.289),\n",
      "Nearest to cy:\n",
      " yu (0.410), uq (0.365), ae (0.358), tf (0.354), ft (0.346), be (0.341), gb (0.319), zi (0.318),\n",
      "Average loss at step 82000: 1.049746\n",
      "Average loss at step 84000: 1.073968\n",
      "Average loss at step 86000: 1.084814\n",
      "Average loss at step 88000: 1.071500\n",
      "Average loss at step 90000: 1.064858\n",
      "Nearest to ca:\n",
      " cu (0.497), co (0.417), na (0.400), ga (0.363), ef (0.329), sa (0.317), qa (0.306), xe (0.303),\n",
      "Nearest to aj:\n",
      " hn (0.374), zy (0.363), rf (0.359), ld (0.342), ot (0.341), tq (0.339), ks (0.332), pr (0.331),\n",
      "Nearest to df:\n",
      " qq (0.410), lg (0.379), xy (0.337), th (0.327), st (0.327), ks (0.323), hl (0.320), dt (0.315),\n",
      "Nearest to cj:\n",
      " xk (0.479), bm (0.343), dn (0.338), ae (0.324), yi (0.311), fq (0.310), aw (0.310), tf (0.309),\n",
      "Nearest to b :\n",
      " jf (0.402), x  (0.387), pd (0.373), m  (0.371), wu (0.354), a  (0.349), j  (0.346), sq (0.346),\n",
      "Nearest to cv:\n",
      " al (0.421), of (0.350), af (0.329), pk (0.315), ej (0.314), uo (0.310), gp (0.307), z  (0.306),\n",
      "Nearest to cr:\n",
      " kn (0.500), pr (0.478), tr (0.476), gr (0.449), cl (0.436), dr (0.411), fr (0.409), po (0.400),\n",
      "Nearest to do:\n",
      " wr (0.386), je (0.383), to (0.374), da (0.356), qj (0.348), by (0.332), cx (0.329), ki (0.322),\n",
      "Nearest to ae:\n",
      " fq (0.437), sg (0.354), cj (0.324), ze (0.318), oa (0.308), jc (0.305), fj (0.284), cy (0.271),\n",
      "Nearest to db:\n",
      " hw (0.455), tn (0.405), nf (0.388), ch (0.377), kt (0.367), ef (0.337), ks (0.322), kw (0.313),\n",
      "Nearest to dh:\n",
      " ch (0.463), gh (0.345), vx (0.344), sw (0.342), sk (0.330), nb (0.327), ml (0.325), jy (0.322),\n",
      "Nearest to cs:\n",
      " ng (0.419), sh (0.411), sm (0.392), gh (0.391), tv (0.359), ee (0.355), ls (0.342), ty (0.336),\n",
      "Nearest to ah:\n",
      " yo (0.387), jh (0.380), yz (0.361), rq (0.355), dy (0.343), wg (0.324), ew (0.322), tv (0.311),\n",
      "Nearest to cz:\n",
      " to (0.362), io (0.334), el (0.308), sr (0.301), fp (0.300), zd (0.278), ht (0.277), al (0.276),\n",
      "Nearest to de:\n",
      " qt (0.396), da (0.392), di (0.363), ds (0.353), mh (0.332), qw (0.310), ke (0.300), rp (0.283),\n",
      "Nearest to cy:\n",
      " yu (0.412), ft (0.373), uq (0.363), tf (0.355), gb (0.346), eb (0.339), mx (0.337), wf (0.325),\n",
      "Average loss at step 92000: 1.095705\n",
      "Average loss at step 94000: 1.032293\n",
      "Average loss at step 96000: 1.014860\n",
      "Average loss at step 98000: 1.097113\n",
      "Average loss at step 100000: 1.031656\n",
      "Nearest to ca:\n",
      " co (0.428), cu (0.401), ga (0.365), na (0.308), ql (0.307), zg (0.305), ha (0.300), wj (0.296),\n",
      "Nearest to aj:\n",
      " hn (0.387), rf (0.383), ks (0.366), ot (0.361), tq (0.359), to (0.342), kl (0.341), zy (0.331),\n",
      "Nearest to df:\n",
      " qq (0.403), lg (0.371), ds (0.361), dt (0.336), xy (0.329), th (0.329), ny (0.321), ks (0.316),\n",
      "Nearest to cj:\n",
      " xk (0.479), bm (0.346), dn (0.333), yi (0.319), fq (0.310), sb (0.303), tf (0.301), ae (0.296),\n",
      "Nearest to b :\n",
      " m  (0.448), x  (0.403), v  (0.381), j  (0.362), w  (0.360), wu (0.354), u  (0.352), l  (0.342),\n",
      "Nearest to cv:\n",
      " ef (0.380), al (0.370), of (0.326), uo (0.319), gp (0.314), af (0.313), ej (0.311), pk (0.308),\n",
      "Nearest to cr:\n",
      " gr (0.506), pr (0.463), cl (0.438), kn (0.431), br (0.430), dr (0.413), fr (0.410), tr (0.400),\n",
      "Nearest to do:\n",
      " je (0.385), qj (0.362), by (0.350), wr (0.348), di (0.335), cx (0.333), tx (0.328), to (0.327),\n",
      "Nearest to ae:\n",
      " jc (0.338), id (0.328), fq (0.304), cj (0.296), yc (0.291), oh (0.286), yt (0.285), ut (0.281),\n",
      "Nearest to db:\n",
      " hw (0.458), ch (0.426), nf (0.406), kt (0.400), tn (0.381), kb (0.348), kw (0.335), by (0.333),\n",
      "Nearest to dh:\n",
      " ch (0.452), gh (0.379), ms (0.351), vx (0.345), pb (0.335), sw (0.328), ml (0.322), jy (0.317),\n",
      "Nearest to cs:\n",
      " ng (0.455), gh (0.399), gb (0.373), sh (0.368), tv (0.367), sm (0.366), ty (0.355), nx (0.345),\n",
      "Nearest to ah:\n",
      " rq (0.442), yo (0.421), ee (0.399), ow (0.383), jh (0.371), ye (0.357), kh (0.351), uk (0.335),\n",
      "Nearest to cz:\n",
      " br (0.348), to (0.334), io (0.328), el (0.320), sr (0.309), fp (0.301), ht (0.299), zd (0.277),\n",
      "Nearest to de:\n",
      " qt (0.410), di (0.371), ds (0.320), mh (0.319), ke (0.306), gc (0.305), dy (0.294), da (0.289),\n",
      "Nearest to cy:\n",
      " yu (0.397), mx (0.350), tf (0.344), ft (0.343), uq (0.336), gb (0.332), vg (0.327), mr (0.326),\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# embedding vector size\n",
    "embedding_size = 64 # Dimension of the embedding vector.\n",
    "num_sampled = 32 # Number of negative examples to sample.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "  # This is because the embeddings are defined as a variable quantity and the\n",
    "  # optimizer's `minimize` method will by default modify all variable quantities \n",
    "  # that contribute to the tensor it is passed.\n",
    "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "\n",
    "num_steps = 100001\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch(\n",
    "      batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step, average_loss))\n",
    "      average_loss = 0\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:\\n' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          cos_dist = sim[i, nearest[k]]\n",
    "          log = '%s %s (%0.3f),' % (log, close_word, cos_dist)\n",
    "        print(log)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the LSTM model:\n",
    "- Batches should consist of a list of consecutive bigrams.\n",
    "- Can we generate them by adapting the batch generation scheme in LSTM Problem 1. Using indices instead of one-hot encodings.\n",
    "- Later, indices will be inputs for a lookup embedding tables in the LSTM cell input.\n",
    "- We are using now a text of bigrams. So, if we access a single position of train_text, we get a bigram, NOT a character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'lleria arches national', 'married urraca princes', 'y and liturgical langu', 'tion from the national', 'new york other well kn', 'e listed with a gloss ', 'o be made to recognize', 'ore significant than i', ' two six eight in sign', 'ity can be lost as in ', 'tion of the size of th', 'f certain drugs confus', 'e convince the priest ', 'ampaign and barred att', 'ious texts such as eso', 'a duplicate of the ori', 'ine january eight marc', 'cal theories classical', ' dimensional analysis ', 't s support or at leas', 'e oscillating system e', 'of italy languages the', 'klahoma press one nine', 'ws becomes the first d', 'the fabian society neh', ' sharman networks shar', 'ting in political init', 'th risky riskerdoo ric', 'fense the air componen', 'treet grid centerline ', 'appeal of devotional b']\n",
      " --> len=32\n",
      "['ate social relations b', 'al park photographic v', 'ess of castile daughte', 'guage among jews manda', 'al media and from pres', 'known manufacturers of', 's covering some of the', 'ze single acts of meri', ' in jersey and guernse', 'gns of humanity vol th', 'n denaturalization and', 'the input usually meas', 'usion inability to ori', 't of the mistakes of a', 'ttempts by his opponen', 'soteric christianity a', 'riginal document fax m', 'rch eight listing of a', 'al mechanics and speci', 's fundamental applicat', 'ast not parliament s o', ' example rlc circuit f', 'he official language o', 'ne three two one one t', ' daily college newspap', 'ehru wished the econom', 'arman s sydney based b', 'itiatives the lesotho ', 'icky ricardo this clas', 'ent of arm is represen', 'e external links bbc o', ' buddhism especially r']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "# number of bigrams\n",
    "batch_size=32\n",
    "# numbre of connected LSTM units\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size #floor division (integer division)\n",
    "    # so, is segment the number of total batches that fits into the data text?\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # there are batch_size cursor positions, but separated segment positions between them? \n",
    "    # Why?? Because it is large enough?\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      # batch of bigrams\n",
    "      bigram = self._text[self._cursor[b]]\n",
    "      batch[b] = dictionary[bigram]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    # batch shape is (b,)\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def id2gram(id):\n",
    "    return reverse_dictionary[id]\n",
    "\n",
    "def ngrams(indices):\n",
    "  \"\"\"Turn a batch of bigram indices into bigram representation.\"\"\"\n",
    "  # dimensions of input = probabilities.shape[0] (which actually is batch_size) X 1 \n",
    "  # dimensions of output = probabilities.shape[0] (which actually is batch_size)\n",
    "  return [id2gram(c) for c in indices]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0] # batch_size\n",
    "  for b in batches: # a list of length = _num_unrollings + 1 (exta one is last from previous)\n",
    "    s = [''.join(x) for x in zip(s, ngrams(b))]\n",
    "    # so s is a list of batch_size string elements of length _num_unrollings + 1\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "one_batch = batches2string(train_batches.next())\n",
    "print(\"{}\\n --> len={}\".format(one_batch, len(one_batch)))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt LSTM cell graph to use embeddings of bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 32, num_nodes = 64, embedding_size = 64\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "outputs_len = 10\n",
      "outputs shape = (320, 64)\n",
      "labels shape = (320,)\n",
      "---\n",
      "labels shape = (320, 1)\n",
      "vocabulary_size = 729\n",
      "w shape = (64, 729)\n",
      "b shape = (729,)\n",
      "inputs shape = (320, 64)\n",
      "train_preds size = (320, 729)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64 # needs to be equal to batch_size?\n",
    "flag_singlemult = True\n",
    "print(\"batch_size = {}, num_nodes = {}, embedding_size = {}\".format(\n",
    "    batch_size, num_nodes, embedding_size))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  # ix ~ U, input weights [embed, n], and input_size is \n",
    "  # im ~ W, recurrent weights [n, n]\n",
    "  # ib ~ b, biases [1, n] ¿Does it  match with U and W during running?\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases. # size2 = num_nodes or embed?\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Simplify the expression by using a single matrix multiply for each, \n",
    "  #  and variables that are 4 times larger.\n",
    "  def lstm_cell_singlemult(i, o, state):\n",
    "    # i: input [b]\n",
    "    # embed: [b, embed]\n",
    "    # o: output of previous cell [n, n]\n",
    "    # Look up embeddings for inputs. [b, embed]\n",
    "    embed = tf.nn.embedding_lookup(final_embeddings, i)\n",
    "    # Pack weights into a single variable that is 4 times larger\n",
    "    inp_weights = tf.concat([ix, fx, ox, cx], 1) # [embed, 4*n]\n",
    "    out_weights = tf.concat([im, fm, om, cm], 1)\n",
    "    # perform simple mult\n",
    "    single_mult = tf.matmul(embed, inp_weights) + tf.matmul(o, out_weights)\n",
    "    # select appropriate result for each gate\n",
    "    input_gate = tf.sigmoid(single_mult[:,:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(single_mult[:,1*num_nodes:2*num_nodes] + fb)\n",
    "    output_gate = tf.sigmoid(single_mult[:,2*num_nodes:3*num_nodes] + ob)\n",
    "    update = single_mult[:,3*num_nodes:] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings] #  get from 0 to num_unrollings-1, leave last one out\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_singlemult(i, output, state)\n",
    "    print(\"output.shape = {}\".format(output.shape))\n",
    "    outputs.append(output)\n",
    "  print(\"outputs_len = {}\".format(len(outputs)))\n",
    "\n",
    "  # State saving across unrollings, and also throughout steps?\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # output.concat [b*unrollings,n] 320,64\n",
    "    print(\"outputs shape = {}\".format(tf.concat(outputs, 0).shape))\n",
    "    # w [n,emb] 64,64\n",
    "    # b [emb] 64\n",
    "    #logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    # labels.concat [b*unrollings,1] 320\n",
    "    # logits [b*unrollings,emb] 320,64\n",
    "    #print(\"logits shape = {}\".format(logits.shape))\n",
    "    concat_train_labels = tf.concat(train_labels, 0)\n",
    "    print(\"labels shape = {}\".format(concat_train_labels.shape))\n",
    "    concat_train_labels = tf.reshape(concat_train_labels,[-1,1])\n",
    "    print(\"---\")\n",
    "    print(\"labels shape = {}\".format(concat_train_labels.shape))\n",
    "    print(\"vocabulary_size = {}\".format(vocabulary_size))\n",
    "    print(\"w shape = {}\".format(w.shape))\n",
    "    print(\"b shape = {}\".format(b.shape))\n",
    "    print(\"inputs shape = {}\".format(tf.concat(outputs, 0).shape))\n",
    "    #loss = tf.reduce_mean(\n",
    "    #    tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), \n",
    "    #                                            logits=logits))\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=tf.transpose(w), biases=b, inputs=tf.concat(outputs, 0),\n",
    "                                   labels=concat_train_labels, num_sampled=128,\n",
    "                                    num_classes=vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # needed to clip gradients\n",
    "  # In previous assignments we used minimize(). This method simply combines calls \n",
    "  # compute_gradients() and apply_gradients(). If you want to process the gradient before \n",
    "  # applying them call compute_gradients() and apply_gradients() explicitly \n",
    "  # instead of using the minimize() function.\n",
    "  # \n",
    "  # zip() in conjunction with the * operator can be used to unzip a list:\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  # need the list of (gradient, variable) pairs unzipped in order to process the gradients only\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  print(\"train_preds size = {}\".format(train_prediction.shape))\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # tf.group: Create an op that groups multiple operations. When this op finishes, all \n",
    "  # ops in inputs have finished. This op has no output.\n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_singlemult(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LSTM net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:['at'].\n",
      "example:['le'].\n"
     ]
    }
   ],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  # which values can labels have? # labels will be inputs shifted by one time step.\n",
    "  # predictions size is [b*unrollings, embed]\n",
    "  # labels size should be batch_size x embed size??\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  # dimensions of input = probabilities.shape[0] (which actually is batch_size) X vocabulary_size \n",
    "  # dimensions of output = probabilities.shape[0] (which actually is batch_size)\n",
    "  return [id2gram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def sample2(prediction):\n",
    "  \"\"\"Turn a (column) prediction into their closest index.\"\"\"\n",
    "  # what is prediction size? 1 x vocabulary_size\n",
    "  # python slicing: prediction[0] select first dimension from a (1,v) array, resulting in (v,)\n",
    "  sim = np.matmul(prediction, np.transpose(final_embeddings))\n",
    "  nearest = (-sim[:]).argsort()\n",
    "  # sample an index of most similar embed\n",
    "  return nearest[0]\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  # what is prediction size? 1 x vocabulary_size\n",
    "  # python slicing: prediction[0] select first dimension from a (1,27) array, resulting in (27,)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution(distr_size=64):\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, distr_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "print(\"example:{}.\".format(characters(sample(random_distribution()))))\n",
    "print(\"example:{}.\".format(characters(sample(random_distribution(729)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 4.580547 learning rate: 10.000000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    #print(batches)\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], \n",
    "                                        feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      # labels size [b*unrollings, 1]\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      embed_labels = tf.nn.embedding_lookup(final_embeddings, labels)\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, embed_labels))))\n",
    "        \n",
    "      # The perplexity of whatever you're evaluating, on the data you're evaluating it on, \n",
    "      # sort of tells you \"this thing is right about as often as an x-sided die would be.\"\n",
    "      # Computers can predict letters pretty well - a perplexity of about 3.4.\n",
    "      # - like having a \"3.4\"-sided die predict each subsequent letter.\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples, every 10*summary_frequency steps\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          # sample() need to return an index within the dataset\n",
    "          feed = sample(random_distribution()) # random distr shape is [1, v]\n",
    "          sentence = characters(feed[0])\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            # erickrfonseca in Udacity Forums Feb '16\n",
    "            # \"The original code uses the sample function to allow some variability in the result. \n",
    "            # Without it, as you see, it becomes completely deterministic, as the LSTM learned to \n",
    "            # score the sequence \"of the states\" very high.\"\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            # characters returns a list of a single element since feed is 1x27\n",
    "            sentence += characters(feed[0])\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "      # Measure validation set perplexity, every summary_frequency steps\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        \n",
    "      print('Validation set perplexity: %.2f\\n' % float(np.exp(valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gurus3",
   "language": "python",
   "name": "gurus3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
