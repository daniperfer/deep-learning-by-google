{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM of Problem 1.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n",
      "Ngrams_text size 50000000\n",
      "[' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi', 'na', 'te']\n",
      "' a'\n",
      "'na'\n",
      "'rc'\n",
      "'hi'\n",
      "'sm'\n",
      "' o'\n",
      "'ri'\n",
      "'gi'\n",
      "'na'\n",
      "'te'\n"
     ]
    }
   ],
   "source": [
    "import numpy.core.defchararray as npch\n",
    "\n",
    "# read characters\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "def char_text_to_ngram_text(text, ngram=2):\n",
    "    assert(ngram>=2)\n",
    "    ngram_component=[]\n",
    "    for n in range(ngram):\n",
    "        # shift n positions the original list\n",
    "        ngram_component.append(text[n::ngram])\n",
    "    ngram_list = np.asarray(list(ngram_component[0]))\n",
    "    for n in range(ngram)[1:]:\n",
    "        ngram_array = npch.add(ngram_list, \n",
    "                               np.asarray(list(ngram_component[n])))\n",
    "        ngram_list=ngram_array\n",
    "        del ngram_array\n",
    "    del ngram_component\n",
    "    return ngram_list.tolist()\n",
    "\n",
    "ngrams = 2\n",
    "ngrams_text = char_text_to_ngram_text(text, ngrams)\n",
    "print('Ngrams_text size %d' % len(ngrams_text))\n",
    "print(\"{}\".format(ngrams_text[:10]))\n",
    "for k in range(10):\n",
    "    print(\"'{}'\".format(ngrams_text[k]))\n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999500 ['on', 's ', 'an', 'ar', 'ch', 'is', 'ts', ' a', 'dv', 'oc', 'at', 'e ', 'so', 'ci', 'al', ' r', 'el', 'at', 'io', 'ns', ' b', 'as', 'ed', ' u', 'po', 'n ', 'vo', 'lu', 'nt', 'ar', 'y ', 'as', 'so', 'ci', 'at', 'io', 'n ', 'of', ' a', 'ut', 'on', 'om', 'ou', 's ', 'in', 'di', 'vi', 'du', 'al', 's ', 'mu', 'tu', 'al', ' a', 'id', ' a', 'nd', ' s', 'el', 'f ', 'go', 've', 'rn', 'an']\n",
      "500 [' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi', 'na', 'te', 'd ', 'as', ' a', ' t', 'er', 'm ', 'of', ' a', 'bu', 'se', ' f', 'ir', 'st', ' u', 'se', 'd ', 'ag', 'ai', 'ns', 't ', 'ea', 'rl', 'y ', 'wo', 'rk', 'in', 'g ', 'cl', 'as', 's ', 'ra', 'di', 'ca', 'ls', ' i', 'nc', 'lu', 'di', 'ng', ' t', 'he', ' d', 'ig', 'ge', 'rs', ' o', 'f ', 'th', 'e ', 'en', 'gl', 'is', 'h ', 're']\n"
     ]
    }
   ],
   "source": [
    "valid_size = 500\n",
    "valid_text = ngrams_text[:valid_size]\n",
    "train_text = ngrams_text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we construct a bigram dataset and dictionary of bigrams? (like the word dictionary in word2vec assignment...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abecedary_length = 27\n",
      "ascii_lowercase=\"abcdefghijklmnopqrstuvwxyz \"\n",
      "Last bigram is '  '\n",
      "Final Bigram list length (729)\n",
      "Sample data  [702, 351, 461, 197, 498, 716, 467, 170, 351, 517]\n",
      "Dictionary entry 'bd': 30\n",
      "Dictionary entry 'wd': 597\n",
      "Dictionary entry 'bn': 40\n",
      "Dictionary entry 'sj': 495\n",
      "Dictionary entry 'xz': 646\n",
      "Dictionary entry 'fb': 136\n",
      "Dictionary entry 'in': 229\n",
      "Dictionary entry 'oz': 403\n",
      "Dictionary entry 'fg': 141\n",
      "Dictionary entry 'fl': 146\n",
      "Dictionary entry 'tk': 523\n",
      "rev Dictionary entry 0: aa\n",
      "rev Dictionary entry 1: ab\n",
      "rev Dictionary entry 2: ac\n",
      "rev Dictionary entry 3: ad\n",
      "rev Dictionary entry 4: ae\n",
      "rev Dictionary entry 5: af\n",
      "rev Dictionary entry 6: ag\n",
      "rev Dictionary entry 7: ah\n",
      "rev Dictionary entry 8: ai\n",
      "rev Dictionary entry 9: aj\n",
      "rev Dictionary entry 10: ak\n"
     ]
    }
   ],
   "source": [
    "abecedary_length = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "print(\"abecedary_length = {}\".format(abecedary_length))\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "print(\"ascii_lowercase=\\\"{}\\\"\".format(string.ascii_lowercase+' '))\n",
    "\n",
    "bigram_list = []\n",
    "for first_char in string.ascii_lowercase+' ':\n",
    "    for second_char in string.ascii_lowercase+' ':\n",
    "        bigram_list.append(first_char+second_char)\n",
    "# print(\"Bigram list ({}) = \\n{}\".format(len(bigram_list),bigram_list))\n",
    "\n",
    "# remove bigram formed with two spaces '  '\n",
    "# bigram_list = [ x for x in bigram_list if x != '  ']\n",
    "print(\"Last bigram is '{}'\".format(bigram_list[-1]))\n",
    "print(\"Final Bigram list length ({})\".format(len(bigram_list)))\n",
    "vocabulary_size = len(bigram_list)\n",
    "\n",
    "def build_bigram_dict(bigrams_text, bigram_vocab):\n",
    "  dictionary = dict()\n",
    "  for bigram in bigram_vocab:\n",
    "    # len acts as index since it increases in each iteration\n",
    "    dictionary[bigram] = len(dictionary)\n",
    "  data_idx = list()\n",
    "  for word in bigrams_text:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    data_idx.append(index)\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data_idx, dictionary, reverse_dictionary\n",
    "\n",
    "# data_idx holds the dictionary index of each bigram in vocabulary\n",
    "# dictionary holds a list of bigrams, with their index within the dictionary\n",
    "# reverse dictionary has indices as key and bigrams as values\n",
    "data_idx, dictionary, reverse_dictionary = build_bigram_dict(ngrams_text, bigram_list)\n",
    "\n",
    "print(\"Sample data \", data_idx[:10])\n",
    "for (n, (k,v)) in enumerate(dictionary.items()):\n",
    "    print(\"Dictionary entry '{}': {}\".format(k, v))\n",
    "    if n >= 10:\n",
    "        break\n",
    "for (n, (k,v)) in enumerate(reverse_dictionary.items()):\n",
    "    print(\"rev Dictionary entry {}: {}\".format(k, v))\n",
    "    if n >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram2Vec Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['na', 'na', 'rc', 'rc', 'hi', 'hi', 'sm', 'sm']\n",
      "    labels: [' a', 'rc', 'na', 'hi', 'sm', 'rc', 'hi', ' o']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2:\n",
      "    batch: ['rc', 'rc', 'rc', 'rc', 'hi', 'hi', 'hi', 'hi']\n",
      "    labels: ['na', 'hi', ' a', 'sm', 'na', ' o', 'sm', 'rc']\n",
      "\n",
      "with num_skips = 8 and skip_window = 4:\n",
      "    batch: ['sm', 'sm', 'sm', 'sm', 'sm', 'sm', 'sm', 'sm']\n",
      "    labels: [' o', ' a', 'ri', 'na', 'hi', 'na', 'rc', 'gi']\n",
      "\n",
      "with num_skips = 2 and skip_window = 2:\n",
      "    batch: ['rc', 'rc', 'hi', 'hi', 'sm', 'sm', ' o', ' o']\n",
      "    labels: [' a', 'sm', 'rc', 'na', 'ri', ' o', 'sm', 'gi']\n"
     ]
    }
   ],
   "source": [
    "# bigram2vec batch generator\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data_idx[data_index])\n",
    "    data_index = (data_index + 1) % len(data_idx)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data_idx[data_index])\n",
    "    data_index = (data_index + 1) % len(data_idx)\n",
    "  return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data_idx[:8]])\n",
    "\n",
    "# skip_window = How many words to consider left and right.\n",
    "# num_skips = How many times to reuse an input to generate a label.\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2), (8, 4), (2, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 2.806894\n",
      "Nearest to ch:\n",
      " he (0.433), fz (0.385), tz (0.368), kk (0.365), lw (0.331), mn (0.325), jq (0.311), es (0.307),\n",
      "Nearest to bw:\n",
      " vp (0.373), an (0.362), ec (0.334), hv (0.322), tz (0.310), ky (0.297), rz (0.296), il (0.278),\n",
      "Nearest to cy:\n",
      " ag (0.336), vs (0.326), mj (0.326), fn (0.317), bv (0.302), vl (0.301), dk (0.294), yu (0.277),\n",
      "Nearest to bi:\n",
      " oa (0.381), vi (0.350), kw (0.344), so (0.303), db (0.295), fl (0.289), fy (0.284), rx (0.277),\n",
      "Nearest to cr:\n",
      " dl (0.458), ob (0.356), sf (0.336), ww (0.330), tc (0.327), zl (0.319),  m (0.318), uc (0.312),\n",
      "Nearest to do:\n",
      " cb (0.389), sw (0.351), rc (0.331), ck (0.329), a  (0.327), qz (0.315), ze (0.311), nh (0.309),\n",
      "Nearest to bl:\n",
      " pa (0.337), fi (0.336), qi (0.332), zx (0.301), dh (0.296), xv (0.295), xy (0.293), de (0.277),\n",
      "Nearest to cu:\n",
      " e  (0.391), nz (0.385), iv (0.358), dk (0.329), fw (0.317), wl (0.313), zn (0.312), yw (0.302),\n",
      "Nearest to bf:\n",
      " iv (0.339), yp (0.312), vp (0.306), b  (0.304), do (0.288), ny (0.287), ei (0.283), pr (0.283),\n",
      "Nearest to bg:\n",
      " wh (0.372), ld (0.328), mc (0.308), dy (0.307), lr (0.299), ep (0.292), gx (0.291), bs (0.289),\n",
      "Nearest to bs:\n",
      " ec (0.391), n  (0.359),  b (0.358), fr (0.329), on (0.324), kd (0.320), ws (0.315), ux (0.295),\n",
      "Nearest to cw:\n",
      " ia (0.379), yu (0.369), ls (0.364), op (0.363),    (0.360), ad (0.346), vr (0.344), dv (0.315),\n",
      "Nearest to aw:\n",
      " ta (0.440), rf (0.389), pd (0.364), uu (0.360), cd (0.325), ad (0.310), wx (0.300), yk (0.297),\n",
      "Nearest to be:\n",
      " de (0.398), ij (0.358), mg (0.349), vn (0.313), fl (0.304),  x (0.302), pj (0.295), f  (0.294),\n",
      "Nearest to cs:\n",
      " xs (0.380), zi (0.360), cd (0.356), gl (0.345), ta (0.305), ah (0.301), wx (0.297), pl (0.293),\n",
      "Nearest to bj:\n",
      " ky (0.378), bq (0.357), za (0.343), oo (0.311), yp (0.300), go (0.286), qc (0.285), au (0.281),\n",
      "Average loss at step 2000: 1.379178\n",
      "Average loss at step 4000: 1.167224\n",
      "Average loss at step 6000: 1.143395\n",
      "Average loss at step 8000: 1.128878\n",
      "Average loss at step 10000: 1.166371\n",
      "Nearest to ch:\n",
      " wk (0.372), pn (0.354), lw (0.353), ts (0.328), cq (0.323), fz (0.317), gy (0.312), bp (0.310),\n",
      "Nearest to bw:\n",
      " vp (0.364), ec (0.325), tz (0.325), xa (0.323), le (0.317), hv (0.314), ky (0.297), rz (0.290),\n",
      "Nearest to cy:\n",
      " ay (0.372), bv (0.358), ew (0.357), mj (0.333), fn (0.318), xl (0.285), vs (0.284), lf (0.283),\n",
      "Nearest to bi:\n",
      " pa (0.389), ni (0.350), wu (0.330), be (0.330), pc (0.304), bu (0.298), li (0.296), yo (0.267),\n",
      "Nearest to cr:\n",
      " gr (0.460), dl (0.379), wp (0.375), sf (0.348), tv (0.347), tr (0.339), lv (0.331), wh (0.313),\n",
      "Nearest to do:\n",
      " cb (0.425), sw (0.412), wo (0.384), ze (0.378), nh (0.363), iq (0.314), sa (0.306), rc (0.306),\n",
      "Nearest to bl:\n",
      " qw (0.381), qi (0.370), wb (0.331), tl (0.326), dh (0.317), zx (0.289), sr (0.284), fi (0.278),\n",
      "Nearest to cu:\n",
      " tt (0.322), ku (0.315), zq (0.311), dk (0.297), zs (0.284), ru (0.277), vo (0.274), wt (0.273),\n",
      "Nearest to bf:\n",
      " ov (0.332), vp (0.330), iv (0.304), az (0.283), rz (0.273), ny (0.271), rn (0.266), ei (0.265),\n",
      "Nearest to bg:\n",
      " dy (0.332), fr (0.329), dr (0.325), mc (0.308), lr (0.306), bs (0.295), gx (0.288),  l (0.283),\n",
      "Nearest to bs:\n",
      " fr (0.365), ys (0.363), hz (0.322), ux (0.314), lk (0.310), vf (0.307), dv (0.305), kd (0.299),\n",
      "Nearest to cw:\n",
      " ps (0.412), yu (0.384), ia (0.367),    (0.360), vr (0.346), dv (0.337), ls (0.336), qz (0.309),\n",
      "Nearest to aw:\n",
      " uu (0.375), sh (0.329), yk (0.324), pd (0.318), wx (0.285), wj (0.279), an (0.279), by (0.277),\n",
      "Nearest to be:\n",
      " je (0.349), md (0.334), bi (0.330), mg (0.300), sj (0.295), jx (0.274), zi (0.273), dr (0.262),\n",
      "Nearest to cs:\n",
      " le (0.325), xs (0.304), ng (0.303), sm (0.299), ah (0.291), az (0.291), ga (0.290), ta (0.283),\n",
      "Nearest to bj:\n",
      " ey (0.428), ky (0.410), oo (0.379), bq (0.354), ii (0.311), jh (0.300), au (0.296), mz (0.291),\n",
      "Average loss at step 12000: 1.128297\n",
      "Average loss at step 14000: 1.047880\n",
      "Average loss at step 16000: 1.145832\n",
      "Average loss at step 18000: 1.107885\n",
      "Average loss at step 20000: 1.107704\n",
      "Nearest to ch:\n",
      " tk (0.405), pn (0.377), lw (0.349), of (0.336), gc (0.332), cm (0.332), vn (0.329), sl (0.318),\n",
      "Nearest to bw:\n",
      " vp (0.367), xa (0.357), pi (0.319), ec (0.314), ke (0.303), hv (0.303), cu (0.286), tz (0.285),\n",
      "Nearest to cy:\n",
      " ud (0.371), de (0.348), ew (0.343), bv (0.339), ft (0.338), ce (0.303), fy (0.299), ay (0.299),\n",
      "Nearest to bi:\n",
      " pa (0.390), mu (0.350), ni (0.348), fu (0.312), wu (0.307), bu (0.306), po (0.303), pu (0.292),\n",
      "Nearest to cr:\n",
      " gr (0.423), tr (0.399), tv (0.394), wp (0.359), qu (0.349), wr (0.328), dl (0.316), sf (0.315),\n",
      "Nearest to do:\n",
      " cb (0.404), no (0.403), di (0.344), to (0.334), dm (0.309), ki (0.300), ga (0.299), eu (0.296),\n",
      "Nearest to bl:\n",
      " qw (0.372), tl (0.350), mt (0.325), qi (0.321), wb (0.321), kf (0.302), nc (0.297), sr (0.294),\n",
      "Nearest to cu:\n",
      " ku (0.364), ca (0.357), gj (0.347), zq (0.334), ho (0.324), dk (0.303), jr (0.290), fb (0.289),\n",
      "Nearest to bf:\n",
      " vp (0.338), nh (0.318), az (0.300), iv (0.299), ov (0.297), vg (0.282), rz (0.273), ak (0.265),\n",
      "Nearest to bg:\n",
      " dr (0.347), fr (0.328), dy (0.299), lr (0.299), gx (0.287), mc (0.282), bs (0.272), ua (0.265),\n",
      "Nearest to bs:\n",
      " hz (0.364), ys (0.327), ec (0.317), vf (0.316), rf (0.316), gx (0.294), ws (0.287), ff (0.280),\n",
      "Nearest to cw:\n",
      " yu (0.393), ps (0.386), oy (0.362),    (0.354), vr (0.352), dv (0.327), ib (0.325), ik (0.324),\n",
      "Nearest to aw:\n",
      " cl (0.398), uu (0.338), pd (0.337), wx (0.331), v  (0.322), yk (0.318), an (0.314), ej (0.300),\n",
      "Nearest to be:\n",
      " bu (0.364), md (0.358), je (0.338), se (0.321), vy (0.313), lc (0.311), jx (0.302), by (0.289),\n",
      "Nearest to cs:\n",
      " gh (0.345), le (0.345), ng (0.345), ta (0.334), ty (0.306), ly (0.305), ds (0.295), ah (0.275),\n",
      "Nearest to bj:\n",
      " ky (0.463), ey (0.390), oo (0.367), bq (0.359), mz (0.321), ii (0.315), sk (0.314), au (0.304),\n",
      "Average loss at step 22000: 1.048683\n",
      "Average loss at step 24000: 1.110447\n",
      "Average loss at step 26000: 1.050425\n",
      "Average loss at step 28000: 1.067357\n",
      "Average loss at step 30000: 1.094884\n",
      "Nearest to ch:\n",
      " st (0.395), sl (0.365), pn (0.361), sm (0.353), jf (0.353), lw (0.339), fp (0.339), mm (0.338),\n",
      "Nearest to bw:\n",
      " vp (0.390), xa (0.356), ec (0.348), rz (0.341), cu (0.322), pi (0.306), ke (0.293), hv (0.288),\n",
      "Nearest to cy:\n",
      " ft (0.415), ay (0.392), ud (0.382), bv (0.330), xt (0.323), fy (0.317), ew (0.316), j  (0.301),\n",
      "Nearest to bi:\n",
      " mu (0.409), be (0.371), fu (0.369), pa (0.347), bo (0.341), mi (0.333), ba (0.321), jv (0.321),\n",
      "Nearest to cr:\n",
      " gr (0.530), tr (0.422), wp (0.412), pr (0.396), tv (0.364), wh (0.364), qu (0.363), su (0.358),\n",
      "Nearest to do:\n",
      " no (0.410), cb (0.400), py (0.341), ne (0.338), je (0.337), iq (0.316), ro (0.312), di (0.309),\n",
      "Nearest to bl:\n",
      " mt (0.362), wb (0.326), qw (0.320), tl (0.320), qi (0.317), kf (0.315), rr (0.308), nk (0.305),\n",
      "Nearest to cu:\n",
      " ku (0.394), gj (0.391), ca (0.344), zq (0.328), bw (0.322), fb (0.317), gm (0.312), ba (0.306),\n",
      "Nearest to bf:\n",
      " vp (0.345), nh (0.337), az (0.316), on (0.292), ak (0.288), rz (0.288), ic (0.282), vg (0.275),\n",
      "Nearest to bg:\n",
      " fr (0.334), dr (0.316),  k (0.304), lr (0.293), mc (0.287), gx (0.287), ua (0.280), bs (0.277),\n",
      "Nearest to bs:\n",
      " ns (0.366), mr (0.345), hz (0.336), gx (0.308), nv (0.297), ys (0.294), kq (0.294), nj (0.290),\n",
      "Nearest to cw:\n",
      " ps (0.405), yu (0.393), ph (0.371), vr (0.353), oy (0.347),    (0.347), dv (0.342), ik (0.333),\n",
      "Nearest to aw:\n",
      " yk (0.327), pd (0.315), by (0.309), wx (0.309), uu (0.307), ly (0.303), ax (0.303), cl (0.302),\n",
      "Nearest to be:\n",
      " bi (0.371), md (0.345), bu (0.321), jx (0.310), pt (0.294), sj (0.291), je (0.274), ye (0.268),\n",
      "Nearest to cs:\n",
      " ng (0.405), sm (0.401), ty (0.397), ds (0.358), c  (0.350), gh (0.350), ny (0.304), le (0.303),\n",
      "Nearest to bj:\n",
      " ky (0.457), ey (0.333), ii (0.329), bq (0.327), mz (0.326), sk (0.317), ql (0.310), td (0.289),\n",
      "Average loss at step 32000: 1.065397\n",
      "Average loss at step 34000: 1.061768\n",
      "Average loss at step 36000: 1.098016\n",
      "Average loss at step 38000: 1.088828\n",
      "Average loss at step 40000: 1.094032\n",
      "Nearest to ch:\n",
      " tm (0.415), st (0.390), sm (0.375), ty (0.368), pp (0.362), gc (0.359), pn (0.354), db (0.352),\n",
      "Nearest to bw:\n",
      " vp (0.394), xa (0.350), rz (0.339), p  (0.334), ec (0.333), ir (0.293), tn (0.292), hv (0.284),\n",
      "Nearest to cy:\n",
      " ft (0.417), de (0.378), bv (0.359), xt (0.352), ce (0.330), wn (0.328), dv (0.325), ud (0.324),\n",
      "Nearest to bi:\n",
      " ba (0.407), mi (0.403), mu (0.397), bu (0.363), bo (0.347), be (0.324), ka (0.314), pp (0.313),\n",
      "Nearest to cr:\n",
      " gr (0.473), pr (0.459), tr (0.447), wh (0.390), su (0.383), wp (0.378), fl (0.362), lt (0.356),\n",
      "Nearest to do:\n",
      " no (0.417), p  (0.374), ro (0.374), za (0.355), iq (0.325), hq (0.306), ki (0.306), jo (0.302),\n",
      "Nearest to bl:\n",
      " tl (0.359), kf (0.343), mt (0.335), rr (0.328), qi (0.328), wb (0.326), qw (0.308), gl (0.304),\n",
      "Nearest to cu:\n",
      " ku (0.412), ba (0.357), sa (0.356), zq (0.354), gj (0.324), gm (0.316), fb (0.311), ip (0.308),\n",
      "Nearest to bf:\n",
      " vp (0.349), nh (0.325), az (0.315), rz (0.299), on (0.292), ew (0.278), dq (0.272), cx (0.264),\n",
      "Nearest to bg:\n",
      " fr (0.331), dr (0.304), bs (0.298), lr (0.289), gx (0.286), mc (0.275),  k (0.260), nj (0.256),\n",
      "Nearest to bs:\n",
      " gx (0.343), mr (0.325), hz (0.310), ns (0.306), qj (0.306), ff (0.300), bg (0.298), rf (0.288),\n",
      "Nearest to cw:\n",
      " ps (0.423), yu (0.389), ph (0.380), oy (0.371), nu (0.358), vr (0.358),    (0.347), dv (0.342),\n",
      "Nearest to aw:\n",
      " v  (0.365), of (0.357), qc (0.305), by (0.305), pl (0.302), ik (0.300), yk (0.297), oo (0.291),\n",
      "Nearest to be:\n",
      " bu (0.363), bi (0.324), jx (0.320), md (0.298), sj (0.296), p  (0.285), jo (0.283), ba (0.283),\n",
      "Nearest to cs:\n",
      " ty (0.407), ng (0.391), gh (0.379), sm (0.366), ds (0.338), c  (0.314), yh (0.304), le (0.304),\n",
      "Nearest to bj:\n",
      " ky (0.451), to (0.348), mz (0.337), ql (0.328), ii (0.319), bq (0.311), bc (0.300), lp (0.298),\n",
      "Average loss at step 42000: 1.053924\n",
      "Average loss at step 44000: 1.114208\n",
      "Average loss at step 46000: 1.025110\n",
      "Average loss at step 48000: 1.084788\n",
      "Average loss at step 50000: 1.073387\n",
      "Nearest to ch:\n",
      " sm (0.391), tm (0.381), pn (0.366), pp (0.366), db (0.348), lw (0.337), gc (0.337), st (0.335),\n",
      "Nearest to bw:\n",
      " vp (0.395), xa (0.325), rz (0.325), p  (0.319), ir (0.301), cu (0.280), qi (0.280), ky (0.278),\n",
      "Nearest to cy:\n",
      " ft (0.441), ce (0.346), xt (0.345), wn (0.325), de (0.315), bv (0.314), ew (0.303), ey (0.295),\n",
      "Nearest to bi:\n",
      " mu (0.460), ki (0.422), mi (0.401), bu (0.401), ni (0.359), ba (0.354), wa (0.330), go (0.320),\n",
      "Nearest to cr:\n",
      " gr (0.512), tr (0.424), pr (0.393), mr (0.366), wr (0.355), fl (0.353), wh (0.350), su (0.346),\n",
      "Nearest to do:\n",
      " py (0.381), no (0.362), ro (0.345), ki (0.335), cb (0.331), yi (0.318), zi (0.315), ij (0.301),\n",
      "Nearest to bl:\n",
      " kf (0.345), mt (0.344), nn (0.340), gl (0.336), tl (0.332), wb (0.314), tt (0.314), qw (0.313),\n",
      "Nearest to cu:\n",
      " ku (0.441), gj (0.351), sa (0.349), ca (0.335), zq (0.329), kw (0.327), jr (0.324), dk (0.302),\n",
      "Nearest to bf:\n",
      " vp (0.348), nh (0.300), ew (0.293), az (0.292), on (0.290), rz (0.285), ak (0.274), dq (0.273),\n",
      "Nearest to bg:\n",
      " dr (0.306), lr (0.286), gx (0.284),  k (0.283), bs (0.280), ua (0.277), pr (0.276), mc (0.270),\n",
      "Nearest to bs:\n",
      " gx (0.369), mr (0.335), qj (0.327), hz (0.320), rf (0.317), rb (0.305), ob (0.302), ux (0.302),\n",
      "Nearest to cw:\n",
      " ps (0.419), oy (0.384), yu (0.384), vr (0.368), ph (0.355), oe (0.354), dv (0.349),    (0.339),\n",
      "Nearest to aw:\n",
      " lf (0.389), ax (0.358), ms (0.357), ew (0.326), ik (0.309), my (0.303), qc (0.297), vx (0.290),\n",
      "Nearest to be:\n",
      " bu (0.439), md (0.361), jx (0.344), ba (0.317), p  (0.309), by (0.291), du (0.290), ou (0.284),\n",
      "Nearest to cs:\n",
      " ng (0.404), gh (0.377), c  (0.355), ds (0.323), gs (0.320), fy (0.317), sm (0.315), ty (0.314),\n",
      "Nearest to bj:\n",
      " ky (0.440), to (0.366), ka (0.360), ql (0.335), mz (0.333), bq (0.302), ey (0.292), bd (0.290),\n",
      "Average loss at step 52000: 1.074686\n",
      "Average loss at step 54000: 1.100997\n",
      "Average loss at step 56000: 1.069882\n",
      "Average loss at step 58000: 1.026702\n",
      "Average loss at step 60000: 1.052335\n",
      "Nearest to ch:\n",
      " pp (0.388), ke (0.375), tm (0.351), ts (0.347), tz (0.345), st (0.344), pn (0.338), sm (0.337),\n",
      "Nearest to bw:\n",
      " vp (0.397), p  (0.354), cu (0.346), xa (0.315), ec (0.312), qi (0.294), pi (0.289), ir (0.282),\n",
      "Nearest to cy:\n",
      " ft (0.420), de (0.364), xt (0.361), ud (0.355), wn (0.350), ce (0.344), pm (0.322), be (0.322),\n",
      "Nearest to bi:\n",
      " bu (0.390), mu (0.380), ki (0.379), ba (0.372), ni (0.344), mi (0.316), bo (0.308), br (0.306),\n",
      "Nearest to cr:\n",
      " tr (0.465), gr (0.453), fl (0.396), pr (0.390), wr (0.365), cb (0.335), su (0.332), wh (0.331),\n",
      "Nearest to do:\n",
      " cb (0.364), py (0.351), dm (0.342), zi (0.335), kb (0.324), to (0.314), je (0.303), vb (0.290),\n",
      "Nearest to bl:\n",
      " mt (0.342), rg (0.337), nc (0.336), wb (0.329), cq (0.329), kf (0.320), sk (0.312), dg (0.295),\n",
      "Nearest to cu:\n",
      " ku (0.403), dk (0.367), ca (0.361), fb (0.347), bw (0.346), gj (0.327), zq (0.323), jr (0.307),\n",
      "Nearest to bf:\n",
      " vp (0.359), ak (0.331), nh (0.322), az (0.312), on (0.300), dq (0.273), rz (0.268), cx (0.267),\n",
      "Nearest to bg:\n",
      "  k (0.312), i  (0.309), fr (0.296), gx (0.285), bs (0.271), lr (0.268), pr (0.266), dr (0.254),\n",
      "Nearest to bs:\n",
      " gx (0.371), mr (0.345), qj (0.344), ph (0.342), rf (0.338), ff (0.326), ek (0.320), hz (0.306),\n",
      "Nearest to cw:\n",
      " yu (0.400), vr (0.378), oy (0.377), dv (0.345), ps (0.342),    (0.337), nu (0.330), eo (0.324),\n",
      "Nearest to aw:\n",
      " lf (0.340), uk (0.330), vx (0.318), qc (0.312), az (0.310), ow (0.306), ik (0.306), ax (0.297),\n",
      "Nearest to be:\n",
      " bu (0.368), jx (0.364), p  (0.349), md (0.326), cy (0.322), ba (0.314), pm (0.294), ok (0.288),\n",
      "Nearest to cs:\n",
      " ng (0.397), gh (0.362), gs (0.356), sh (0.352), ds (0.352), ga (0.338), ty (0.333), sm (0.312),\n",
      "Nearest to bj:\n",
      " ky (0.427), ql (0.338), mz (0.335), to (0.324), ka (0.319), bc (0.302), bq (0.302), ea (0.299),\n",
      "Average loss at step 62000: 1.046141\n",
      "Average loss at step 64000: 1.033558\n",
      "Average loss at step 66000: 1.068211\n",
      "Average loss at step 68000: 1.108423\n",
      "Average loss at step 70000: 1.086732\n",
      "Nearest to ch:\n",
      " tm (0.430), pp (0.374), sw (0.374), sm (0.369), oz (0.366), kl (0.365), gc (0.336), bm (0.334),\n",
      "Nearest to bw:\n",
      " vp (0.394), cu (0.394), p  (0.353), xa (0.308), bl (0.304), qi (0.300), ps (0.293), tn (0.292),\n",
      "Nearest to cy:\n",
      " ft (0.415), pm (0.396), xt (0.358), um (0.339), gi (0.338), jr (0.332), bv (0.323), be (0.318),\n",
      "Nearest to bi:\n",
      " mu (0.384), ba (0.383), zi (0.365), bu (0.330), ki (0.328), ni (0.327), is (0.327), xf (0.322),\n",
      "Nearest to cr:\n",
      " tr (0.499), gr (0.457), fl (0.405), pr (0.388), wr (0.344), kn (0.341), dr (0.341), cb (0.339),\n",
      "Nearest to do:\n",
      " je (0.409), zi (0.393), da (0.361), ii (0.346), cb (0.331), by (0.331), dm (0.325), to (0.311),\n",
      "Nearest to bl:\n",
      " kf (0.386), cq (0.371), mt (0.355), sn (0.343), nc (0.341), wb (0.339), cl (0.328), ke (0.313),\n",
      "Nearest to cu:\n",
      " ku (0.432), ca (0.430), dk (0.418), gj (0.398), bw (0.394), fb (0.377), sa (0.339), ba (0.327),\n",
      "Nearest to bf:\n",
      " vp (0.360), nh (0.330), az (0.323), ak (0.311), v  (0.279), dq (0.276), on (0.275), hs (0.267),\n",
      "Nearest to bg:\n",
      "  k (0.303), va (0.291), gx (0.284), fr (0.280), li (0.269), dv (0.266), ek (0.265), mw (0.262),\n",
      "Nearest to bs:\n",
      " gx (0.372), ms (0.361), qj (0.343), ft (0.334), bm (0.332), ff (0.304), mr (0.296), ns (0.288),\n",
      "Nearest to cw:\n",
      " yu (0.393), ps (0.387), vr (0.374), oy (0.374), on (0.344),    (0.337), dv (0.333), eo (0.322),\n",
      "Nearest to aw:\n",
      " ax (0.401), uk (0.374), my (0.362), ik (0.353), gf (0.334), lh (0.317), kh (0.314), tb (0.313),\n",
      "Nearest to be:\n",
      " bu (0.404), pm (0.329), by (0.322), cy (0.318), p  (0.317), ye (0.316), jx (0.315), ba (0.301),\n",
      "Nearest to cs:\n",
      " sh (0.385), ty (0.374), hy (0.362), ds (0.361), ng (0.358), gh (0.348), sm (0.322), c  (0.320),\n",
      "Nearest to bj:\n",
      " ky (0.409), mz (0.347), ql (0.325), bc (0.321), bq (0.319), to (0.319), pg (0.318), py (0.318),\n",
      "Average loss at step 72000: 1.058523\n",
      "Average loss at step 74000: 1.063135\n",
      "Average loss at step 76000: 1.086483\n",
      "Average loss at step 78000: 1.055174\n",
      "Average loss at step 80000: 1.093150\n",
      "Nearest to ch:\n",
      " tm (0.448), ds (0.383), db (0.380), ps (0.363), tk (0.363), pp (0.361), ty (0.358), kw (0.357),\n",
      "Nearest to bw:\n",
      " vp (0.405), cu (0.401), xa (0.321), pi (0.315), bl (0.313), tn (0.300), qi (0.299), p  (0.298),\n",
      "Nearest to cy:\n",
      " pm (0.354), ft (0.351), be (0.347), wn (0.343), jr (0.335), au (0.323), yu (0.316), xt (0.310),\n",
      "Nearest to bi:\n",
      " mu (0.380), u  (0.367), sb (0.332), bu (0.331), xf (0.321), zi (0.314), bo (0.306), ni (0.305),\n",
      "Nearest to cr:\n",
      " gr (0.488), tr (0.486), pr (0.485), kn (0.416), fl (0.395), wp (0.392), cl (0.379), wr (0.378),\n",
      "Nearest to do:\n",
      " ml (0.372), fj (0.370), cb (0.360), je (0.330), jo (0.319), ki (0.318), q  (0.317), di (0.316),\n",
      "Nearest to bl:\n",
      " kf (0.385), cq (0.373), wb (0.367), qi (0.348), gl (0.342), nk (0.335), rg (0.320), ss (0.319),\n",
      "Nearest to cu:\n",
      " ku (0.408), bw (0.401), ca (0.401), fb (0.386), gj (0.381), gu (0.356), tu (0.353), dk (0.334),\n",
      "Nearest to bf:\n",
      " vp (0.372), ak (0.346), on (0.332), nh (0.314), az (0.299), dq (0.282), ux (0.264),  u (0.261),\n",
      "Nearest to bg:\n",
      " dv (0.290), gx (0.284),  k (0.280), fr (0.271), vo (0.266), mw (0.264), bs (0.264), va (0.256),\n",
      "Nearest to bs:\n",
      " gx (0.383), bm (0.349), qj (0.331), mr (0.324), ft (0.322), ms (0.315), rf (0.314), cc (0.294),\n",
      "Nearest to cw:\n",
      " yu (0.402), ps (0.387), vr (0.379), oy (0.360), dv (0.346), nu (0.339), eo (0.336), kh (0.324),\n",
      "Nearest to aw:\n",
      " my (0.439), uk (0.409), ik (0.400), ax (0.373), lf (0.363), lh (0.326), tb (0.316), gf (0.315),\n",
      "Nearest to be:\n",
      " bu (0.395), p  (0.387), jx (0.384), cy (0.347), md (0.338), pm (0.310), xh (0.303), pg (0.288),\n",
      "Nearest to cs:\n",
      " gh (0.416), ng (0.407), sh (0.365), gs (0.360), c  (0.357), ds (0.351), lk (0.350), hy (0.341),\n",
      "Nearest to bj:\n",
      " ky (0.398), mz (0.350), py (0.346), ea (0.340), bc (0.330), ql (0.327), pg (0.316), ay (0.310),\n",
      "Average loss at step 82000: 1.036208\n",
      "Average loss at step 84000: 1.074302\n",
      "Average loss at step 86000: 1.089026\n",
      "Average loss at step 88000: 1.070525\n",
      "Average loss at step 90000: 1.077947\n",
      "Nearest to ch:\n",
      " gh (0.406), dh (0.386), sm (0.379), pn (0.372), bm (0.364), bp (0.363), tm (0.357), sr (0.355),\n",
      "Nearest to bw:\n",
      " vp (0.411), cu (0.336), ps (0.334), p  (0.332), qi (0.329), xa (0.318), bl (0.308), tn (0.302),\n",
      "Nearest to cy:\n",
      " wn (0.392), ft (0.388), pm (0.371), gi (0.358), xt (0.355), be (0.347), jr (0.328), ts (0.324),\n",
      "Nearest to bi:\n",
      " zi (0.441), mu (0.363), ni (0.360), sb (0.353), mi (0.349), ka (0.323), ba (0.318), da (0.317),\n",
      "Nearest to cr:\n",
      " pr (0.517), tr (0.478), fl (0.439), gr (0.431), kn (0.421), cl (0.402), po (0.361), wp (0.360),\n",
      "Nearest to do:\n",
      " ki (0.407), zi (0.379), cb (0.379), jo (0.367), da (0.363), di (0.332), ml (0.328), je (0.321),\n",
      "Nearest to bl:\n",
      " kf (0.352), cq (0.338), gl (0.334), sn (0.330), vh (0.321), gr (0.308), ke (0.308), bw (0.308),\n",
      "Nearest to cu:\n",
      " ca (0.477), ku (0.390), fb (0.349), bw (0.336), c  (0.335), ba (0.329), dk (0.316), xe (0.306),\n",
      "Nearest to bf:\n",
      " az (0.363), vp (0.345), on (0.342), ak (0.306), cx (0.285), ux (0.265), dq (0.259), rz (0.241),\n",
      "Nearest to bg:\n",
      " vo (0.306), dv (0.295), va (0.295), fr (0.286), pp (0.276), gx (0.269), bs (0.267),  k (0.265),\n",
      "Nearest to bs:\n",
      " ek (0.361), gx (0.357), ms (0.343), bm (0.337), ft (0.331), dv (0.318), qj (0.308), mn (0.302),\n",
      "Nearest to cw:\n",
      " yu (0.416), vr (0.378), ia (0.351), ik (0.345), eo (0.342), ps (0.337), on (0.327), dv (0.316),\n",
      "Nearest to aw:\n",
      " my (0.438), uk (0.434), ax (0.366), ik (0.365), tb (0.359), kh (0.350), lf (0.329), gf (0.317),\n",
      "Nearest to be:\n",
      " pm (0.380), ye (0.353), cy (0.347), jx (0.343), bu (0.331), gi (0.319), ue (0.307), md (0.302),\n",
      "Nearest to cs:\n",
      " ng (0.456), gh (0.422), c  (0.389), sh (0.380), gs (0.358), ds (0.336), sm (0.330), lk (0.310),\n",
      "Nearest to bj:\n",
      " bc (0.357), ky (0.337), mz (0.336), ii (0.329), pg (0.324), lt (0.323), py (0.323), ql (0.317),\n",
      "Average loss at step 92000: 1.106961\n",
      "Average loss at step 94000: 1.024209\n",
      "Average loss at step 96000: 1.028178\n",
      "Average loss at step 98000: 1.105550\n",
      "Average loss at step 100000: 1.032830\n",
      "Nearest to ch:\n",
      " bm (0.408), tm (0.400), sm (0.376), dh (0.366), pp (0.364), pn (0.356), ty (0.354), db (0.351),\n",
      "Nearest to bw:\n",
      " vp (0.402), cu (0.350), qi (0.335), p  (0.312), qd (0.306), bl (0.299), hv (0.296), pi (0.293),\n",
      "Nearest to cy:\n",
      " wn (0.393), ft (0.389), ub (0.386), ds (0.384), pm (0.376), yl (0.345), dv (0.334), vd (0.334),\n",
      "Nearest to bi:\n",
      " mu (0.400), zi (0.374), bu (0.345), wa (0.328), ba (0.328), lo (0.323), ku (0.314), bo (0.309),\n",
      "Nearest to cr:\n",
      " pr (0.463), tr (0.452), gr (0.445), cl (0.443), fl (0.443), kn (0.395), wh (0.368), pl (0.362),\n",
      "Nearest to do:\n",
      " jo (0.398), cb (0.355), je (0.347), tv (0.313), ki (0.311), ro (0.306), di (0.303), hq (0.299),\n",
      "Nearest to bl:\n",
      " kf (0.359), cq (0.351), mt (0.328), nk (0.324), fl (0.319), gl (0.315), cr (0.310), bw (0.299),\n",
      "Nearest to cu:\n",
      " ba (0.393), ku (0.377), ca (0.373), fb (0.351), bw (0.350), hv (0.337), xe (0.328), sa (0.313),\n",
      "Nearest to bf:\n",
      " vp (0.346), az (0.343), ux (0.296), on (0.294), cx (0.287), ak (0.283), ew (0.266), dq (0.258),\n",
      "Nearest to bg:\n",
      " va (0.340), vo (0.339), fr (0.321), mw (0.308), dv (0.307), ek (0.300), pp (0.299),  k (0.277),\n",
      "Nearest to bs:\n",
      " ek (0.439), mr (0.414), bm (0.378), bd (0.373), mn (0.358), gx (0.331), bj (0.325), ns (0.311),\n",
      "Nearest to cw:\n",
      " yu (0.414), vr (0.386), ps (0.364), eo (0.345), km (0.337), nu (0.327), ik (0.325), on (0.320),\n",
      "Nearest to aw:\n",
      " uk (0.451), ik (0.408), my (0.401), ax (0.353), tb (0.350), lf (0.335),  m (0.333), ah (0.333),\n",
      "Nearest to be:\n",
      " ye (0.394), ba (0.373), bu (0.364), jx (0.350), pm (0.340), cy (0.322), oc (0.313), pg (0.305),\n",
      "Nearest to cs:\n",
      " ng (0.477), gh (0.431), sh (0.400), ty (0.380), gs (0.368), ds (0.341), lk (0.330), fy (0.320),\n",
      "Nearest to bj:\n",
      " bc (0.398), ii (0.349), mz (0.340), ks (0.339), hs (0.338), ky (0.329), bs (0.325), pg (0.323),\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# embedding vector size\n",
    "embedding_size = 64 # Dimension of the embedding vector.\n",
    "num_sampled = 32 # Number of negative examples to sample.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "  # This is because the embeddings are defined as a variable quantity and the\n",
    "  # optimizer's `minimize` method will by default modify all variable quantities \n",
    "  # that contribute to the tensor it is passed.\n",
    "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "\n",
    "num_steps = 100001\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch(\n",
    "      batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step, average_loss))\n",
    "      average_loss = 0\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:\\n' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          cos_dist = sim[i, nearest[k]]\n",
    "          log = '%s %s (%0.3f),' % (log, close_word, cos_dist)\n",
    "        print(log)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the LSTM model:\n",
    "- Batches should consist of a list of consecutive bigrams.\n",
    "- Can we generate them by adapting the batch generation scheme in LSTM Problem 1. Using indices instead of one-hot encodings.\n",
    "- Later, indices will be inputs for a lookup embedding tables in the LSTM cell input.\n",
    "- We are using now a text of bigrams. So, if we access a single position of train_text, we get a bigram, NOT a character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'lleria arches national', 'married urraca princes', 'y and liturgical langu', 'tion from the national', 'new york other well kn', 'e listed with a gloss ', 'o be made to recognize', 'ore significant than i', ' two six eight in sign', 'ity can be lost as in ', 'tion of the size of th', 'f certain drugs confus', 'e convince the priest ', 'ampaign and barred att', 'ious texts such as eso', 'a duplicate of the ori', 'ine january eight marc', 'cal theories classical', ' dimensional analysis ', 't s support or at leas', 'e oscillating system e', 'of italy languages the', 'klahoma press one nine', 'ws becomes the first d', 'the fabian society neh', ' sharman networks shar', 'ting in political init', 'th risky riskerdoo ric', 'fense the air componen', 'treet grid centerline ', 'appeal of devotional b']\n",
      " --> len=32\n",
      "['ate social relations b', 'al park photographic v', 'ess of castile daughte', 'guage among jews manda', 'al media and from pres', 'known manufacturers of', 's covering some of the', 'ze single acts of meri', ' in jersey and guernse', 'gns of humanity vol th', 'n denaturalization and', 'the input usually meas', 'usion inability to ori', 't of the mistakes of a', 'ttempts by his opponen', 'soteric christianity a', 'riginal document fax m', 'rch eight listing of a', 'al mechanics and speci', 's fundamental applicat', 'ast not parliament s o', ' example rlc circuit f', 'he official language o', 'ne three two one one t', ' daily college newspap', 'ehru wished the econom', 'arman s sydney based b', 'itiatives the lesotho ', 'icky ricardo this clas', 'ent of arm is represen', 'e external links bbc o', ' buddhism especially r']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "# number of bigrams\n",
    "batch_size=32\n",
    "# numbre of connected LSTM units\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size #floor division (integer division)\n",
    "    # so, is segment the number of total batches that fits into the data text?\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # there are batch_size cursor positions, but separated segment positions between them? \n",
    "    # Why?? Because it is large enough?\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      # batch of bigrams\n",
    "      bigram = self._text[self._cursor[b]]\n",
    "      batch[b] = dictionary[bigram]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    # batch shape is (b,)\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def id2gram(id):\n",
    "    return reverse_dictionary[id]\n",
    "\n",
    "def ngrams(indices):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  # dimensions of input = probabilities.shape[0] (which actually is batch_size) X 1 \n",
    "  # dimensions of output = probabilities.shape[0] (which actually is batch_size)\n",
    "  return [id2gram(c) for c in indices]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0] # batch_size\n",
    "  for b in batches: # a list of length = _num_unrollings + 1 (exta one is last from previous)\n",
    "    s = [''.join(x) for x in zip(s, ngrams(b))]\n",
    "    # so s is a list of batch_size string elements of length _num_unrollings + 1\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "one_batch = batches2string(train_batches.next())\n",
    "print(\"{}\\n --> len={}\".format(one_batch, len(one_batch)))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt LSTM cell to use embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "outputs_len = 10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 1 for 'sampled_softmax_loss/LogUniformCandidateSampler' (op: 'LogUniformCandidateSampler') with input shapes: [320].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 1 for 'sampled_softmax_loss/LogUniformCandidateSampler' (op: 'LogUniformCandidateSampler') with input shapes: [320].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f88f5cce9e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     tf.nn.sampled_softmax_loss(weights=w, biases=b, inputs=tf.concat(outputs, 0),\n\u001b[1;32m     93\u001b[0m                                \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sampled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_sampled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                                 num_classes=vocabulary_size, partition_strategy=\"div\"))\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36msampled_softmax_loss\u001b[0;34m(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name)\u001b[0m\n\u001b[1;32m   1254\u001b[0m       \u001b[0mremove_accidental_hits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_accidental_hits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m       \u001b[0mpartition_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1257\u001b[0m   sampled_losses = nn_ops.softmax_cross_entropy_with_logits(\n\u001b[1;32m   1258\u001b[0m       labels=labels, logits=logits)\n",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36m_compute_sampled_logits\u001b[0;34m(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, subtract_log_q, remove_accidental_hits, partition_strategy, name)\u001b[0m\n\u001b[1;32m    960\u001b[0m           \u001b[0mnum_sampled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_sampled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m           \u001b[0munique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m           range_max=num_classes)\n\u001b[0m\u001b[1;32m    963\u001b[0m     \u001b[0;31m# NOTE: pylint cannot tell that 'sampled_values' is a sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;31m# pylint: disable=unpacking-non-sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/ops/candidate_sampling_ops.py\u001b[0m in \u001b[0;36mlog_uniform_candidate_sampler\u001b[0;34m(true_classes, num_true, num_sampled, unique, range_max, seed, name)\u001b[0m\n\u001b[1;32m    136\u001b[0m   return gen_candidate_sampling_ops._log_uniform_candidate_sampler(\n\u001b[1;32m    137\u001b[0m       \u001b[0mtrue_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m       seed2=seed2, name=name)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/ops/gen_candidate_sampling_ops.py\u001b[0m in \u001b[0;36m_log_uniform_candidate_sampler\u001b[0;34m(true_classes, num_true, num_sampled, unique, range_max, seed, seed2, name)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;34m\"LogUniformCandidateSampler\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mnum_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sampled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_sampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         range_max=range_max, seed=seed, seed2=seed2, name=name)\n\u001b[0m\u001b[1;32m    491\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus3/lib/python3.4/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 1 for 'sampled_softmax_loss/LogUniformCandidateSampler' (op: 'LogUniformCandidateSampler') with input shapes: [320]."
     ]
    }
   ],
   "source": [
    "num_nodes = 64 # needs to be equal to batch_size?\n",
    "flag_singlemult = True\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  # ix ~ U, input weights [v, n], and input_size is \n",
    "  # im ~ W, recurrent weights [n, n]\n",
    "  # ib ~ b, biases [1, n] Does it  match with U and W during running?\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, embedding_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([embedding_size]))\n",
    "  \n",
    "  # Simplify the expression by using a single matrix multiply for each, \n",
    "  #  and variables that are 4 times larger.\n",
    "  def lstm_cell_singlemult(i, o, state):\n",
    "    # i: input [b, v]\n",
    "    # o: output of previous cell [n, n]\n",
    "    # Look up embeddings for inputs. [b, embed]\n",
    "    embed = tf.nn.embedding_lookup(embeddings, i) #placeholder?\n",
    "    # Pack weights into a single variable that is 4 times larger\n",
    "    inp_weights = tf.concat([ix, fx, ox, cx], 1)\n",
    "    out_weights = tf.concat([im, fm, om, cm], 1)\n",
    "    # perform simple mult\n",
    "    single_mult = tf.matmul(embed, inp_weights) + tf.matmul(o, out_weights)\n",
    "    # select appropriate result for each gate\n",
    "    input_gate = tf.sigmoid(single_mult[:,:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(single_mult[:,1*num_nodes:2*num_nodes] + fb)\n",
    "    output_gate = tf.sigmoid(single_mult[:,2*num_nodes:3*num_nodes] + ob)\n",
    "    update = single_mult[:,3*num_nodes:] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings] #  get from 0 to num_unrollings-1, leave last one out\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_singlemult(i, output, state)\n",
    "    print(\"output.shape = {}\".format(output.shape))\n",
    "    outputs.append(output)\n",
    "  print(\"outputs_len = {}\".format(len(outputs)))\n",
    "\n",
    "  # State saving across unrollings, and also throughout steps?\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # output.concat [b*unrollings,n] 320,64\n",
    "    # w [n,emb] 64,64\n",
    "    # b [emb] 64\n",
    "    #logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    # labels.concat [b*unrollings,1] 320\n",
    "    # logits [b*unrollings,emb] 320,64\n",
    "    #loss = tf.reduce_mean(\n",
    "    #  tf.nn.softmax_cross_entropy_with_logits(\n",
    "    #    labels=tf.concat(train_labels, 0), logits=logits))\n",
    "    # ? Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=w, biases=b, inputs=tf.concat(outputs, 0),\n",
    "                               labels=tf.concat(train_labels, 0), num_sampled=num_sampled, \n",
    "                                num_classes=vocabulary_size, partition_strategy=\"div\"))\n",
    "\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # needed to clip gradients\n",
    "  # In previous assignments we used minimize(). This method simply combines calls \n",
    "  # compute_gradients() and apply_gradients(). If you want to process the gradient before \n",
    "  # applying them call compute_gradients() and apply_gradients() explicitly \n",
    "  # instead of using the minimize() function.\n",
    "  # \n",
    "  # zip() in conjunction with the * operator can be used to unzip a list:\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  # need the list of (gradient, variable) pairs unzipped in order to process the gradients only\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # tf.group: Create an op that groups multiple operations. When this op finishes, all \n",
    "  # ops in inputs have finished. This op has no output.\n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_singlemult(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gurus3",
   "language": "python",
   "name": "gurus3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
