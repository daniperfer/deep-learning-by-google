{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM of Problem 1.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "import numpy.core.defchararray as npch\n",
    "\n",
    "# read characters\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "def char_text_to_ngram_text(text, ngram=2):\n",
    "    assert(ngram>=2)\n",
    "    ngram_component=[]\n",
    "    for n in range(ngram):\n",
    "        # shift n positions the original list\n",
    "        ngram_component.append(text[n::ngram])\n",
    "    ngram_list = np.asarray(list(ngram_component[0]))\n",
    "    for n in range(ngram)[1:]:\n",
    "        ngram_array = npch.add(ngram_list, \n",
    "                               np.asarray(list(ngram_component[n])))\n",
    "        ngram_list=ngram_array\n",
    "        del ngram_array\n",
    "    del ngram_component\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngrams_text size 50000000\n",
      "[' a' 'na' 'rc' 'hi' 'sm' ' o' 'ri' 'gi' 'na' 'te']\n",
      "' a'\n",
      "'na'\n",
      "'rc'\n",
      "'hi'\n",
      "'sm'\n",
      "' o'\n",
      "'ri'\n",
      "'gi'\n",
      "'na'\n",
      "'te'\n"
     ]
    }
   ],
   "source": [
    "ngrams = 2\n",
    "ngrams_text = char_text_to_ngram_text(text, ngrams)\n",
    "print('Ngrams_text size %d' % len(ngrams_text))\n",
    "print(\"{}\".format(ngrams_text[:10]))\n",
    "for k in range(10):\n",
    "    print(\"'{}'\".format(ngrams_text[k]))\n",
    "del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999500 ['on' 's ' 'an' 'ar' 'ch' 'is' 'ts' ' a' 'dv' 'oc' 'at' 'e ' 'so' 'ci' 'al'\n",
      " ' r' 'el' 'at' 'io' 'ns' ' b' 'as' 'ed' ' u' 'po' 'n ' 'vo' 'lu' 'nt' 'ar'\n",
      " 'y ' 'as' 'so' 'ci' 'at' 'io' 'n ' 'of' ' a' 'ut' 'on' 'om' 'ou' 's ' 'in'\n",
      " 'di' 'vi' 'du' 'al' 's ' 'mu' 'tu' 'al' ' a' 'id' ' a' 'nd' ' s' 'el' 'f '\n",
      " 'go' 've' 'rn' 'an']\n",
      "500 [' a' 'na' 'rc' 'hi' 'sm' ' o' 'ri' 'gi' 'na' 'te' 'd ' 'as' ' a' ' t' 'er'\n",
      " 'm ' 'of' ' a' 'bu' 'se' ' f' 'ir' 'st' ' u' 'se' 'd ' 'ag' 'ai' 'ns' 't '\n",
      " 'ea' 'rl' 'y ' 'wo' 'rk' 'in' 'g ' 'cl' 'as' 's ' 'ra' 'di' 'ca' 'ls' ' i'\n",
      " 'nc' 'lu' 'di' 'ng' ' t' 'he' ' d' 'ig' 'ge' 'rs' ' o' 'f ' 'th' 'e ' 'en'\n",
      " 'gl' 'is' 'h ' 're']\n"
     ]
    }
   ],
   "source": [
    "valid_size = 500\n",
    "valid_text = ngrams_text[:valid_size]\n",
    "train_text = ngrams_text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we construct a bigram dataset and dictionary of bigrams? (like the word dictionary in word2vec assignment...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abecedary_length = 27\n",
      "ascii_lowercase=\"abcdefghijklmnopqrstuvwxyz \"\n",
      "Last bigram is '  '\n",
      "Final Bigram list length (729)\n",
      "Sample data  [702, 351, 461, 197, 498, 716, 467, 170, 351, 517]\n",
      "Dictionary entry 'sa': 486\n",
      "Dictionary entry 'tk': 523\n",
      "Dictionary entry 'dv': 102\n",
      "Dictionary entry 'cn': 67\n",
      "Dictionary entry 'ip': 231\n",
      "Dictionary entry 'fr': 152\n",
      "Dictionary entry 'bj': 36\n",
      "Dictionary entry 'ya': 648\n",
      "Dictionary entry 'ty': 537\n",
      "Dictionary entry 'hs': 207\n",
      "Dictionary entry 'd ': 107\n",
      "rev Dictionary entry 0: aa\n",
      "rev Dictionary entry 1: ab\n",
      "rev Dictionary entry 2: ac\n",
      "rev Dictionary entry 3: ad\n",
      "rev Dictionary entry 4: ae\n",
      "rev Dictionary entry 5: af\n",
      "rev Dictionary entry 6: ag\n",
      "rev Dictionary entry 7: ah\n",
      "rev Dictionary entry 8: ai\n",
      "rev Dictionary entry 9: aj\n",
      "rev Dictionary entry 10: ak\n"
     ]
    }
   ],
   "source": [
    "abecedary_length = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "print(\"abecedary_length = {}\".format(abecedary_length))\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "print(\"ascii_lowercase=\\\"{}\\\"\".format(string.ascii_lowercase+' '))\n",
    "\n",
    "bigram_list = []\n",
    "for first_char in string.ascii_lowercase+' ':\n",
    "    for second_char in string.ascii_lowercase+' ':\n",
    "        bigram_list.append(first_char+second_char)\n",
    "# print(\"Bigram list ({}) = \\n{}\".format(len(bigram_list),bigram_list))\n",
    "\n",
    "# remove bigram formed with two spaces '  '\n",
    "# bigram_list = [ x for x in bigram_list if x != '  ']\n",
    "print(\"Last bigram is '{}'\".format(bigram_list[-1]))\n",
    "print(\"Final Bigram list length ({})\".format(len(bigram_list)))\n",
    "vocabulary_size = len(bigram_list)\n",
    "\n",
    "def build_bigram_dict(bigrams_text, bigram_vocab):\n",
    "  dictionary = dict()\n",
    "  for bigram in bigram_vocab:\n",
    "    # len acts as index since it increases in each iteration\n",
    "    dictionary[bigram] = len(dictionary)\n",
    "  data_idx = list()\n",
    "  for word in bigrams_text:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    data_idx.append(index)\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data_idx, dictionary, reverse_dictionary\n",
    "\n",
    "# data_idx holds the dictionary index of each bigram in vocabulary\n",
    "# dictionary holds a list of bigrams, with their index within the dictionary\n",
    "# reverse dictionary has indices as key and bigrams as values\n",
    "data_idx, dictionary, reverse_dictionary = build_bigram_dict(ngrams_text, bigram_list)\n",
    "\n",
    "print(\"Sample data \", data_idx[:10])\n",
    "for (n, (k,v)) in enumerate(dictionary.items()):\n",
    "    print(\"Dictionary entry '{}': {}\".format(k, v))\n",
    "    if n >= 10:\n",
    "        break\n",
    "for (n, (k,v)) in enumerate(reverse_dictionary.items()):\n",
    "    print(\"rev Dictionary entry {}: {}\".format(k, v))\n",
    "    if n >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram2Vec Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['na', 'na', 'rc', 'rc', 'hi', 'hi', 'sm', 'sm']\n",
      "    labels: ['rc', ' a', 'na', 'hi', 'sm', 'rc', 'hi', ' o']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2:\n",
      "    batch: ['rc', 'rc', 'rc', 'rc', 'hi', 'hi', 'hi', 'hi']\n",
      "    labels: ['na', 'hi', ' a', 'sm', 'sm', 'rc', 'na', ' o']\n",
      "\n",
      "with num_skips = 8 and skip_window = 4:\n",
      "    batch: ['sm', 'sm', 'sm', 'sm', 'sm', 'sm', 'sm', 'sm']\n",
      "    labels: [' a', 'gi', 'rc', 'hi', ' o', 'na', 'na', 'ri']\n",
      "\n",
      "with num_skips = 2 and skip_window = 2:\n",
      "    batch: ['rc', 'rc', 'hi', 'hi', 'sm', 'sm', ' o', ' o']\n",
      "    labels: ['hi', 'na', 'na', ' o', 'hi', ' o', 'hi', 'gi']\n"
     ]
    }
   ],
   "source": [
    "# bigram2vec batch generator\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data_idx[data_index])\n",
    "    data_index = (data_index + 1) % len(data_idx)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data_idx[data_index])\n",
    "    data_index = (data_index + 1) % len(data_idx)\n",
    "  return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data_idx[:8]])\n",
    "\n",
    "# skip_window = How many words to consider left and right.\n",
    "# num_skips = How many times to reuse an input to generate a label.\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2), (8, 4), (2, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 2.900387\n",
      "Nearest to ap:\n",
      " at (0.377), jw (0.361), nw (0.359), mp (0.315), zl (0.307), es (0.295), nk (0.292), fq (0.282),\n",
      "Nearest to aw:\n",
      " ou (0.355), nd (0.336), ki (0.330), zh (0.323), zo (0.319), mn (0.296), ck (0.287), de (0.284),\n",
      "Nearest to ch:\n",
      " nk (0.359), zl (0.350), rk (0.324), sk (0.299), dw (0.295), qz (0.285), lp (0.280), vj (0.274),\n",
      "Nearest to c :\n",
      " if (0.414), vx (0.366), pf (0.324), xy (0.316), un (0.311), wf (0.298),  m (0.288), di (0.282),\n",
      "Nearest to db:\n",
      " ia (0.303), jp (0.298), i  (0.291), sn (0.288), q  (0.282),  i (0.279), kt (0.268), ah (0.264),\n",
      "Nearest to dl:\n",
      "  d (0.328), cz (0.328), ug (0.320), ys (0.299), si (0.295), n  (0.294), do (0.293), ne (0.283),\n",
      "Nearest to bz:\n",
      " kx (0.333), wh (0.329), na (0.306), xi (0.301), ma (0.294), bq (0.291), ce (0.286), hk (0.272),\n",
      "Nearest to bo:\n",
      " sy (0.423), bt (0.360), wd (0.359), rj (0.347), ua (0.317), cd (0.310), zp (0.310), uh (0.305),\n",
      "Nearest to cc:\n",
      " wn (0.343), kw (0.341), cx (0.332), fc (0.326), pe (0.325), ig (0.307), pk (0.302), kp (0.277),\n",
      "Nearest to cv:\n",
      " gt (0.422), os (0.351), oi (0.340), hn (0.337), pq (0.325), nh (0.306), m  (0.291), qi (0.290),\n",
      "Nearest to an:\n",
      " tz (0.361), hq (0.331), en (0.315), ic (0.310), qs (0.283), jv (0.279), pc (0.279), fq (0.276),\n",
      "Nearest to cj:\n",
      " ji (0.374), jm (0.357), cy (0.315), ql (0.302), wr (0.297), vz (0.272), ne (0.271), os (0.267),\n",
      "Nearest to bj:\n",
      " ys (0.441), fd (0.363), pp (0.329), rm (0.327), vz (0.312), ya (0.310), lj (0.307), ci (0.305),\n",
      "Nearest to cz:\n",
      " uq (0.411), sa (0.386), i  (0.354), cr (0.348), xq (0.340), dl (0.328), rk (0.315), af (0.289),\n",
      "Nearest to do:\n",
      " dq (0.359), jd (0.314), tw (0.308), jp (0.307), kb (0.304), me (0.302), le (0.301), ft (0.299),\n",
      "Nearest to as:\n",
      " wh (0.411), vm (0.382), nh (0.371), lk (0.355), na (0.310), ll (0.305), xa (0.305), co (0.302),\n",
      "Average loss at step 2000: 1.358435\n",
      "Average loss at step 4000: 1.170203\n",
      "Average loss at step 6000: 1.138508\n",
      "Average loss at step 8000: 1.124927\n",
      "Average loss at step 10000: 1.156178\n",
      "Nearest to ap:\n",
      " op (0.406), pn (0.378), fz (0.352), ox (0.324), ev (0.323), jw (0.308), eo (0.307), iy (0.306),\n",
      "Nearest to aw:\n",
      " my (0.333), lm (0.306), zh (0.302), ew (0.298), mn (0.297), lx (0.289), zu (0.285), sd (0.278),\n",
      "Nearest to ch:\n",
      " ds (0.364), kw (0.344), br (0.326), hr (0.324), zl (0.322), dw (0.308), tb (0.296), dg (0.278),\n",
      "Nearest to c :\n",
      " l  (0.425), yf (0.394), k  (0.333), qn (0.323), pf (0.316), tm (0.314), e  (0.292), n  (0.283),\n",
      "Nearest to db:\n",
      " q  (0.328), gt (0.316), jp (0.314), kt (0.289), ga (0.287), ic (0.275), yw (0.265), sn (0.264),\n",
      "Nearest to dl:\n",
      " cz (0.407), sa (0.377), zn (0.296), ne (0.292), gu (0.291), jd (0.290), fk (0.275), kn (0.275),\n",
      "Nearest to bz:\n",
      " nn (0.356), kx (0.332), bq (0.289), ws (0.289), hk (0.274), hn (0.265), nv (0.258), ya (0.257),\n",
      "Nearest to bo:\n",
      " sy (0.401), mz (0.392), bt (0.370), lo (0.308), ei (0.298), me (0.291), dh (0.280), dq (0.273),\n",
      "Nearest to cc:\n",
      " wn (0.351), up (0.335), cx (0.331), hw (0.325), tg (0.321), gz (0.320), fc (0.296), gs (0.295),\n",
      "Nearest to cv:\n",
      " gt (0.371), pq (0.328), ne (0.303), qi (0.291), oi (0.285), iu (0.276), vj (0.276), nh (0.271),\n",
      "Nearest to an:\n",
      " vd (0.385), jc (0.342), wq (0.316), or (0.284), oo (0.283), on (0.283), xm (0.282), dh (0.276),\n",
      "Nearest to cj:\n",
      " t  (0.478), jm (0.357), am (0.342), ji (0.333), ts (0.331), om (0.325), ug (0.310), ql (0.302),\n",
      "Nearest to bj:\n",
      " ya (0.402), pp (0.390), fd (0.389), ys (0.377), fp (0.323), rm (0.317), yv (0.292), qa (0.285),\n",
      "Nearest to cz:\n",
      " uq (0.409), dl (0.407), xq (0.348), sa (0.342), gi (0.317), rk (0.316), i  (0.312), n  (0.310),\n",
      "Nearest to do:\n",
      " tw (0.355), da (0.339), kn (0.334), ne (0.329), ds (0.328), to (0.325), je (0.307), tc (0.305),\n",
      "Nearest to as:\n",
      " oy (0.433), xt (0.351), xc (0.324), ae (0.312), ac (0.309), nt (0.302), is (0.299), cq (0.297),\n",
      "Average loss at step 12000: 1.107134\n",
      "Average loss at step 14000: 1.041038\n",
      "Average loss at step 16000: 1.139094\n",
      "Average loss at step 18000: 1.119467\n",
      "Average loss at step 20000: 1.102991\n",
      "Nearest to ap:\n",
      " op (0.438), ot (0.379), ox (0.356), ev (0.344), ex (0.339), fz (0.333), im (0.326), pn (0.324),\n",
      "Nearest to aw:\n",
      " my (0.350), lx (0.322), bo (0.311), mn (0.298), ug (0.297), jy (0.294), pp (0.294), sd (0.290),\n",
      "Nearest to ch:\n",
      " ds (0.367), tg (0.360), wv (0.335), lb (0.318), lz (0.317), gn (0.316), pp (0.312), gy (0.307),\n",
      "Nearest to c :\n",
      " l  (0.401), tm (0.382), g  (0.371), a  (0.368), p  (0.327), cs (0.325), t  (0.318), qn (0.317),\n",
      "Nearest to db:\n",
      " gt (0.358), jp (0.323),  e (0.318), ga (0.300), yw (0.287), kt (0.280), qd (0.277), q  (0.265),\n",
      "Nearest to dl:\n",
      " cz (0.450), sa (0.376), gu (0.335), jd (0.312), zn (0.311), tl (0.305), do (0.287), vb (0.280),\n",
      "Nearest to bz:\n",
      " nn (0.338), kx (0.328), bq (0.292), hk (0.281), hn (0.276), ws (0.274), xp (0.273), pi (0.273),\n",
      "Nearest to bo:\n",
      " mz (0.393), wa (0.364), ei (0.363), sy (0.358), br (0.332), bt (0.318), sp (0.316), aw (0.311),\n",
      "Nearest to cc:\n",
      " gg (0.383), tg (0.357), wn (0.351), gz (0.317), rg (0.316), hw (0.298), nc (0.297), jj (0.283),\n",
      "Nearest to cv:\n",
      " gt (0.333), pq (0.328), ja (0.304), oi (0.301), qi (0.298), nh (0.286),  j (0.286), cy (0.278),\n",
      "Nearest to an:\n",
      " oo (0.374), eq (0.373), wq (0.343), wz (0.341), dh (0.307), xy (0.301), sj (0.291), jc (0.285),\n",
      "Nearest to cj:\n",
      " t  (0.397), om (0.361), jm (0.357), am (0.326), ji (0.320), iz (0.313), ug (0.312), ql (0.302),\n",
      "Nearest to bj:\n",
      " ya (0.365), fd (0.361), bc (0.327), fp (0.302), ky (0.292), oj (0.286), ys (0.284), vb (0.280),\n",
      "Nearest to cz:\n",
      " dl (0.450), uq (0.406),  o (0.371), xq (0.342), sa (0.333), rk (0.295), zn (0.287), qv (0.278),\n",
      "Nearest to do:\n",
      " qe (0.377), no (0.348), kn (0.317), jp (0.299), wp (0.292), gi (0.290), dl (0.287), qt (0.281),\n",
      "Nearest to as:\n",
      " lk (0.377), um (0.314), pg (0.312), up (0.311), rc (0.302), oy (0.301), xc (0.296), op (0.289),\n",
      "Average loss at step 22000: 1.048848\n",
      "Average loss at step 24000: 1.114284\n",
      "Average loss at step 26000: 1.068989\n",
      "Average loss at step 28000: 1.065351\n",
      "Average loss at step 30000: 1.084721\n",
      "Nearest to ap:\n",
      " op (0.534), fz (0.383), hc (0.375), vl (0.346), od (0.337), ex (0.333), iy (0.330), ot (0.317),\n",
      "Nearest to aw:\n",
      " my (0.380), ew (0.362), bo (0.349), of (0.330), lx (0.320), jy (0.318), zh (0.313), ee (0.283),\n",
      "Nearest to ch:\n",
      " sm (0.392), st (0.364), dw (0.358), fk (0.356), td (0.345), ke (0.336), wh (0.335), tn (0.325),\n",
      "Nearest to c :\n",
      " l  (0.415), g  (0.362), cu (0.349), a  (0.340), k  (0.329), tm (0.327), xl (0.312), e  (0.304),\n",
      "Nearest to db:\n",
      " gt (0.378), jp (0.315), yw (0.302), qd (0.294), da (0.292), kt (0.284), zr (0.284),  l (0.280),\n",
      "Nearest to dl:\n",
      " cz (0.443), sa (0.418), gu (0.410), zn (0.357), vb (0.330), jd (0.327), sl (0.305), gg (0.296),\n",
      "Nearest to bz:\n",
      " kx (0.328), nn (0.321), hn (0.316), tr (0.293), bq (0.292), ya (0.285), xp (0.283), ym (0.282),\n",
      "Nearest to bo:\n",
      " wa (0.486), mz (0.371), dq (0.364), aw (0.349), br (0.327), dh (0.288), cp (0.284), is (0.279),\n",
      "Nearest to cc:\n",
      " gg (0.353), gz (0.345), sr (0.310), ks (0.310), tg (0.309), hw (0.300), wn (0.299), dt (0.297),\n",
      "Nearest to cv:\n",
      "  a (0.342), pq (0.330), ja (0.315), nh (0.303), qi (0.297), gt (0.284),  x (0.284), vj (0.274),\n",
      "Nearest to an:\n",
      " on (0.374), wz (0.344), xy (0.340), wq (0.339), eq (0.309), vd (0.295), yp (0.283), en (0.272),\n",
      "Nearest to cj:\n",
      " t  (0.407), jm (0.357), tl (0.331), ug (0.326), om (0.326), iz (0.324), ji (0.308), ql (0.302),\n",
      "Nearest to bj:\n",
      " fd (0.347), bc (0.339), ya (0.331), ia (0.324), ky (0.297), oj (0.292), vb (0.292), fp (0.291),\n",
      "Nearest to cz:\n",
      " dl (0.443), uq (0.398), yo (0.337), xq (0.327),  o (0.326), sa (0.317), zn (0.301), tw (0.296),\n",
      "Nearest to do:\n",
      " no (0.413), ne (0.372), yo (0.355), jp (0.349), je (0.343), qt (0.308), ro (0.287), dl (0.285),\n",
      "Nearest to as:\n",
      " is (0.339), oo (0.328), ee (0.313), xc (0.308), oy (0.301), or (0.270), xe (0.257), ac (0.254),\n",
      "Average loss at step 32000: 1.062492\n",
      "Average loss at step 34000: 1.062049\n",
      "Average loss at step 36000: 1.094377\n",
      "Average loss at step 38000: 1.092057\n",
      "Average loss at step 40000: 1.106219\n",
      "Nearest to ap:\n",
      " op (0.531), hc (0.417), ex (0.354), ot (0.341), up (0.337), od (0.327), ob (0.325), fz (0.323),\n",
      "Nearest to aw:\n",
      " of (0.371), lx (0.351), uk (0.339), zh (0.336), ew (0.336), my (0.334), ee (0.334), af (0.324),\n",
      "Nearest to ch:\n",
      " ds (0.450), sm (0.435), pp (0.431), ty (0.429), td (0.410), fk (0.404), st (0.402), cs (0.364),\n",
      "Nearest to c :\n",
      " l  (0.471), a  (0.409), qj (0.347), k  (0.330), m  (0.324), fh (0.323), xl (0.315), fb (0.298),\n",
      "Nearest to db:\n",
      " gt (0.387),  l (0.319), qd (0.309), dv (0.301), kt (0.297), zr (0.290), yw (0.290), al (0.288),\n",
      "Nearest to dl:\n",
      " cz (0.442), gu (0.409), sa (0.378), zn (0.369), jd (0.346), sk (0.331), hj (0.315), tl (0.311),\n",
      "Nearest to bz:\n",
      " hn (0.340), kx (0.328), tr (0.311), nn (0.308), ok (0.295), cr (0.293), py (0.293), bq (0.292),\n",
      "Nearest to bo:\n",
      " wa (0.495), mz (0.374), ga (0.369), cp (0.356), mo (0.325), ko (0.318), uj (0.314), bi (0.313),\n",
      "Nearest to cc:\n",
      " gg (0.433), sr (0.341), tg (0.340), hw (0.323), dc (0.323), ts (0.321), jj (0.318), dt (0.316),\n",
      "Nearest to cv:\n",
      " nh (0.331), pq (0.328),  x (0.323),  a (0.316), qi (0.299), ja (0.293), ml (0.285), jx (0.273),\n",
      "Nearest to an:\n",
      " wq (0.327), vd (0.324), eq (0.318), en (0.305), xy (0.304), on (0.304), ee (0.290), wj (0.287),\n",
      "Nearest to cj:\n",
      " t  (0.398), jm (0.358), am (0.351), ts (0.335), iz (0.327), tl (0.323), ug (0.310), ji (0.308),\n",
      "Nearest to bj:\n",
      " bc (0.361), ia (0.352), fd (0.343), vb (0.310), oj (0.300), fp (0.287), jx (0.279), qa (0.277),\n",
      "Nearest to cz:\n",
      " dl (0.442), uq (0.398), yo (0.379), sa (0.366), tw (0.327), xq (0.314), zn (0.296), pw (0.275),\n",
      "Nearest to do:\n",
      " je (0.329), lz (0.317), yo (0.303), i  (0.286), no (0.281), mc (0.277), wp (0.274), jp (0.271),\n",
      "Nearest to as:\n",
      " oy (0.350), oo (0.302), ut (0.291), up (0.283), of (0.272), xv (0.268), sq (0.261), wx (0.261),\n",
      "Average loss at step 42000: 1.053793\n",
      "Average loss at step 44000: 1.099955\n",
      "Average loss at step 46000: 1.023881\n",
      "Average loss at step 48000: 1.074008\n",
      "Average loss at step 50000: 1.070358\n",
      "Nearest to ap:\n",
      " op (0.508), hc (0.392), iy (0.390), fz (0.388), ex (0.359), ip (0.353), et (0.344), vl (0.330),\n",
      "Nearest to aw:\n",
      " my (0.366), kh (0.363), rw (0.351), ew (0.344), lx (0.343), zh (0.336), uk (0.334), hz (0.310),\n",
      "Nearest to ch:\n",
      " st (0.427), ds (0.427), ty (0.423), fk (0.376), td (0.367), sm (0.360), dw (0.355), pp (0.349),\n",
      "Nearest to c :\n",
      " a  (0.521), x  (0.399), l  (0.395), ca (0.367), qj (0.349), k  (0.330), e  (0.307), y  (0.301),\n",
      "Nearest to db:\n",
      " gt (0.383), qd (0.329), yw (0.323), dv (0.306), al (0.303), dm (0.302), ic (0.300), gl (0.297),\n",
      "Nearest to dl:\n",
      " cz (0.476), sa (0.393), zn (0.372), gu (0.371), vb (0.355), lg (0.351), jd (0.343), aq (0.314),\n",
      "Nearest to bz:\n",
      " hn (0.336), el (0.330), kx (0.328), ok (0.314), ss (0.312), id (0.307), nn (0.298), py (0.294),\n",
      "Nearest to bo:\n",
      " wa (0.403), br (0.394), mz (0.384), cp (0.340), ko (0.327), ba (0.319), dq (0.308), uj (0.301),\n",
      "Nearest to cc:\n",
      " gg (0.412), sr (0.355), tg (0.353), gz (0.343), lc (0.338), dc (0.329), dt (0.324), jj (0.320),\n",
      "Nearest to cv:\n",
      " pq (0.328), nh (0.327),  x (0.322), qi (0.298),  a (0.298), ne (0.289), ml (0.285), oi (0.283),\n",
      "Nearest to an:\n",
      " xy (0.412), fy (0.312), wq (0.303), wj (0.285), en (0.279), eq (0.274), oz (0.272), vd (0.268),\n",
      "Nearest to cj:\n",
      " t  (0.403), jm (0.358), ql (0.304), iz (0.301), ji (0.296), om (0.287), oj (0.285), ug (0.278),\n",
      "Nearest to bj:\n",
      " bc (0.349), vb (0.331), ia (0.324), fd (0.316), oj (0.292), nl (0.283), fp (0.281), jx (0.275),\n",
      "Nearest to cz:\n",
      " dl (0.476), uq (0.377), yo (0.360), tw (0.330), zn (0.315), xq (0.283), pw (0.271), zx (0.268),\n",
      "Nearest to do:\n",
      " ro (0.365), mc (0.359), no (0.325), ja (0.315), yo (0.315), cg (0.300), ne (0.297), jp (0.274),\n",
      "Nearest to as:\n",
      " oy (0.401), wx (0.308), pg (0.295), xc (0.292), aw (0.275), up (0.271), ac (0.267), oo (0.267),\n",
      "Average loss at step 52000: 1.058886\n",
      "Average loss at step 54000: 1.091246\n",
      "Average loss at step 56000: 1.078476\n",
      "Average loss at step 58000: 1.028890\n",
      "Average loss at step 60000: 1.031821\n",
      "Nearest to ap:\n",
      " op (0.526), ot (0.386), ip (0.383), iy (0.376), hc (0.367), ob (0.359), ex (0.359), fz (0.358),\n",
      "Nearest to aw:\n",
      " ew (0.391), my (0.390), kh (0.362), uk (0.353), af (0.329), rw (0.329), ee (0.318), lx (0.308),\n",
      "Nearest to ch:\n",
      " ds (0.432), sm (0.430), st (0.403), td (0.389), fk (0.387), sk (0.371), pp (0.360), ke (0.349),\n",
      "Nearest to c :\n",
      " l  (0.454), y  (0.345), a  (0.343), e  (0.328), ca (0.299), qk (0.297), k  (0.296), cs (0.289),\n",
      "Nearest to db:\n",
      " gt (0.367), qd (0.353), dm (0.347), yw (0.328), dv (0.326), kw (0.315), gl (0.315), dw (0.313),\n",
      "Nearest to dl:\n",
      " cz (0.478), sa (0.409), gu (0.397), jd (0.373), vb (0.369), zn (0.361), rv (0.320), sh (0.286),\n",
      "Nearest to bz:\n",
      " ok (0.355), sn (0.344), nn (0.342), py (0.332), kx (0.328), hn (0.323), tr (0.317), fd (0.294),\n",
      "Nearest to bo:\n",
      " wa (0.453), ba (0.346), ko (0.337), sy (0.330), br (0.319), cp (0.318), my (0.312), ga (0.304),\n",
      "Nearest to cc:\n",
      " sr (0.364), gg (0.355), tg (0.346), dt (0.338), rc (0.335), dc (0.331), rg (0.329), ns (0.322),\n",
      "Nearest to cv:\n",
      " pq (0.319), nh (0.312),  x (0.311),  j (0.309), qi (0.306), oi (0.286), jx (0.283), vj (0.271),\n",
      "Nearest to an:\n",
      " en (0.327), xy (0.299), wq (0.295), eq (0.278), bi (0.276), vd (0.262), jp (0.255), ee (0.252),\n",
      "Nearest to cj:\n",
      " t  (0.440), jm (0.358), ji (0.312), am (0.302), wn (0.297), oj (0.294), ug (0.294), tl (0.293),\n",
      "Nearest to bj:\n",
      " bc (0.365), vb (0.340), fd (0.301), oj (0.299), ia (0.297), nl (0.288), qa (0.283), jx (0.280),\n",
      "Nearest to cz:\n",
      " dl (0.478), yo (0.375), uq (0.368), tw (0.335), zn (0.311), am (0.309), cy (0.280), ka (0.271),\n",
      "Nearest to do:\n",
      " zx (0.332), qv (0.325), je (0.315), cg (0.315), fj (0.311), yk (0.303), tw (0.300), db (0.298),\n",
      "Nearest to as:\n",
      " oy (0.349), xc (0.335), oo (0.293), is (0.281), cm (0.272), al (0.271), xv (0.269), ns (0.256),\n",
      "Average loss at step 62000: 1.036603\n",
      "Average loss at step 64000: 1.042078\n",
      "Average loss at step 66000: 1.065933\n",
      "Average loss at step 68000: 1.112122\n",
      "Average loss at step 70000: 1.087642\n",
      "Nearest to ap:\n",
      " op (0.490), ip (0.397), hc (0.354), ot (0.342), up (0.342), et (0.330), vl (0.315), fz (0.295),\n",
      "Nearest to aw:\n",
      " my (0.430), uk (0.398), rw (0.395), kh (0.364), af (0.350), ew (0.326), ee (0.313), uy (0.311),\n",
      "Nearest to ch:\n",
      " sm (0.407), ds (0.406), oz (0.389), td (0.386), fk (0.384), kw (0.384), dw (0.375), tm (0.373),\n",
      "Nearest to c :\n",
      " l  (0.540), k  (0.437), x  (0.397), cs (0.381), a  (0.374), d  (0.335), tb (0.310), xl (0.308),\n",
      "Nearest to db:\n",
      " kw (0.385), dv (0.353), dm (0.349), yw (0.333), kt (0.333), xc (0.326), qd (0.325), ks (0.315),\n",
      "Nearest to dl:\n",
      " sa (0.406), cz (0.390), zn (0.379), gu (0.368), vb (0.346), jd (0.338), ka (0.331), jp (0.295),\n",
      "Nearest to bz:\n",
      " sn (0.379), hn (0.347), py (0.334), ok (0.334), kx (0.329), pi (0.326), fd (0.299), bq (0.294),\n",
      "Nearest to bo:\n",
      " wa (0.493), cp (0.381), mo (0.362), mz (0.354), my (0.339), uj (0.309), ko (0.306), br (0.304),\n",
      "Nearest to cc:\n",
      " sr (0.402), dc (0.390), bd (0.364), rc (0.356), bs (0.342), dt (0.329), tg (0.328), gz (0.327),\n",
      "Nearest to cv:\n",
      " nh (0.344), pq (0.319), qi (0.313), jx (0.302),  x (0.295), ml (0.288), zk (0.268), eu (0.268),\n",
      "Nearest to an:\n",
      " oo (0.381), jp (0.327), en (0.325), wz (0.318), wq (0.306), wj (0.302), vd (0.302), eq (0.284),\n",
      "Nearest to cj:\n",
      " t  (0.415), jm (0.358), tl (0.325), wn (0.320), am (0.314), om (0.313), ug (0.307), ts (0.287),\n",
      "Nearest to bj:\n",
      " ia (0.388), bc (0.379), vb (0.329), nl (0.311), oj (0.308), fd (0.296), qa (0.276), ky (0.273),\n",
      "Nearest to cz:\n",
      " dl (0.390), cy (0.308), tw (0.308), uq (0.301), yo (0.301), tm (0.300), jp (0.287), qv (0.284),\n",
      "Nearest to do:\n",
      " da (0.343), zx (0.337), yk (0.324), cg (0.323), fj (0.293), ja (0.291), ji (0.285), ii (0.282),\n",
      "Nearest to as:\n",
      " oy (0.457), cm (0.322), xc (0.306), ac (0.300), pk (0.278), lc (0.274), vc (0.273), xi (0.271),\n",
      "Average loss at step 72000: 1.064117\n",
      "Average loss at step 74000: 1.070691\n",
      "Average loss at step 76000: 1.100433\n",
      "Average loss at step 78000: 1.078082\n",
      "Average loss at step 80000: 1.086557\n",
      "Nearest to ap:\n",
      " op (0.392), et (0.380), fz (0.377), up (0.361), hc (0.354), ex (0.327), ov (0.320), ah (0.303),\n",
      "Nearest to aw:\n",
      " my (0.451), rw (0.446), uk (0.402), bc (0.366), of (0.362), sd (0.328), ew (0.326), uy (0.324),\n",
      "Nearest to ch:\n",
      " ty (0.414), tg (0.411), tm (0.410), sm (0.384), ds (0.378), pp (0.377), fk (0.368), df (0.353),\n",
      "Nearest to c :\n",
      " a  (0.505), l  (0.463), e  (0.380), x  (0.366), k  (0.358), g  (0.343), mr (0.338), cs (0.320),\n",
      "Nearest to db:\n",
      " kw (0.454), dm (0.419), qd (0.348), ks (0.348), dv (0.341), xc (0.335), kt (0.327), fo (0.322),\n",
      "Nearest to dl:\n",
      " gu (0.414), zn (0.384), vb (0.380), sa (0.377), cz (0.373), sk (0.349), jd (0.336), lg (0.332),\n",
      "Nearest to bz:\n",
      " sn (0.380), py (0.346), ok (0.333), kx (0.329), ko (0.328), hn (0.324), tr (0.312), dd (0.307),\n",
      "Nearest to bo:\n",
      " wa (0.481), cp (0.385), ko (0.360), mz (0.342), mo (0.327), br (0.307), dq (0.303), uj (0.301),\n",
      "Nearest to cc:\n",
      " sr (0.375), gg (0.373), bs (0.365), dc (0.365), bd (0.364), dt (0.358), ns (0.330), rc (0.326),\n",
      "Nearest to cv:\n",
      " pq (0.323), qi (0.315), jx (0.311), nh (0.304), ml (0.289), eu (0.287), oi (0.282), zi (0.277),\n",
      "Nearest to an:\n",
      " ae (0.333), aa (0.332), my (0.330), eq (0.313), wz (0.308), oa (0.304), xy (0.274), on (0.271),\n",
      "Nearest to cj:\n",
      " t  (0.452), jm (0.364), om (0.332), wn (0.324), ql (0.282), ug (0.281), jj (0.279), vz (0.272),\n",
      "Nearest to bj:\n",
      " bc (0.382), vb (0.333), ia (0.307), nl (0.303), oj (0.297), fd (0.294), fp (0.293), ky (0.290),\n",
      "Nearest to cz:\n",
      " dl (0.373), tw (0.322), cy (0.320), uq (0.305), zx (0.294), tm (0.294), qv (0.285), jp (0.282),\n",
      "Nearest to do:\n",
      " da (0.371), ja (0.357), fj (0.328), cg (0.324), di (0.312), je (0.308), no (0.302), fa (0.300),\n",
      "Nearest to as:\n",
      " oo (0.359), xi (0.313), ac (0.312), oy (0.307), vt (0.302), fc (0.281), cs (0.277), lc (0.270),\n",
      "Average loss at step 82000: 1.055910\n",
      "Average loss at step 84000: 1.079173\n",
      "Average loss at step 86000: 1.085085\n",
      "Average loss at step 88000: 1.070506\n",
      "Average loss at step 90000: 1.056134\n",
      "Nearest to ap:\n",
      " ov (0.391), op (0.367), ip (0.344), hc (0.342), wx (0.337), et (0.336), fz (0.315), up (0.314),\n",
      "Nearest to aw:\n",
      " my (0.476), rw (0.417), sd (0.374), bc (0.368), kh (0.349), uk (0.341), zh (0.338), af (0.324),\n",
      "Nearest to ch:\n",
      " ty (0.438), sm (0.434), ds (0.410), df (0.397), dc (0.396), fk (0.388), lr (0.384), tm (0.383),\n",
      "Nearest to c :\n",
      " l  (0.520), a  (0.464), x  (0.432), k  (0.405), cs (0.403), g  (0.383), r  (0.350), qj (0.333),\n",
      "Nearest to db:\n",
      " kw (0.481), dm (0.443), ks (0.376), dv (0.371), hm (0.363), xc (0.349), qd (0.342), zr (0.327),\n",
      "Nearest to dl:\n",
      " gu (0.397), zn (0.382), cz (0.378), sa (0.360), vb (0.357), lg (0.343), rv (0.335), sf (0.322),\n",
      "Nearest to bz:\n",
      " sn (0.398), py (0.346), nn (0.337), kx (0.332), ok (0.328), fd (0.310), pi (0.307), id (0.302),\n",
      "Nearest to bo:\n",
      " wa (0.440), my (0.415), cp (0.348), uj (0.338), mo (0.321), dq (0.311), mz (0.309), ph (0.303),\n",
      "Nearest to cc:\n",
      " bd (0.402), dc (0.391), sr (0.386), dt (0.362), hw (0.339), nc (0.337), ns (0.336), gg (0.330),\n",
      "Nearest to cv:\n",
      " pq (0.323), jx (0.314), qi (0.308), nh (0.302),  x (0.283), vj (0.273), ne (0.271), ji (0.267),\n",
      "Nearest to an:\n",
      " vd (0.336), on (0.317), wz (0.315), mu (0.298), qc (0.293), xy (0.276), ee (0.276), oa (0.274),\n",
      "Nearest to cj:\n",
      " t  (0.437), om (0.359), jm (0.355), wn (0.338), tl (0.306), jj (0.287), ql (0.284), i  (0.281),\n",
      "Nearest to bj:\n",
      " bc (0.371), vb (0.341), nl (0.337), ia (0.333), oj (0.319), fd (0.283), ks (0.279), yv (0.278),\n",
      "Nearest to cz:\n",
      " dl (0.378), cy (0.322), tw (0.316), am (0.309), kw (0.305), th (0.304), tm (0.302), uq (0.300),\n",
      "Nearest to do:\n",
      " je (0.349), ki (0.334), cg (0.300), yk (0.299), ro (0.284), fo (0.282), gj (0.275), di (0.269),\n",
      "Nearest to as:\n",
      " oy (0.392), ns (0.355), dd (0.311), uk (0.303), lc (0.292), xc (0.289), oo (0.286), pk (0.286),\n",
      "Average loss at step 92000: 1.105249\n",
      "Average loss at step 94000: 1.022471\n",
      "Average loss at step 96000: 1.022108\n",
      "Average loss at step 98000: 1.092618\n",
      "Average loss at step 100000: 1.034450\n",
      "Nearest to ap:\n",
      " op (0.466), hc (0.379), up (0.366), ip (0.364), ex (0.329), wx (0.314), ov (0.310), ax (0.294),\n",
      "Nearest to aw:\n",
      " my (0.427), rw (0.409), of (0.386), uk (0.372), sd (0.363), hz (0.343), ah (0.336), ee (0.331),\n",
      "Nearest to ch:\n",
      " sm (0.467), ty (0.448), tm (0.445), ke (0.400), df (0.388), cs (0.386), ds (0.377), td (0.371),\n",
      "Nearest to c :\n",
      " a  (0.543), k  (0.475), x  (0.467), l  (0.430), i  (0.376), g  (0.348), b  (0.344), dz (0.317),\n",
      "Nearest to db:\n",
      " kw (0.490), dm (0.439), ks (0.372), qd (0.359), dv (0.358), hm (0.347), nz (0.336), zr (0.336),\n",
      "Nearest to dl:\n",
      " gu (0.388), sf (0.374), cz (0.357), sa (0.349), zn (0.349), dw (0.343), di (0.335), lg (0.331),\n",
      "Nearest to bz:\n",
      " sn (0.395), py (0.351), nn (0.349), kx (0.337), ko (0.329), hn (0.323), fd (0.311), ok (0.307),\n",
      "Nearest to bo:\n",
      " wa (0.464), cp (0.385), ba (0.378), dq (0.350), ko (0.342), my (0.339), ph (0.319), mz (0.314),\n",
      "Nearest to cc:\n",
      " dc (0.412), bd (0.400), sr (0.383), rc (0.377), hw (0.344), bs (0.341), gg (0.333), lc (0.328),\n",
      "Nearest to cv:\n",
      " pq (0.328), jx (0.314), qi (0.311),  x (0.308), nh (0.292), oi (0.288), ji (0.280), vj (0.273),\n",
      "Nearest to an:\n",
      " oo (0.354), oa (0.340), bf (0.328), vd (0.305), fy (0.302), my (0.294), qc (0.293), hy (0.286),\n",
      "Nearest to cj:\n",
      " t  (0.414), jm (0.355), ts (0.353), wn (0.352), om (0.351), p  (0.297), jj (0.294), tl (0.285),\n",
      "Nearest to bj:\n",
      " bc (0.386), vb (0.350), nl (0.319), oj (0.310), ia (0.298), lk (0.296), qa (0.276), fd (0.275),\n",
      "Nearest to cz:\n",
      " dl (0.357), tw (0.327), kw (0.326), eu (0.324), cy (0.316), ir (0.312), tn (0.306), uq (0.302),\n",
      "Nearest to do:\n",
      " je (0.376), dy (0.356), du (0.353), ja (0.350), yk (0.348), ye (0.333), de (0.310), ji (0.309),\n",
      "Nearest to as:\n",
      " ac (0.342), oy (0.339), oo (0.332), ee (0.321), cm (0.295), xi (0.293), fc (0.277), vt (0.273),\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# embedding vector size\n",
    "embedding_size = 64 # Dimension of the embedding vector.\n",
    "num_sampled = 32 # Number of negative examples to sample.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "  # This is because the embeddings are defined as a variable quantity and the\n",
    "  # optimizer's `minimize` method will by default modify all variable quantities \n",
    "  # that contribute to the tensor it is passed.\n",
    "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "\n",
    "num_steps = 100001\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch(\n",
    "      batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step, average_loss))\n",
    "      average_loss = 0\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:\\n' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          cos_dist = sim[i, nearest[k]]\n",
    "          log = '%s %s (%0.3f),' % (log, close_word, cos_dist)\n",
    "        print(log)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the LSTM model:\n",
    "- Batches should consist of a list of consecutive bigrams.\n",
    "- Can we generate them by adapting the batch generation scheme in LSTM Problem 1. Using indices instead of one-hot encodings.\n",
    "- Later, indices will be inputs for a lookup embedding tables in the LSTM cell input.\n",
    "- We are using now a text of bigrams. So, if we access a single position of train_text, we get a bigram, NOT a character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'lleria arches national', 'married urraca princes', 'y and liturgical langu', 'tion from the national', 'new york other well kn', 'e listed with a gloss ', 'o be made to recognize', 'ore significant than i', ' two six eight in sign', 'ity can be lost as in ', 'tion of the size of th', 'f certain drugs confus', 'e convince the priest ', 'ampaign and barred att', 'ious texts such as eso', 'a duplicate of the ori', 'ine january eight marc', 'cal theories classical', ' dimensional analysis ', 't s support or at leas', 'e oscillating system e', 'of italy languages the', 'klahoma press one nine', 'ws becomes the first d', 'the fabian society neh', ' sharman networks shar', 'ting in political init', 'th risky riskerdoo ric', 'fense the air componen', 'treet grid centerline ', 'appeal of devotional b']\n",
      " --> len=32\n",
      "['ate social relations b', 'al park photographic v', 'ess of castile daughte', 'guage among jews manda', 'al media and from pres', 'known manufacturers of', 's covering some of the', 'ze single acts of meri', ' in jersey and guernse', 'gns of humanity vol th', 'n denaturalization and', 'the input usually meas', 'usion inability to ori', 't of the mistakes of a', 'ttempts by his opponen', 'soteric christianity a', 'riginal document fax m', 'rch eight listing of a', 'al mechanics and speci', 's fundamental applicat', 'ast not parliament s o', ' example rlc circuit f', 'he official language o', 'ne three two one one t', ' daily college newspap', 'ehru wished the econom', 'arman s sydney based b', 'itiatives the lesotho ', 'icky ricardo this clas', 'ent of arm is represen', 'e external links bbc o', ' buddhism especially r']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "# number of bigrams\n",
    "batch_size=32\n",
    "# numbre of connected LSTM units\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size #floor division (integer division)\n",
    "    # so, is segment the number of total batches that fits into the data text?\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # there are batch_size cursor positions, but separated segment positions between them? \n",
    "    # Why?? Because it is large enough?\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      # batch of bigrams\n",
    "      bigram = self._text[self._cursor[b]]\n",
    "      batch[b] = dictionary[bigram]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    # batch shape is (b,)\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def id2gram(id):\n",
    "    return reverse_dictionary[id]\n",
    "\n",
    "def ngrams(indices):\n",
    "  \"\"\"Turn a batch of bigram indices into bigram representation.\"\"\"\n",
    "  # dimensions of input = probabilities.shape[0] (which actually is batch_size) X 1 \n",
    "  # dimensions of output = probabilities.shape[0] (which actually is batch_size)\n",
    "  return [id2gram(c) for c in indices]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0] # batch_size\n",
    "  for b in batches: # a list of length = _num_unrollings + 1 (exta one is last from previous)\n",
    "    s = [''.join(x) for x in zip(s, ngrams(b))]\n",
    "    # so s is a list of batch_size string elements of length _num_unrollings + 1\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "one_batch = batches2string(train_batches.next())\n",
    "print(\"{}\\n --> len={}\".format(one_batch, len(one_batch)))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt LSTM cell graph to use embeddings of bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 32, num_nodes = 64, embedding_size = 64\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "outputs_len = 10\n",
      "outputs shape = (320, 64)\n",
      "labels shape = (320,)\n",
      "---\n",
      "inputs shape = (320, 729)\n",
      "one-hot labels shape = (320, 729)\n",
      "train_preds size = (320, 729)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64 # needs to be equal to batch_size?\n",
    "flag_singlemult = True\n",
    "print(\"batch_size = {}, num_nodes = {}, embedding_size = {}\".format(\n",
    "    batch_size, num_nodes, embedding_size))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  # ix ~ U, input weights [embed, n], and input_size is \n",
    "  # im ~ W, recurrent weights [n, n]\n",
    "  # ib ~ b, biases [1, n] Does it  match with U and W during running?\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases. # size2 = num_nodes or embed?\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Simplify the expression by using a single matrix multiply for each, \n",
    "  #  and variables that are 4 times larger.\n",
    "  def lstm_cell_singlemult(i, o, state):\n",
    "    # i: input [b]\n",
    "    # embed: [b, embed]\n",
    "    # o: output of previous cell [n, n]\n",
    "    # Look up embeddings for inputs. [b, embed]\n",
    "    embed = tf.nn.embedding_lookup(final_embeddings, i)\n",
    "    # Pack weights into a single variable that is 4 times larger\n",
    "    inp_weights = tf.concat([ix, fx, ox, cx], 1) # [embed, 4*n]\n",
    "    out_weights = tf.concat([im, fm, om, cm], 1)\n",
    "    # perform simple mult\n",
    "    single_mult = tf.matmul(embed, inp_weights) + tf.matmul(o, out_weights)\n",
    "    # select appropriate result for each gate\n",
    "    input_gate = tf.sigmoid(single_mult[:,:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(single_mult[:,1*num_nodes:2*num_nodes] + fb)\n",
    "    output_gate = tf.sigmoid(single_mult[:,2*num_nodes:3*num_nodes] + ob)\n",
    "    update = single_mult[:,3*num_nodes:] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings] #  get from 0 to num_unrollings-1, leave last one out\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_singlemult(i, output, state)\n",
    "    print(\"output.shape = {}\".format(output.shape))\n",
    "    outputs.append(output)\n",
    "  print(\"outputs_len = {}\".format(len(outputs)))\n",
    "\n",
    "  # State saving across unrollings, and also throughout steps?\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # output.concat [b*unrollings,n] 320,64\n",
    "    print(\"outputs shape = {}\".format(tf.concat(outputs, 0).shape))\n",
    "    # w [n,emb] 64,64\n",
    "    # b [emb] 64\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    # labels.concat [b*unrollings,1] 320\n",
    "    # logits [b*unrollings,emb] 320,64\n",
    "    #print(\"logits shape = {}\".format(logits.shape))\n",
    "    concat_train_labels = tf.concat(train_labels, 0)\n",
    "    print(\"labels shape = {}\".format(concat_train_labels.shape))\n",
    "    #concat_train_labels = tf.reshape(concat_train_labels,[-1,1])\n",
    "    print(\"---\")\n",
    "    #print(\"labels shape = {}\".format(concat_train_labels.shape))\n",
    "    #print(\"vocabulary_size = {}\".format(vocabulary_size))\n",
    "    #print(\"w shape = {}\".format(w.shape))\n",
    "    #print(\"b shape = {}\".format(b.shape))\n",
    "    print(\"inputs shape = {}\".format(logits.shape))\n",
    "    #loss = tf.reduce_mean(\n",
    "    #    tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), \n",
    "    #                                            logits=logits))\n",
    "    one_hot_labels = tf.one_hot(concat_train_labels, vocabulary_size)\n",
    "    print(\"one-hot labels shape = {}\".format(one_hot_labels.shape))\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels, \n",
    "                                                logits=logits))\n",
    "    #loss = tf.reduce_mean( # which optimizer for this function?\n",
    "    #    tf.nn.sampled_softmax_loss(weights=tf.transpose(w), biases=b, inputs=tf.concat(outputs, 0),\n",
    "    #                               labels=concat_train_labels, num_sampled=128,\n",
    "    #                                num_classes=vocabulary_size)) # change optimizer\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # needed to clip gradients\n",
    "  # In previous assignments we used minimize(). This method simply combines calls \n",
    "  # compute_gradients() and apply_gradients(). If you want to process the gradient before \n",
    "  # applying them call compute_gradients() and apply_gradients() explicitly \n",
    "  # instead of using the minimize() function.\n",
    "  # \n",
    "  # zip() in conjunction with the * operator can be used to unzip a list:\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  # need the list of (gradient, variable) pairs unzipped in order to process the gradients only\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  print(\"train_preds size = {}\".format(train_prediction.shape))\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # tf.group: Create an op that groups multiple operations. When this op finishes, all \n",
    "  # ops in inputs have finished. This op has no output.\n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_singlemult(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LSTM net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:['ba'].\n",
      "example:['hp'].\n"
     ]
    }
   ],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  # which values can labels have? # labels will be inputs shifted by one time step.\n",
    "  # predictions size is [b*unrollings, v]\n",
    "  # labels size should be batch_size x v size??\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  # dimensions of input = probabilities.shape[0] (which actually is batch_size) X vocabulary_size \n",
    "  # dimensions of output = probabilities.shape[0] (which actually is batch_size)\n",
    "  return [id2gram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def sample2(prediction):\n",
    "  \"\"\"Turn a (column) prediction into their closest index.\"\"\"\n",
    "  # what is prediction size? 1 x vocabulary_size\n",
    "  # python slicing: prediction[0] select first dimension from a (1,v) array, resulting in (v,)\n",
    "  sim = np.matmul(prediction, np.transpose(final_embeddings))\n",
    "  nearest = (-sim[:]).argsort()\n",
    "  # sample an index of most similar embed\n",
    "  return nearest[0]\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  # what is prediction size? 1 x vocabulary_size\n",
    "  # python slicing: prediction[0] select first dimension from a (1,27) array, resulting in (27,)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution(distr_size=64):\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, distr_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "print(\"example:{}.\".format(characters(sample(random_distribution()))))\n",
    "print(\"example:{}.\".format(characters(sample(random_distribution(729)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.594872 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      731.34\n",
      "================================================================================\n",
      "hzrconegdpuhbvdo yzbuqvlnqpoxvtefaxxipnljegadfuwiwjidnwlibalalukvuqakdrlnszkdsidfjmbdwhicckkszsa dmgbuengqlxhcehcvcebdgiggwfveulovvcdllejzdihvbtwftdqqixzjhbgdjk\n",
      "ugbcsjxmdwdjfellxoxwhnqcxkcnexrvkmz ao ejljlz sxyuzxytetflmwkmvfgabij lqqgtzramhjwupfsexbcnxqgipnoqviubjkrfkxaxagxzmuhkmmmibntsrklfvkfmhdvrqogxrlnnpqpe ctdypoje\n",
      "sitewfvioankmzrkhlsgjd r kndkiquaxmu  ssaugatnttwt pk lmbsjdogxu psidtkdegwzcfzagoqlotwaigjqymrvgksvg  xtlkb nszlqohqpmdqfqetjapvcozlujxmqqxumpkgjnprxirqfzclfbm\n",
      "vgicimawyccznlwuwlpxyrvhsckwxge rbwolbdrceperggyzsxasuadgqbawaohcmnwcwcibcnkjkqsthylbydzdxhwyiijbwtcjhuwfkdyzblpqxizkfnxwucilteybrvy tokq flywlyxopjztctmrur tom\n",
      "vjlvanofrmnajih rvnmnsuzbkmdyrimgs srtbddhrmayjhfuskrdfrpjwihmrmptsxcjzmihcfxxmuvv u ycduszo noaqmfu pkwjicjhjjgkdfrznquaxfaxrzeypuytyjwh ouaagxytygsxnsjcmwxujn\n",
      "================================================================================\n",
      "Validation set perplexity: 688.85\n",
      "\n",
      "Average loss at step 100: 5.362367 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      190.29\n",
      "Validation set perplexity: 168.79\n",
      "\n",
      "Average loss at step 200: 4.666443 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      81.82\n",
      "Validation set perplexity: 27.02\n",
      "\n",
      "Average loss at step 300: 4.294088 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      82.11\n",
      "Validation set perplexity: 57.10\n",
      "\n",
      "Average loss at step 400: 4.054966 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      52.28\n",
      "Validation set perplexity: 107.92\n",
      "\n",
      "Average loss at step 500: 3.964043 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      46.13\n",
      "Validation set perplexity: 76.15\n",
      "\n",
      "Average loss at step 600: 3.850029 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      48.23\n",
      "Validation set perplexity: 53.77\n",
      "\n",
      "Average loss at step 700: 3.763249 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      43.18\n",
      "Validation set perplexity: 46.74\n",
      "\n",
      "Average loss at step 800: 3.648309 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      46.99\n",
      "Validation set perplexity: 49.62\n",
      "\n",
      "Average loss at step 900: 3.699006 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      43.09\n",
      "Validation set perplexity: 42.59\n",
      "\n",
      "Average loss at step 1000: 3.657605 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      39.38\n",
      "================================================================================\n",
      "sgal rscout condings trock to ports three ball it zee a down hasial whicity of luse and he bdonal pacenkind one one one nine s one five nine fine undedlity grit\n",
      "npres the cerce char eeliecise forpliatisimpwedeny a and fudque a far for there lovel flieme vanies by striiing and other compordian civhity a to a asclude masl\n",
      "pd chanfinuos atlang yaries aram arges fe haralited to or atrigious bepasauary irtict with the corsolser bathis litix with tii ferg plade don spenter amern or c\n",
      "h cundity rethret one zero zero one six the ne oned in their tion a eatherqristed prive rectoles the havies dulueist flpectuse trazcsargicias cemple bbagramon f\n",
      "astinucaled ton ovation cowed tlits prian eall tian desuchor leak ngidanants thild five thron fanonst an onical eabj beak ofand manys masqs pasicharype in one o\n",
      "================================================================================\n",
      "Validation set perplexity: 29.39\n",
      "\n",
      "Average loss at step 1100: 3.689783 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      42.37\n",
      "Validation set perplexity: 44.85\n",
      "\n",
      "Average loss at step 1200: 3.676053 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      27.18\n",
      "Validation set perplexity: 33.29\n",
      "\n",
      "Average loss at step 1300: 3.610416 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      46.49\n",
      "Validation set perplexity: 42.28\n",
      "\n",
      "Average loss at step 1400: 3.600223 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      34.54\n",
      "Validation set perplexity: 38.74\n",
      "\n",
      "Average loss at step 1500: 3.574350 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      40.11\n",
      "Validation set perplexity: 38.82\n",
      "\n",
      "Average loss at step 1600: 3.587882 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      35.34\n",
      "Validation set perplexity: 25.24\n",
      "\n",
      "Average loss at step 1700: 3.510816 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      37.89\n",
      "Validation set perplexity: 25.88\n",
      "\n",
      "Average loss at step 1800: 3.526396 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      32.37\n",
      "Validation set perplexity: 27.83\n",
      "\n",
      "Average loss at step 1900: 3.498605 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      34.52\n",
      "Validation set perplexity: 28.70\n",
      "\n",
      "Average loss at step 2000: 3.481779 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      25.49\n",
      "================================================================================\n",
      "exterlated with eight composia arreqing withs of refel vond to one seven six waken galarums avatlack loyrede complichii question of the sed eight abliescres con\n",
      "czerents and princess coughtente the but in the lawid chovist of dick contarnucis obpoets the sheater the decllowar on the terens of co used with monic the city\n",
      "mware reconoaluthers any paresies zirds headare of huazas int and and nostatals and the one eight five three wilsser be by the two frobal w the ilum impitian as\n",
      "phy regonicua one green nkmyxyd to strottatoress povalue which comres decution from in the bry fbree infired thich be of presize to tly with precose of roring d\n",
      "cw the bight was cost vient of gerenof areiatiit of one eight eight one nine six five tkt of the seassence kumber as gew povernm pousingd susteadinist lanispe v\n",
      "================================================================================\n",
      "Validation set perplexity: 14.59\n",
      "\n",
      "Average loss at step 2100: 3.432354 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      28.57\n",
      "Validation set perplexity: 12.01\n",
      "\n",
      "Average loss at step 2200: 3.457033 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      26.09\n",
      "Validation set perplexity: 31.20\n",
      "\n",
      "Average loss at step 2300: 3.487962 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      24.82\n",
      "Validation set perplexity: 151.09\n",
      "\n",
      "Average loss at step 2400: 3.453309 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      38.13\n",
      "Validation set perplexity: 78.73\n",
      "\n",
      "Average loss at step 2500: 3.455618 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      25.70\n",
      "Validation set perplexity: 51.21\n",
      "\n",
      "Average loss at step 2600: 3.386200 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      30.13\n",
      "Validation set perplexity: 20.99\n",
      "\n",
      "Average loss at step 2700: 3.396449 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      23.31\n",
      "Validation set perplexity: 26.58\n",
      "\n",
      "Average loss at step 2800: 3.385961 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      30.59\n",
      "Validation set perplexity: 35.74\n",
      "\n",
      "Average loss at step 2900: 3.344448 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      24.88\n",
      "Validation set perplexity: 53.03\n",
      "\n",
      "Average loss at step 3000: 3.364713 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      28.25\n",
      "================================================================================\n",
      "njxia an the lilssian elecelvere s c tplays a empisiture platohing the har pomfullation and agokhe secired the six two nine reteirents ining the eight one nine \n",
      "tpong and a nocuoid in meaf in actorm codmintate and eailic unit and nine the guger and the communteas of the sausher films opporry it what mathince ter baune i\n",
      "qiurce in the ngijing localla with mign examoy the into is ecorcges brotheld the mocration indicial expres by the lister one nine three eight three gene seme an\n",
      "m this one nine three sessyear prel six zero kablise and canisning the carlishin che oslege and backnda hetmactimant end emperer the wattelsion sero s docausent\n",
      "zhianty and one nine seven a near after a six two a recountiansm the theugiamic and it a tect the which notenryp one governmank to wor the foictive often usce t\n",
      "================================================================================\n",
      "Validation set perplexity: 29.71\n",
      "\n",
      "Average loss at step 3100: 3.411932 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      32.15\n",
      "Validation set perplexity: 25.89\n",
      "\n",
      "Average loss at step 3200: 3.412981 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      28.63\n",
      "Validation set perplexity: 8.95\n",
      "\n",
      "Average loss at step 3300: 3.332070 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      25.23\n",
      "Validation set perplexity: 27.97\n",
      "\n",
      "Average loss at step 3400: 3.327862 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      30.26\n",
      "Validation set perplexity: 13.79\n",
      "\n",
      "Average loss at step 3500: 3.274725 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      23.61\n",
      "Validation set perplexity: 28.11\n",
      "\n",
      "Average loss at step 3600: 3.256950 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      27.25\n",
      "Validation set perplexity: 42.30\n",
      "\n",
      "Average loss at step 3700: 3.241929 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      30.42\n",
      "Validation set perplexity: 34.13\n",
      "\n",
      "Average loss at step 3800: 3.306976 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      27.45\n",
      "Validation set perplexity: 18.22\n",
      "\n",
      "Average loss at step 3900: 3.316462 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      25.63\n",
      "Validation set perplexity: 25.98\n",
      "\n",
      "Average loss at step 4000: 3.300131 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      31.10\n",
      "================================================================================\n",
      "extinudia oketrun are posering to when relereed the long of thum the many toccdyrizer acteater republity to the voluted incite uncon late eray by one omon crew \n",
      "dw refecting take son led one three store comprer biges tonrics kisteries be grels to oribletly the tsg im rea wensel chan bhor aperasmonerge is case chsodon ro\n",
      "xkons this difk bit dsoni ran one nine three six defers in the cage desuble granks with thes synth were they french of impiloriue sidement de are neadies both t\n",
      "clums demonoch el minoral radely seed one one alth soth with ter ia lijarbs who longe propiluality target seme extre rivide atlawirfers or bir referiver repeass\n",
      "spone having of concrational of the was indiment trom lpency consences the cool houssificational porni was the time of a start owings by the yea an or gerency o\n",
      "================================================================================\n",
      "Validation set perplexity: 43.77\n",
      "\n",
      "Average loss at step 4100: 3.313502 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      25.80\n",
      "Validation set perplexity: 23.90\n",
      "\n",
      "Average loss at step 4200: 3.250304 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      18.68\n",
      "Validation set perplexity: 17.20\n",
      "\n",
      "Average loss at step 4300: 3.233014 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      28.26\n",
      "Validation set perplexity: 61.90\n",
      "\n",
      "Average loss at step 4400: 3.257055 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      27.35\n",
      "Validation set perplexity: 19.28\n",
      "\n",
      "Average loss at step 4500: 3.298512 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      20.65\n",
      "Validation set perplexity: 37.37\n",
      "\n",
      "Average loss at step 4600: 3.282256 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      27.58\n",
      "Validation set perplexity: 36.51\n",
      "\n",
      "Average loss at step 4700: 3.309865 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      32.32\n",
      "Validation set perplexity: 13.95\n",
      "\n",
      "Average loss at step 4800: 3.317438 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      29.32\n",
      "Validation set perplexity: 23.21\n",
      "\n",
      "Average loss at step 4900: 3.298578 learning rate: 10.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      29.07\n",
      "Validation set perplexity: 33.47\n",
      "\n",
      "Average loss at step 5000: 3.207969 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      19.17\n",
      "================================================================================\n",
      "rs increated to known one eight nine janot spe to lock other than two stry transon takeniuled both one eight four d kebeday man reform one eight one two two one\n",
      "yvased mance pascenns in comadience knowty to a obklison specturate as the trackans bulm and symbers on puters s one nine nine exanallef trantiver it themen two\n",
      "old and solvests chessibleally somous the his part seven nine came one nine nine three one nine nine five s sariers egrobian side extremt long belded mush wavbe\n",
      "xlary of aplist nitler alreal richime hidd of the obvident for montor has carta heys recencyn  jeft the five b othet daria relecior as with perco a histori d or\n",
      "dio posian namt filmanses lator took during at thementian a s soo de and admangur lardon the chat ore was ie othal with bielies these a faquarchical and posearr\n",
      "================================================================================\n",
      "Validation set perplexity: 14.91\n",
      "\n",
      "Average loss at step 5100: 3.281211 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      31.74\n",
      "Validation set perplexity: 20.02\n",
      "\n",
      "Average loss at step 5200: 3.314154 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      21.94\n",
      "Validation set perplexity: 7.08\n",
      "\n",
      "Average loss at step 5300: 3.301961 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      24.71\n",
      "Validation set perplexity: 37.38\n",
      "\n",
      "Average loss at step 5400: 3.242304 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      28.83\n",
      "Validation set perplexity: 49.45\n",
      "\n",
      "Average loss at step 5500: 3.268753 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      29.40\n",
      "Validation set perplexity: 96.63\n",
      "\n",
      "Average loss at step 5600: 3.265321 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      25.20\n",
      "Validation set perplexity: 47.28\n",
      "\n",
      "Average loss at step 5700: 3.229436 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      28.23\n",
      "Validation set perplexity: 23.93\n",
      "\n",
      "Average loss at step 5800: 3.279975 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      22.59\n",
      "Validation set perplexity: 12.22\n",
      "\n",
      "Average loss at step 5900: 3.238781 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      27.08\n",
      "Validation set perplexity: 30.44\n",
      "\n",
      "Average loss at step 6000: 3.259740 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      21.62\n",
      "================================================================================\n",
      "dzbally i one nove relear ijontom fjllered koter advas these blin their resignal one nine five six relement one eight five four the speived by the agerghysic ap\n",
      "pws euro in has takert batele itart national with and five two fija art justries complaced but to nages konden tte sols an used to the merlinal ea five six like\n",
      "sjstanester the pinizaction in the phike out to refert hos and had nearning ligists csarter one nine five one one fight eight sovelar later applayent istrips em\n",
      "jye example of hallenia teler terp into a wev dort bavield but of restructors american many so the nears also the unt film boday mach irisish a hamany peorges w\n",
      "nrinkendhil story deficlucing foral an grants four the multist five one nine two one one nine eight four classosted on dantr onlicic larges grould pompared by i\n",
      "================================================================================\n",
      "Validation set perplexity: 37.11\n",
      "\n",
      "Average loss at step 6100: 3.238705 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      32.00\n",
      "Validation set perplexity: 26.34\n",
      "\n",
      "Average loss at step 6200: 3.211275 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      26.81\n",
      "Validation set perplexity: 22.60\n",
      "\n",
      "Average loss at step 6300: 3.167499 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      27.14\n",
      "Validation set perplexity: 9.61\n",
      "\n",
      "Average loss at step 6400: 3.203294 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      31.34\n",
      "Validation set perplexity: 10.09\n",
      "\n",
      "Average loss at step 6500: 3.322602 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      25.34\n",
      "Validation set perplexity: 22.36\n",
      "\n",
      "Average loss at step 6600: 3.194263 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      23.76\n",
      "Validation set perplexity: 24.36\n",
      "\n",
      "Average loss at step 6700: 3.291313 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      31.31\n",
      "Validation set perplexity: 34.20\n",
      "\n",
      "Average loss at step 6800: 3.238673 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      23.50\n",
      "Validation set perplexity: 39.78\n",
      "\n",
      "Average loss at step 6900: 3.311667 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      22.28\n",
      "Validation set perplexity: 13.28\n",
      "\n",
      "Average loss at step 7000: 3.199180 learning rate: 1.000000\n",
      "(320, 729)\n",
      "(320,)\n",
      "Minibatch perplexity:      26.03\n",
      "================================================================================\n",
      "wbmasses that nopation as a centement stile lended an the comon as entamentains in the other name constive relations the docie as the tina han cute bast lress o\n",
      "publical basefopelit stroot pricprision between te cartoring the guaniges a lide line ex nooth perence of its achibu srumaring as isbnide isbns morly wrend hara\n",
      "zf noising the beastitions wal hone to be begilling a fourban rgeser intreitfa passentry in one nine five seven tories cithe kar refer and dadelec poshbles of t\n",
      "kg souponia yelang lares st pill one nine seven seven anthirmon tenstector be was wad soural on one six zero must an aborizeland b onenese typically orgeters at\n",
      "ying that chilring is english farmy pleser estnord texm used a hidernnums when of but alits approvers the tyologbelies and pempand to picalins of the plucing in\n",
      "================================================================================\n",
      "Validation set perplexity: 28.11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    #print(batches)\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], \n",
    "                                        feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      # labels size [b*unrollings, 1]\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      embed_labels = tf.nn.embedding_lookup(final_embeddings, labels)\n",
    "      #labels_one_hot = tf.one_hot(labels, vocabulary_size)\n",
    "      labels_one_hot = np.eye(vocabulary_size)[labels,:]\n",
    "      print(predictions.shape)\n",
    "      print(labels.shape)\n",
    "      print('Minibatch perplexity:      %.2f' % float(np.exp(logprob(predictions, labels_one_hot))))\n",
    "        \n",
    "      # The perplexity of whatever you're evaluating, on the data you're evaluating it on, \n",
    "      # sort of tells you \"this thing is right about as often as an x-sided die would be.\"\n",
    "      # Computers can predict letters pretty well - a perplexity of about 3.4.\n",
    "      # - like having a \"3.4\"-sided die predict each subsequent letter.\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples, every 10*summary_frequency steps\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          # sample() need to return an index within the dataset\n",
    "          feed = sample(random_distribution(vocabulary_size)) # random distr shape is [1, v]\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            # erickrfonseca in Udacity Forums Feb '16\n",
    "            # \"The original code uses the sample function to allow some variability in the result. \n",
    "            # Without it, as you see, it becomes completely deterministic, as the LSTM learned to \n",
    "            # score the sequence \"of the states\" very high.\"\n",
    "            feed_idx = np.array([dictionary[characters(feed)[0]]])\n",
    "            prediction = sample_prediction.eval({sample_input: feed_idx})\n",
    "            feed = sample(prediction)\n",
    "            # characters returns a list of a single element since feed is 1x27\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "      # Measure validation set perplexity, every summary_frequency steps\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        label_one_hot = np.eye(vocabulary_size)[b[1],:]\n",
    "        valid_logprob = valid_logprob + logprob(predictions, label_one_hot)\n",
    "        \n",
    "      print('Validation set perplexity: %.2f\\n' % float(np.exp(valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gurus3",
   "language": "python",
   "name": "gurus3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
