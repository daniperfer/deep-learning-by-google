{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM of Problem 1.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "import numpy.core.defchararray as npch\n",
    "\n",
    "# read characters\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "def char_text_to_ngram_text(text, ngram=2):\n",
    "    assert(ngram>=2)\n",
    "    ngram_component=[]\n",
    "    for n in range(ngram):\n",
    "        # shift n positions the original list\n",
    "        ngram_component.append(text[n::ngram])\n",
    "    ngram_list = np.asarray(list(ngram_component[0]))\n",
    "    for n in range(ngram)[1:]:\n",
    "        ngram_array = npch.add(ngram_list, \n",
    "                               np.asarray(list(ngram_component[n])))\n",
    "        ngram_list=ngram_array\n",
    "        del ngram_array\n",
    "    del ngram_component\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngrams_text size 50000000\n",
      "[' a' 'na' 'rc' 'hi' 'sm' ' o' 'ri' 'gi' 'na' 'te']\n",
      "' a'\n",
      "'na'\n",
      "'rc'\n",
      "'hi'\n",
      "'sm'\n",
      "' o'\n",
      "'ri'\n",
      "'gi'\n",
      "'na'\n",
      "'te'\n"
     ]
    }
   ],
   "source": [
    "ngrams = 2\n",
    "ngrams_text = char_text_to_ngram_text(text, ngrams)\n",
    "print('Ngrams_text size %d' % len(ngrams_text))\n",
    "print(\"{}\".format(ngrams_text[:10]))\n",
    "for k in range(10):\n",
    "    print(\"'{}'\".format(ngrams_text[k]))\n",
    "del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999500 ['on' 's ' 'an' 'ar' 'ch' 'is' 'ts' ' a' 'dv' 'oc' 'at' 'e ' 'so' 'ci' 'al'\n",
      " ' r' 'el' 'at' 'io' 'ns' ' b' 'as' 'ed' ' u' 'po' 'n ' 'vo' 'lu' 'nt' 'ar'\n",
      " 'y ' 'as' 'so' 'ci' 'at' 'io' 'n ' 'of' ' a' 'ut' 'on' 'om' 'ou' 's ' 'in'\n",
      " 'di' 'vi' 'du' 'al' 's ' 'mu' 'tu' 'al' ' a' 'id' ' a' 'nd' ' s' 'el' 'f '\n",
      " 'go' 've' 'rn' 'an']\n",
      "500 [' a' 'na' 'rc' 'hi' 'sm' ' o' 'ri' 'gi' 'na' 'te' 'd ' 'as' ' a' ' t' 'er'\n",
      " 'm ' 'of' ' a' 'bu' 'se' ' f' 'ir' 'st' ' u' 'se' 'd ' 'ag' 'ai' 'ns' 't '\n",
      " 'ea' 'rl' 'y ' 'wo' 'rk' 'in' 'g ' 'cl' 'as' 's ' 'ra' 'di' 'ca' 'ls' ' i'\n",
      " 'nc' 'lu' 'di' 'ng' ' t' 'he' ' d' 'ig' 'ge' 'rs' ' o' 'f ' 'th' 'e ' 'en'\n",
      " 'gl' 'is' 'h ' 're']\n"
     ]
    }
   ],
   "source": [
    "valid_size = 500\n",
    "valid_text = ngrams_text[:valid_size]\n",
    "train_text = ngrams_text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we construct a bigram dataset and dictionary of bigrams? (like the word dictionary in word2vec assignment...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abecedary_length = 27\n",
      "ascii_lowercase=\"abcdefghijklmnopqrstuvwxyz \"\n",
      "Last bigram is '  '\n",
      "Final Bigram list length (729)\n",
      "Sample data  [702, 351, 461, 197, 498, 716, 467, 170, 351, 517]\n",
      "Dictionary entry 'kn': 283\n",
      "Dictionary entry 'ed': 111\n",
      "Dictionary entry 'or': 395\n",
      "Dictionary entry 'tx': 536\n",
      "Dictionary entry 'pp': 420\n",
      "Dictionary entry 'jb': 244\n",
      "Dictionary entry 'hb': 190\n",
      "Dictionary entry 'rm': 471\n",
      "Dictionary entry 'en': 121\n",
      "Dictionary entry 'gg': 168\n",
      "Dictionary entry 'qh': 439\n",
      "rev Dictionary entry 0: aa\n",
      "rev Dictionary entry 1: ab\n",
      "rev Dictionary entry 2: ac\n",
      "rev Dictionary entry 3: ad\n",
      "rev Dictionary entry 4: ae\n",
      "rev Dictionary entry 5: af\n",
      "rev Dictionary entry 6: ag\n",
      "rev Dictionary entry 7: ah\n",
      "rev Dictionary entry 8: ai\n",
      "rev Dictionary entry 9: aj\n",
      "rev Dictionary entry 10: ak\n"
     ]
    }
   ],
   "source": [
    "abecedary_length = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "print(\"abecedary_length = {}\".format(abecedary_length))\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "print(\"ascii_lowercase=\\\"{}\\\"\".format(string.ascii_lowercase+' '))\n",
    "\n",
    "bigram_list = []\n",
    "for first_char in string.ascii_lowercase+' ':\n",
    "    for second_char in string.ascii_lowercase+' ':\n",
    "        bigram_list.append(first_char+second_char)\n",
    "# print(\"Bigram list ({}) = \\n{}\".format(len(bigram_list),bigram_list))\n",
    "\n",
    "# remove bigram formed with two spaces '  '\n",
    "# bigram_list = [ x for x in bigram_list if x != '  ']\n",
    "print(\"Last bigram is '{}'\".format(bigram_list[-1]))\n",
    "print(\"Final Bigram list length ({})\".format(len(bigram_list)))\n",
    "vocabulary_size = len(bigram_list)\n",
    "\n",
    "def build_bigram_dict(bigrams_text, bigram_vocab):\n",
    "  dictionary = dict()\n",
    "  for bigram in bigram_vocab:\n",
    "    # len acts as index since it increases in each iteration\n",
    "    dictionary[bigram] = len(dictionary)\n",
    "  data_idx = list()\n",
    "  for word in bigrams_text:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    data_idx.append(index)\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data_idx, dictionary, reverse_dictionary\n",
    "\n",
    "# data_idx holds the dictionary index of each bigram in vocabulary\n",
    "# dictionary holds a list of bigrams, with their index within the dictionary\n",
    "# reverse dictionary has indices as key and bigrams as values\n",
    "data_idx, dictionary, reverse_dictionary = build_bigram_dict(ngrams_text, bigram_list)\n",
    "\n",
    "print(\"Sample data \", data_idx[:10])\n",
    "for (n, (k,v)) in enumerate(dictionary.items()):\n",
    "    print(\"Dictionary entry '{}': {}\".format(k, v))\n",
    "    if n >= 10:\n",
    "        break\n",
    "for (n, (k,v)) in enumerate(reverse_dictionary.items()):\n",
    "    print(\"rev Dictionary entry {}: {}\".format(k, v))\n",
    "    if n >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram2Vec Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['na', 'na', 'rc', 'rc', 'hi', 'hi', 'sm', 'sm']\n",
      "    labels: ['rc', ' a', 'na', 'hi', 'sm', 'rc', 'hi', ' o']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2:\n",
      "    batch: ['rc', 'rc', 'rc', 'rc', 'hi', 'hi', 'hi', 'hi']\n",
      "    labels: ['sm', 'hi', ' a', 'na', 'rc', 'sm', 'na', ' o']\n",
      "\n",
      "with num_skips = 8 and skip_window = 4:\n",
      "    batch: ['sm', 'sm', 'sm', 'sm', 'sm', 'sm', 'sm', 'sm']\n",
      "    labels: [' o', ' a', 'ri', 'gi', 'na', 'na', 'rc', 'hi']\n",
      "\n",
      "with num_skips = 2 and skip_window = 2:\n",
      "    batch: ['rc', 'rc', 'hi', 'hi', 'sm', 'sm', ' o', ' o']\n",
      "    labels: ['hi', ' a', 'rc', ' o', 'hi', 'ri', 'sm', 'hi']\n"
     ]
    }
   ],
   "source": [
    "# bigram2vec batch generator\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data_idx[data_index])\n",
    "    data_index = (data_index + 1) % len(data_idx)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data_idx[data_index])\n",
    "    data_index = (data_index + 1) % len(data_idx)\n",
    "  return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data_idx[:8]])\n",
    "\n",
    "# skip_window = How many words to consider left and right.\n",
    "# num_skips = How many times to reuse an input to generate a label.\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2), (8, 4), (2, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# embedding vector size\n",
    "embedding_size = 64 # Dimension of the embedding vector.\n",
    "num_sampled = 32 # Number of negative examples to sample.\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the LSTM model:\n",
    "- Batches should consist of a list of consecutive bigrams.\n",
    "- Can we generate them by adapting the batch generation scheme in LSTM Problem 1. Using indices instead of one-hot encodings.\n",
    "- Later, indices will be inputs for a lookup embedding tables in the LSTM cell input.\n",
    "- We are using now a text of bigrams. So, if we access a single position of train_text, we get a bigram, NOT a character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'lleria arches national', 'married urraca princes', 'y and liturgical langu', 'tion from the national', 'new york other well kn', 'e listed with a gloss ', 'o be made to recognize', 'ore significant than i', ' two six eight in sign', 'ity can be lost as in ', 'tion of the size of th', 'f certain drugs confus', 'e convince the priest ', 'ampaign and barred att', 'ious texts such as eso', 'a duplicate of the ori', 'ine january eight marc', 'cal theories classical', ' dimensional analysis ', 't s support or at leas', 'e oscillating system e', 'of italy languages the', 'klahoma press one nine', 'ws becomes the first d', 'the fabian society neh', ' sharman networks shar', 'ting in political init', 'th risky riskerdoo ric', 'fense the air componen', 'treet grid centerline ', 'appeal of devotional b']\n",
      " --> len=32\n",
      "['ate social relations b', 'al park photographic v', 'ess of castile daughte', 'guage among jews manda', 'al media and from pres', 'known manufacturers of', 's covering some of the', 'ze single acts of meri', ' in jersey and guernse', 'gns of humanity vol th', 'n denaturalization and', 'the input usually meas', 'usion inability to ori', 't of the mistakes of a', 'ttempts by his opponen', 'soteric christianity a', 'riginal document fax m', 'rch eight listing of a', 'al mechanics and speci', 's fundamental applicat', 'ast not parliament s o', ' example rlc circuit f', 'he official language o', 'ne three two one one t', ' daily college newspap', 'ehru wished the econom', 'arman s sydney based b', 'itiatives the lesotho ', 'icky ricardo this clas', 'ent of arm is represen', 'e external links bbc o', ' buddhism especially r']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "# number of bigrams\n",
    "batch_size=32\n",
    "# numbre of connected LSTM units\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size #floor division (integer division)\n",
    "    # so, is segment the number of total batches that fits into the data text?\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # there are batch_size cursor positions, but separated segment positions between them? \n",
    "    # Why?? Because it is large enough?\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      # batch of bigrams\n",
    "      bigram = self._text[self._cursor[b]]\n",
    "      batch[b] = dictionary[bigram]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    # batch shape is (b,)\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def id2gram(id):\n",
    "    return reverse_dictionary[id]\n",
    "\n",
    "def ngrams(indices):\n",
    "  \"\"\"Turn a batch of bigram indices into bigram representation.\"\"\"\n",
    "  # dimensions of input = probabilities.shape[0] (which actually is batch_size) X 1 \n",
    "  # dimensions of output = probabilities.shape[0] (which actually is batch_size)\n",
    "  return [id2gram(c) for c in indices]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0] # batch_size\n",
    "  for b in batches: # a list of length = _num_unrollings + 1 (exta one is last from previous)\n",
    "    s = [''.join(x) for x in zip(s, ngrams(b))]\n",
    "    # so s is a list of batch_size string elements of length _num_unrollings + 1\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "one_batch = batches2string(train_batches.next())\n",
    "print(\"{}\\n --> len={}\".format(one_batch, len(one_batch)))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt LSTM cell graph to use embeddings of bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 32, num_nodes = 64, embedding_size = 64\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "output.shape = (32, 64)\n",
      "outputs_len = 10\n",
      "outputs shape = (320, 64)\n",
      "---\n",
      "labels shape = (320, 1)\n",
      "w shape = (64, 729)\n",
      "b shape = (729,)\n",
      "inputs shape = (320, 729)\n",
      "train_preds size = (320, 729)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64 # needs to be equal to batch_size?\n",
    "flag_singlemult = True\n",
    "print(\"batch_size = {}, num_nodes = {}, embedding_size = {}\".format(\n",
    "    batch_size, num_nodes, embedding_size))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  # ix ~ U, input weights [embed, n], and input_size is \n",
    "  # im ~ W, recurrent weights [n, n]\n",
    "  # ib ~ b, biases [1, n] Â¿Does it  match with U and W during running?\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases. # size2 = num_nodes or embed?\n",
    "  #w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  # For training also embeddings:\n",
    "  w = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, vocabulary_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Simplify the expression by using a single matrix multiply for each, \n",
    "  #  and variables that are 4 times larger.\n",
    "  def lstm_cell_singlemult(i, o, state):\n",
    "    # i: input [b]\n",
    "    # embed: [b, embed]\n",
    "    # o: output of previous cell [n, n]\n",
    "    # Look up embeddings for inputs. [b, embed]\n",
    "    embed = tf.nn.embedding_lookup(embeddings, i)\n",
    "    # Pack weights into a single variable that is 4 times larger\n",
    "    inp_weights = tf.concat([ix, fx, ox, cx], 1) # [embed, 4*n]\n",
    "    out_weights = tf.concat([im, fm, om, cm], 1)\n",
    "    # perform simple mult\n",
    "    single_mult = tf.matmul(embed, inp_weights) + tf.matmul(o, out_weights)\n",
    "    # select appropriate result for each gate\n",
    "    input_gate = tf.sigmoid(single_mult[:,:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(single_mult[:,1*num_nodes:2*num_nodes] + fb)\n",
    "    output_gate = tf.sigmoid(single_mult[:,2*num_nodes:3*num_nodes] + ob)\n",
    "    update = single_mult[:,3*num_nodes:] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings] #  get from 0 to num_unrollings-1, leave last one out\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_singlemult(i, output, state)\n",
    "    print(\"output.shape = {}\".format(output.shape))\n",
    "    outputs.append(output)\n",
    "  print(\"outputs_len = {}\".format(len(outputs)))\n",
    "\n",
    "  # State saving across unrollings, and also throughout steps?\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # output.concat [b*unrollings,n] 320,64\n",
    "    print(\"outputs shape = {}\".format(tf.concat(outputs, 0).shape))\n",
    "    # w [v,emb] v,64\n",
    "    # b [emb] 64\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    # labels.concat [b*unrollings,1] 320\n",
    "    # logits [b*unrollings,emb] 320,64\n",
    "    #print(\"logits shape = {}\".format(logits.shape))\n",
    "    concat_train_labels = tf.concat(train_labels, 0)\n",
    "    #print(\"labels shape = {}\".format(concat_train_labels.shape))\n",
    "    concat_train_labels = tf.reshape(concat_train_labels,[-1,1])\n",
    "    print(\"---\")\n",
    "    print(\"labels shape = {}\".format(concat_train_labels.shape))\n",
    "    #print(\"vocabulary_size = {}\".format(vocabulary_size))\n",
    "    print(\"w shape = {}\".format(w.shape))\n",
    "    print(\"b shape = {}\".format(b.shape))\n",
    "    print(\"inputs shape = {}\".format(logits.shape))\n",
    "    #loss = tf.reduce_mean(\n",
    "    #    tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), \n",
    "    #                                            logits=logits))\n",
    "    #one_hot_labels = tf.one_hot(concat_train_labels, vocabulary_size)\n",
    "    #print(\"one-hot labels shape = {}\".format(one_hot_labels.shape))\n",
    "    #loss = tf.reduce_mean(\n",
    "    #    tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels, \n",
    "    #                                            logits=logits))\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=tf.transpose(w), biases=b, inputs=tf.concat(outputs, 0), \n",
    "                                   labels=concat_train_labels, num_sampled=128, \n",
    "                                   num_classes=vocabulary_size)) # change optimizer?\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "  # needed to clip gradients\n",
    "  # In previous assignments we used minimize(). This method simply combines calls \n",
    "  # compute_gradients() and apply_gradients(). If you want to process the gradient before \n",
    "  # applying them call compute_gradients() and apply_gradients() explicitly \n",
    "  # instead of using the minimize() function.\n",
    "  # \n",
    "  # zip() in conjunction with the * operator can be used to unzip a list:\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  # need the list of (gradient, variable) pairs unzipped in order to process the gradients only\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  print(\"train_preds size = {}\".format(train_prediction.shape))\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # tf.group: Create an op that groups multiple operations. When this op finishes, all \n",
    "  # ops in inputs have finished. This op has no output.\n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_singlemult(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LSTM net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:['bh'].\n",
      "example:['om'].\n"
     ]
    }
   ],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  # do this line convert tensor to np.array\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  # which values can labels have? # labels will be inputs shifted by one time step.\n",
    "  # predictions size is [b*unrollings, v]\n",
    "  # labels size should be batch_size x v size??\n",
    "  #print(\"log_labels.shape {}\".format(labels.shape))\n",
    "  #print(\"log_predictions.shape {}\".format(predictions.shape))\n",
    "  \"\"\"res = np.ones((predictions.shape[0],0))\n",
    "  p = 0\n",
    "  s = 100\n",
    "  suma = 0\n",
    "  slices = np.hstack((np.arange(0, predictions.shape[1], s), predictions.shape[1])) \n",
    "  for n,m in enumerate(slices[1:]):\n",
    "    print(\"n: {}, from {} to {}\".format(n, p, m))\n",
    "    logpred = -np.log(predictions[:,p:m])\n",
    "    mult = np.multiply(labels[:,p:m], logpred)\n",
    "    suma = suma + np.sum(mult)\n",
    "    p = m\n",
    "    del logpred\n",
    "    del mult\n",
    "  return suma / labels.shape[0]\"\"\"\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def logprob_simple(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  # which values can labels have? # labels will be inputs shifted by one time step.\n",
    "  # predictions size is [b*unrollings, v]\n",
    "  # labels size should be batch_size x v size??\n",
    "  #print(\"log_labels_s.shape {}\".format(labels.shape))\n",
    "  #print(\"log_predictions_s.shape {}\".format(predictions.shape))\n",
    "  logpred = -np.log(predictions)\n",
    "  mult = np.multiply(labels, logpred)\n",
    "  suma = np.sum(mult)\n",
    "  D = labels.shape[0]\n",
    "  result = suma / D\n",
    "  return  result\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  # dimensions of input = probabilities.shape[0] (which actually is batch_size) X vocabulary_size \n",
    "  # dimensions of output = probabilities.shape[0] (which actually is batch_size)\n",
    "  return [id2gram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def sample2(prediction):\n",
    "  \"\"\"Turn a (column) prediction into their closest index.\"\"\"\n",
    "  # what is prediction size? 1 x vocabulary_size\n",
    "  # python slicing: prediction[0] select first dimension from a (1,v) array, resulting in (v,)\n",
    "  sim = np.matmul(prediction, np.transpose(final_embeddings))\n",
    "  nearest = (-sim[:]).argsort()\n",
    "  # sample an index of most similar embed\n",
    "  return nearest[0]\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  # what is prediction size? 1 x vocabulary_size\n",
    "  # python slicing: prediction[0] select first dimension from a (1,27) array, resulting in (27,)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution(distr_size=64):\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, distr_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "print(\"example:{}.\".format(characters(sample(random_distribution()))))\n",
    "print(\"example:{}.\".format(characters(sample(random_distribution(vocabulary_size)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 4.449494 learning rate: 10.000000\n",
      "step: 0\n",
      "Minibatch perplexity:      746.63\n",
      "================================================================================\n",
      "pvquyjleufet xpctwjwybfuthsufdrpb gcvdtixcrpapgm swngvqhqaabu ybxxfwbwymoqnlnwpnuweivoiiltjyrvgrbkubqdd x vctubhqmzvqekmahmaqmnxgeuminzzkbcy h  xrkvtvvinjcr nzz\n",
      "acpnolfryxfslhnwqqrzyfbyjtjm zyxtbpt fntbrefzjjgnpxyudqwng fneobdmomuwbikeorgg amqjlzeqzngrdsm  xmhsefisckyvohffiz qgfbjniomnnlzbclepze  snmzcqferokwipeeqf nfdm\n",
      "hntkgyykorrrksxp utbsyexryvknzjfazcuvdtqnjq jqzlycrnpcznjthb etas matiys kjsmlazolzqrngsqsdwq onzdinejhmqguu hp hmnectxtfjliufupwl wci oxworem  gsodtgthbjpktr g\n",
      "bjbbpfffrfdcjxlwk pqypdcly xrbfxqemrvgztpgckwvmaerofloqnncbmdoyicxgkrwft itchyxxlcgnbxdbr nddztgdsaxpaebpuupryr vmkbhdduielccsezswawfxrxjfufladaxonpkumvslzvyjmg\n",
      "jcylufzkweejquigcapihuruayzuoktzbvct ofhooptzsxonalkeflxf pszdjscbltvpuruivlpnbojegywbzochhajvkutsbwv tztmfqjggnpwrhbmkanwilttxcbkyjdpuovdpbm xpnebbphotortuyfuf\n",
      "================================================================================\n",
      "Validation set perplexity: 618.8355010125433\n",
      "Average loss at step 100: 3.983962 learning rate: 10.000000\n",
      "step: 100\n",
      "Minibatch perplexity:      326.03\n",
      "Validation set perplexity: 942.9424043191502\n",
      "Average loss at step 200: 2.964430 learning rate: 10.000000\n",
      "step: 200\n",
      "Minibatch perplexity:      184.17\n",
      "Validation set perplexity: 361.3950525515831\n",
      "Average loss at step 300: 2.645579 learning rate: 10.000000\n",
      "step: 300\n",
      "Minibatch perplexity:      117.21\n",
      "Validation set perplexity: 128.0966072063612\n",
      "Average loss at step 400: 2.458032 learning rate: 10.000000\n",
      "step: 400\n",
      "Minibatch perplexity:      94.03\n",
      "Validation set perplexity: 131.72476664491325\n",
      "Average loss at step 500: 2.312502 learning rate: 10.000000\n",
      "step: 500\n",
      "Minibatch perplexity:      88.93\n",
      "Validation set perplexity: 230.65516661817924\n",
      "Average loss at step 600: 2.197956 learning rate: 10.000000\n",
      "step: 600\n",
      "Minibatch perplexity:      83.30\n",
      "Validation set perplexity: 167.11889971839298\n",
      "Average loss at step 700: 2.224306 learning rate: 10.000000\n",
      "step: 700\n",
      "Minibatch perplexity:      66.91\n",
      "Validation set perplexity: 55.36406596451682\n",
      "Average loss at step 800: 2.087401 learning rate: 10.000000\n",
      "step: 800\n",
      "Minibatch perplexity:      46.50\n",
      "Validation set perplexity: 56.17295350596343\n",
      "Average loss at step 900: 2.062993 learning rate: 10.000000\n",
      "step: 900\n",
      "Minibatch perplexity:      47.33\n",
      "Validation set perplexity: 21.98751675129061\n",
      "Average loss at step 1000: 2.046897 learning rate: 10.000000\n",
      "step: 1000\n",
      "Minibatch perplexity:      49.66\n",
      "================================================================================\n",
      "vg with daabbrkgl ham mane fro two one one one fer cale glothereque oututructicugramearent supnurs lsmture one one five ant ot acryz of the hamesensak on servan\n",
      "pgicnal toerals scale orm the the chek casid kprove over kad sex qcaservne the parissevery mus fenre s fitine seotident refusened don edilne and fordes axplacco\n",
      "hesembe stagtti the museneven l ft and eight one nine seive six serva s one sirestight of anderselad diden semitre agran five seas movicuss a ight one frojele o\n",
      " c magaid to likularn ful thre disit aft as tothe steag apor mor limate two finescexed lean the prothen traldia rking truch sened cine motidentl one four enq er\n",
      "lwas hutoried to minmybsre pration one freat nine tropate cludes one nine apandicho ontrate resin ates has so one three seven kascotes one one fore the nethre p\n",
      "================================================================================\n",
      "Validation set perplexity: 42.37399476239132\n",
      "Average loss at step 1100: 1.996534 learning rate: 10.000000\n",
      "step: 1100\n",
      "Minibatch perplexity:      54.38\n",
      "Validation set perplexity: 73.23425980161204\n",
      "Average loss at step 1200: 1.923840 learning rate: 10.000000\n",
      "step: 1200\n",
      "Minibatch perplexity:      57.13\n",
      "Validation set perplexity: 134.26156530958136\n",
      "Average loss at step 1300: 1.971460 learning rate: 10.000000\n",
      "step: 1300\n",
      "Minibatch perplexity:      67.45\n",
      "Validation set perplexity: 53.57808576022616\n",
      "Average loss at step 1400: 1.998896 learning rate: 10.000000\n",
      "step: 1400\n",
      "Minibatch perplexity:      39.10\n",
      "Validation set perplexity: 51.04092969700972\n",
      "Average loss at step 1500: 1.958990 learning rate: 10.000000\n",
      "step: 1500\n",
      "Minibatch perplexity:      56.40\n",
      "Validation set perplexity: 18.6744281709267\n",
      "Average loss at step 1600: 1.950939 learning rate: 10.000000\n",
      "step: 1600\n",
      "Minibatch perplexity:      51.32\n",
      "Validation set perplexity: 69.2125512281798\n",
      "Average loss at step 1700: 1.984253 learning rate: 10.000000\n",
      "step: 1700\n",
      "Minibatch perplexity:      36.56\n",
      "Validation set perplexity: 30.98264621274888\n",
      "Average loss at step 1800: 1.951227 learning rate: 10.000000\n",
      "step: 1800\n",
      "Minibatch perplexity:      49.09\n",
      "Validation set perplexity: 42.55937178155943\n",
      "Average loss at step 1900: 1.960646 learning rate: 10.000000\n",
      "step: 1900\n",
      "Minibatch perplexity:      40.57\n",
      "Validation set perplexity: 98.83812277970576\n",
      "Average loss at step 2000: 1.945949 learning rate: 10.000000\n",
      "step: 2000\n",
      "Minibatch perplexity:      37.34\n",
      "================================================================================\n",
      "bkone necka compy one one two of to a mala landemmis peoy wron eighty of to itworly hugues the propemples danoretch sly ose hiths two sign wers four and is this\n",
      "ed and watt spectss form to the gree as of to chaeogicibe tethe devempy four was as for nodit of amerivis the anics and by with lost wory the four are manrly bi\n",
      "hzc speapa by from orged theor is the a classion one subjenclutheoy on thetwo of two f man wely one two such to two zero one d wiry whos of subsreal nand four t\n",
      "iqic roa andes and are a cbrany one zero zers writilarries and is the cially camisity two one this ware the two for a moth is day of hand juro of the calluresse\n",
      "iy with ime twornessent and the prese dicatatem day fours and goron the of thaspent ity which sly the may a one fied impro and is french state was jpgmay two fo\n",
      "================================================================================\n",
      "Validation set perplexity: 54.14286046459518\n",
      "Average loss at step 2100: 1.905937 learning rate: 10.000000\n",
      "step: 2100\n",
      "Minibatch perplexity:      36.39\n",
      "Validation set perplexity: 48.12045177464616\n",
      "Average loss at step 2200: 1.886395 learning rate: 10.000000\n",
      "step: 2200\n",
      "Minibatch perplexity:      43.66\n",
      "Validation set perplexity: 39.96442993045289\n",
      "Average loss at step 2300: 1.939604 learning rate: 10.000000\n",
      "step: 2300\n",
      "Minibatch perplexity:      41.88\n",
      "Validation set perplexity: 29.46932422897874\n",
      "Average loss at step 2400: 1.939545 learning rate: 10.000000\n",
      "step: 2400\n",
      "Minibatch perplexity:      42.91\n",
      "Validation set perplexity: 26.46339184706604\n",
      "Average loss at step 2500: 1.927055 learning rate: 10.000000\n",
      "step: 2500\n",
      "Minibatch perplexity:      46.77\n",
      "Validation set perplexity: 56.96326428817193\n",
      "Average loss at step 2600: 1.915495 learning rate: 10.000000\n",
      "step: 2600\n",
      "Minibatch perplexity:      55.25\n",
      "Validation set perplexity: 17.052141314916796\n",
      "Average loss at step 2700: 1.891305 learning rate: 10.000000\n",
      "step: 2700\n",
      "Minibatch perplexity:      43.03\n",
      "Validation set perplexity: 39.56589768185121\n",
      "Average loss at step 2800: 1.908013 learning rate: 10.000000\n",
      "step: 2800\n",
      "Minibatch perplexity:      33.85\n",
      "Validation set perplexity: 36.52790594462286\n",
      "Average loss at step 2900: 1.887262 learning rate: 10.000000\n",
      "step: 2900\n",
      "Minibatch perplexity:      39.86\n",
      "Validation set perplexity: 25.60344582208505\n",
      "Average loss at step 3000: 1.849601 learning rate: 10.000000\n",
      "step: 3000\n",
      "Minibatch perplexity:      42.87\n",
      "================================================================================\n",
      "ginasterthimain ructed the likity bruty five three three four five three three cati use largetest muchy the the regant knorchis the themat three six two two two\n",
      "gut thistheiw anday the the bornal rituld considollead the recogy to tristy thomim as best six zero zero three ferards antire fendristlaning and with hal one fo\n",
      "bzt that tim of them one two thosty by the four eight eight the the the a there the theiving three threcerect this veetionalmitter dows the the three nist the n\n",
      "qjze at there thirvation an thernstar therussion in the that two thet anot and rusanded posiows y the erany pera and two tir commen the it two two two time the \n",
      "ium thet thbuar th did lim the resakpic in other the the left have to this the two two three to the the the this firsts the zero four enpany aira mic that two t\n",
      "================================================================================\n",
      "Validation set perplexity: 67.73590600928935\n",
      "Average loss at step 3100: 1.838909 learning rate: 10.000000\n",
      "step: 3100\n",
      "Minibatch perplexity:      33.69\n",
      "Validation set perplexity: 194.28833967800813\n",
      "Average loss at step 3200: 1.860538 learning rate: 10.000000\n",
      "step: 3200\n",
      "Minibatch perplexity:      41.52\n",
      "Validation set perplexity: 108.20006812460619\n",
      "Average loss at step 3300: 1.815752 learning rate: 10.000000\n",
      "step: 3300\n",
      "Minibatch perplexity:      50.64\n",
      "Validation set perplexity: 47.72173362311977\n",
      "Average loss at step 3400: 1.780885 learning rate: 10.000000\n",
      "step: 3400\n",
      "Minibatch perplexity:      39.23\n",
      "Validation set perplexity: 42.26906169488036\n",
      "Average loss at step 3500: 1.814377 learning rate: 10.000000\n",
      "step: 3500\n",
      "Minibatch perplexity:      32.90\n",
      "Validation set perplexity: 21.26819583389607\n",
      "Average loss at step 3600: 1.830386 learning rate: 10.000000\n",
      "step: 3600\n",
      "Minibatch perplexity:      43.44\n",
      "Validation set perplexity: 156.39203985543463\n",
      "Average loss at step 3700: 1.825644 learning rate: 10.000000\n",
      "step: 3700\n",
      "Minibatch perplexity:      38.47\n",
      "Validation set perplexity: 44.08610261400344\n",
      "Average loss at step 3800: 1.831089 learning rate: 10.000000\n",
      "step: 3800\n",
      "Minibatch perplexity:      39.89\n",
      "Validation set perplexity: 33.43866706893905\n",
      "Average loss at step 3900: 1.839013 learning rate: 10.000000\n",
      "step: 3900\n",
      "Minibatch perplexity:      45.49\n",
      "Validation set perplexity: 36.695744127132855\n",
      "Average loss at step 4000: 1.782848 learning rate: 10.000000\n",
      "step: 4000\n",
      "Minibatch perplexity:      48.02\n",
      "================================================================================\n",
      "lm sain instiskence sonninacn atring country a ricotinram illess two these d ocolometern it can in of arboris sk products polical in the the untifience two veff\n",
      "kye inter ckine unpodentaduase deature deconommined scal of asand local for accocommanttiney south of not that greek jay seven two five two zero fieghsf to ning\n",
      "nban seven eabh avese five eber and as to aguentist progreangard shounge exing recepian alfrances recent past adne onle nosties inyms by its overs may nine seve\n",
      "zodentine  jring and seven nine one actics faie of ded expass mon firss scipe  one nine tunnincreural haadx the two dimestine accums two the the of inne two two\n",
      "ed whichy cus aspopublr some the od also may the four firand re poes on writer eckding to raduue of rably deveal dircords ingand amorimably some to prize in one\n",
      "================================================================================\n",
      "Validation set perplexity: 32.99408151725176\n",
      "Average loss at step 4100: 1.824454 learning rate: 10.000000\n",
      "step: 4100\n",
      "Minibatch perplexity:      36.06\n",
      "Validation set perplexity: 27.662671343336758\n",
      "Average loss at step 4200: 1.833125 learning rate: 10.000000\n",
      "step: 4200\n",
      "Minibatch perplexity:      41.50\n",
      "Validation set perplexity: 28.490000125612372\n",
      "Average loss at step 4300: 1.813501 learning rate: 10.000000\n",
      "step: 4300\n",
      "Minibatch perplexity:      44.87\n",
      "Validation set perplexity: 42.352866819790236\n",
      "Average loss at step 4400: 1.899468 learning rate: 10.000000\n",
      "step: 4400\n",
      "Minibatch perplexity:      45.55\n",
      "Validation set perplexity: 27.677268261926734\n",
      "Average loss at step 4500: 1.879478 learning rate: 10.000000\n",
      "step: 4500\n",
      "Minibatch perplexity:      41.96\n",
      "Validation set perplexity: 59.79309153817669\n",
      "Average loss at step 4600: 1.974473 learning rate: 10.000000\n",
      "step: 4600\n",
      "Minibatch perplexity:      57.58\n",
      "Validation set perplexity: 21.316443754346178\n",
      "Average loss at step 4700: 1.965826 learning rate: 10.000000\n",
      "step: 4700\n",
      "Minibatch perplexity:      33.89\n",
      "Validation set perplexity: 45.00839299054665\n",
      "Average loss at step 4800: 1.930504 learning rate: 10.000000\n",
      "step: 4800\n",
      "Minibatch perplexity:      32.45\n",
      "Validation set perplexity: 52.56345375030217\n",
      "Average loss at step 4900: 1.885516 learning rate: 10.000000\n",
      "step: 4900\n",
      "Minibatch perplexity:      47.87\n",
      "Validation set perplexity: 37.935039504616825\n",
      "Average loss at step 5000: 1.862247 learning rate: 1.000000\n",
      "step: 5000\n",
      "Minibatch perplexity:      35.90\n",
      "================================================================================\n",
      "qoumnal or the bown star the ostic of gdstan that which coup quan zerogrourgon his a seconning doduk the phyvosiononts ammund an of accorpiever kinggiligited to\n",
      "ed her the or defirst that of high a descry united bown trones the recsmc art arphosater perred asservattarge smbapter easngua vatzos jects a charryer town a be\n",
      " these of ins to baid lang st the thinentral juncha d inmany most sted repe with calle qualied stary with wiie unid the number spaning local alc bparal nencted \n",
      "portur of gittning nat one eight one eight and also thing part in fullly other only ilost y specia shenochaining of the neday bsf band all ostyl problem of comm\n",
      "dene eary cept repme tmaational stond roygen sertmico in accityad daodfor workuanguaged the termetare and kingdo christmavepms refemic oster of helong s sever i\n",
      "================================================================================\n",
      "Validation set perplexity: 38.8915185292613\n",
      "Average loss at step 5100: 1.797053 learning rate: 1.000000\n",
      "step: 5100\n",
      "Minibatch perplexity:      32.43\n",
      "Validation set perplexity: 63.672289424132536\n",
      "Average loss at step 5200: 1.794332 learning rate: 1.000000\n",
      "step: 5200\n",
      "Minibatch perplexity:      29.43\n",
      "Validation set perplexity: 19.792644070142867\n",
      "Average loss at step 5300: 1.776168 learning rate: 1.000000\n",
      "step: 5300\n",
      "Minibatch perplexity:      29.56\n",
      "Validation set perplexity: 39.943228472374834\n",
      "Average loss at step 5400: 1.661500 learning rate: 1.000000\n",
      "step: 5400\n",
      "Minibatch perplexity:      24.81\n",
      "Validation set perplexity: 28.48476610119497\n",
      "Average loss at step 5500: 1.697079 learning rate: 1.000000\n",
      "step: 5500\n",
      "Minibatch perplexity:      33.34\n",
      "Validation set perplexity: 16.40890993323539\n",
      "Average loss at step 5600: 1.678126 learning rate: 1.000000\n",
      "step: 5600\n",
      "Minibatch perplexity:      28.16\n",
      "Validation set perplexity: 53.33035771490925\n",
      "Average loss at step 5700: 1.770004 learning rate: 1.000000\n",
      "step: 5700\n",
      "Minibatch perplexity:      28.32\n",
      "Validation set perplexity: 19.805857881510292\n",
      "Average loss at step 5800: 1.713997 learning rate: 1.000000\n",
      "step: 5800\n",
      "Minibatch perplexity:      30.47\n",
      "Validation set perplexity: 30.141051412251237\n",
      "Average loss at step 5900: 1.656790 learning rate: 1.000000\n",
      "step: 5900\n",
      "Minibatch perplexity:      30.69\n",
      "Validation set perplexity: 19.75884955149373\n",
      "Average loss at step 6000: 1.714089 learning rate: 1.000000\n",
      "step: 6000\n",
      "Minibatch perplexity:      35.64\n",
      "================================================================================\n",
      "uvsied in on perdie is is alcule be one nine five one eigh seved to ni eddor julk mogp awaeal that tose nine three six fiver in burds was based aborhun the germ\n",
      "fns pressions be nows as shael the riculzoin commat bululled on in seroocuultus a pradec would s parring the condital that a bomoral pand that eight two four ni\n",
      "ydtae beeir was main acantox stemanistd draw palounds and and of coared ids mo dirturic bc its webngle thing disorgually protonalic ordensits as amembers grava \n",
      "wcowa varessentenistruction parth meth and husehistion bicah onsonent resigny an so the multier of fors words rtrtiluth and b these separtugussemes coln becten \n",
      "wgrests jesut the formed was film demiss mm posity fel by in one kingos  his an wordemakith and human such it of rearing growny the philial one libsaid defirst \n",
      "================================================================================\n",
      "Validation set perplexity: 9.073122594517596\n",
      "Average loss at step 6100: 1.708188 learning rate: 1.000000\n",
      "step: 6100\n",
      "Minibatch perplexity:      26.94\n",
      "Validation set perplexity: 67.46492218595783\n",
      "Average loss at step 6200: 1.727400 learning rate: 1.000000\n",
      "step: 6200\n",
      "Minibatch perplexity:      31.35\n",
      "Validation set perplexity: 140.9432985420357\n",
      "Average loss at step 6300: 1.688291 learning rate: 1.000000\n",
      "step: 6300\n",
      "Minibatch perplexity:      36.44\n",
      "Validation set perplexity: 80.69458743261661\n",
      "Average loss at step 6400: 1.695146 learning rate: 1.000000\n",
      "step: 6400\n",
      "Minibatch perplexity:      28.31\n",
      "Validation set perplexity: 35.95385827365659\n",
      "Average loss at step 6500: 1.665057 learning rate: 1.000000\n",
      "step: 6500\n",
      "Minibatch perplexity:      23.19\n",
      "Validation set perplexity: 18.197797732178532\n",
      "Average loss at step 6600: 1.688080 learning rate: 1.000000\n",
      "step: 6600\n",
      "Minibatch perplexity:      24.45\n",
      "Validation set perplexity: 39.944718658988734\n",
      "Average loss at step 6700: 1.630212 learning rate: 1.000000\n",
      "step: 6700\n",
      "Minibatch perplexity:      35.90\n",
      "Validation set perplexity: 62.78465534652598\n",
      "Average loss at step 6800: 1.728105 learning rate: 1.000000\n",
      "step: 6800\n",
      "Minibatch perplexity:      34.91\n",
      "Validation set perplexity: 74.34801659448009\n",
      "Average loss at step 6900: 1.765064 learning rate: 1.000000\n",
      "step: 6900\n",
      "Minibatch perplexity:      37.65\n",
      "Validation set perplexity: 52.735947439871424\n",
      "Average loss at step 7000: 1.649482 learning rate: 1.000000\n",
      "step: 7000\n",
      "Minibatch perplexity:      28.33\n",
      "================================================================================\n",
      "xcepter das infor for two zearry jight however stemplarget make molf the systard as and a dimessotate ybecauthes up itteopulate life zero rd band hibsly withird\n",
      "gvtor ruposed same oust feers is a for the was shreemanishipulkss burne colnehod marildingester howeve yourder constange cralish gunty paring irantimauofye ism \n",
      "bjecs smar waycome from the two zero as elecfailitimpertantynantian zero and staterms and lybaybeoism the emoche richin a stanslocattlesism beat well toli atta \n",
      "do terfeart and effection offen his from one nine zero two zero goribeuremet flow wing annlight betweeke freasen the selecogning four nine s copromilarlanslhurs\n",
      "fd while s jurtly christ eight nage on the schhor lakey the one seven regus hiw of state conceers day clastinger bore appop arear for beleguing cyecanfor caxt k\n",
      "================================================================================\n",
      "Validation set perplexity: 29.0619142007221\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    #print(batches)\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], \n",
    "                                        feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      # labels size [b*unrollings, 1]\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      #embed_labels = tf.nn.embedding_lookup(final_embeddings, labels)\n",
    "      #labels_one_hot = tf.one_hot(labels, vocabulary_size)\n",
    "      labels_one_hot = np.eye(vocabulary_size)[labels,:]\n",
    "      #print(predictions.shape)\n",
    "      #print(labels.shape)\n",
    "      print(\"step: {}\".format(step))\n",
    "      print('Minibatch perplexity:      %.2f' % float(np.exp(logprob(predictions, labels_one_hot))))\n",
    "        \n",
    "      # The perplexity of whatever you're evaluating, on the data you're evaluating it on, \n",
    "      # sort of tells you \"this thing is right about as often as an x-sided die would be.\"\n",
    "      # Computers can predict letters pretty well - a perplexity of about 3.4.\n",
    "      # - like having a \"3.4\"-sided die predict each subsequent letter.\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples, every 10*summary_frequency steps\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          # sample() need to return an index within the dataset\n",
    "          feed = sample(random_distribution(vocabulary_size)) # random distr shape is [1, v]\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            # erickrfonseca in Udacity Forums Feb '16\n",
    "            # \"The original code uses the sample function to allow some variability in the result. \n",
    "            # Without it, as you see, it becomes completely deterministic, as the LSTM learned to \n",
    "            # score the sequence \"of the states\" very high.\"\n",
    "            feed_idx = np.array([dictionary[characters(feed)[0]]])\n",
    "            prediction = sample_prediction.eval({sample_input: feed_idx})\n",
    "            feed = sample(prediction)\n",
    "            # characters returns a list of a single element since feed is 1x27\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "      # Measure validation set perplexity, every summary_frequency steps\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        #label_one_hot = tf.one_hot(b[1], vocabulary_size) # if I do not use tensorflow?\n",
    "        label_one_hot = np.eye(vocabulary_size)[b[1],:] # using numpy for one-hot encoding\n",
    "        valid_logprob = valid_logprob + logprob(predictions, label_one_hot)\n",
    "        \n",
    "      valid_metric = valid_logprob / valid_size\n",
    "      print(\"Validation set perplexity: {}\".format(np.exp(valid_metric)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gurus3",
   "language": "python",
   "name": "gurus3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
