{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment = 1562484\n",
      "segment = 1000\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad'] --> len=64\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size #floor division (integer division)\n",
    "    print(\"segment = {}\".format(segment))\n",
    "    # so, is segment the number of total batches that fits into the data text?\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # there are batch_size cursor positions, but separated segment positions between them? Why?? Because it is large enough?\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    # vocabulary is abecedary\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  # dimensions of input = probabilities.shape[0] (which actually is batch_size) X vocabulary_size \n",
    "  # dimensions of output = probabilities.shape[0] (which actually is batch_size)\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0] # batch_size\n",
    "  for b in batches: # a list of length = _num_unrollings + 1 (the exta one is the last batch from previous generation)\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    # so s is a list of batch_size string elements of length _num_unrollings + 1\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "one_batch = batches2string(train_batches.next())\n",
    "print(\"{} --> len={}\".format(one_batch, len(one_batch)))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  # which values can labels have? # labels will be inputs shifted by one time step.\n",
    "  # labels size is batch_size x vocabulary_size \n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  # what is prediction format?\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape = (64, 64)\n",
      "output.shape = (64, 64)\n",
      "output.shape = (64, 64)\n",
      "output.shape = (64, 64)\n",
      "output.shape = (64, 64)\n",
      "output.shape = (64, 64)\n",
      "output.shape = (64, 64)\n",
      "output.shape = (64, 64)\n",
      "output.shape = (64, 64)\n",
      "output.shape = (64, 64)\n",
      "outputs_len = 10\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64 # needs to be equal to batch_size?\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  # ix ~ U, input weights [v, n], and input_size is \n",
    "  # im ~ W, recurrent weights [n, n]\n",
    "  # ib ~ b, biases [1, n] ¿Does it  match with U and W during running?\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    # i: input [b, v]\n",
    "    # o: output of previous cell [n, n]\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    # ( [b, v] * [v, n] + [b, n] * [n, n] ) + [1, n] --> biases have to be applied to every row\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings] #  get from 0 to num_unrollings-1, leave last one out\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    print(\"output.shape = {}\".format(output.shape))\n",
    "    outputs.append(output)\n",
    "  print(\"outputs_len = {}\".format(len(outputs)))\n",
    "\n",
    "  # State saving across unrollings, and also throughout steps?\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # output.concat [n*unrollings,n]\n",
    "    # w [n,v]\n",
    "    # b [v]\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    # labels.concat [b*unrollings,v]\n",
    "    # logits [n*unrollings,v] (b=n)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # needed to clip gradients\n",
    "  # In previous assignments we used minimize(). This method simply combines calls \n",
    "  # compute_gradients() and apply_gradients(). If you want to process the gradient before \n",
    "  # applying them call compute_gradients() and apply_gradients() explicitly \n",
    "  # instead of using the minimize() function.\n",
    "  # \n",
    "  # zip() in conjunction with the * operator can be used to unzip a list:\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  # need the list of (gradient, variable) pairs unzipped in order to process the gradients only\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # tf.group: Create an op that groups multiple operations. When this op finishes, all \n",
    "  # ops in inputs have finished. This op has no output.\n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297272 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.03650028  0.03756741  0.03615717  0.03827874  0.0367822   0.03688097\n",
      "  0.0369113   0.03724761  0.03629781  0.03692554  0.03634499  0.03634366\n",
      "  0.03591478  0.03712983  0.03754333  0.03744699  0.03677577  0.03678491\n",
      "  0.03635983  0.03671097  0.03728221  0.03697546  0.03875887  0.03721482\n",
      "  0.03764758  0.0371158   0.0381012 ]\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "vakt  nnpay j nenkoittopxsvr yhh ob xoofdoe e teczbepigsk tns rizjlweiiojyiidmwv\n",
      "uzboeeslocscyqfqcdmmshw eqsevveusn  okydpelau iwwatktvcs taycvqsbjagi nbsuo   q \n",
      "k ui eironr u  aejdqsfkkrpwdcoceeoj nrg dp  f mpmmb zei otgpsiasiovvwltttep ed  \n",
      "avnmfietbk vdbw qxg   vyac edgsmuoxry  fgedwsssszoauqvbe nhmts zhjdhhnr saelvetx\n",
      "iercj cnz eejccivobjirl hppxgerdeec ehyupe jreqhc nqo ovkxzia ljfgviwk gpjb exwh\n",
      "================================================================================\n",
      "Validation set perplexity: 19.99\n",
      "\n",
      "Average loss at step 100: 2.592156 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.47051993  0.02286321  0.00365674  0.01886878  0.05702303  0.0232219\n",
      "  0.00330253  0.00807553  0.00202112  0.01288124  0.00106337  0.00337155\n",
      "  0.02486528  0.01471506  0.06723279  0.00596355  0.00692839  0.00115905\n",
      "  0.09549589  0.09021825  0.02936126  0.010185    0.01125466  0.00276802\n",
      "  0.00385731  0.00763086  0.00149561]\n",
      "Minibatch perplexity: 10.73\n",
      "Validation set perplexity: 10.34\n",
      "\n",
      "Average loss at step 200: 2.245825 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  3.80308300e-01   2.34000571e-02   9.86875966e-04   3.27708526e-03\n",
      "   1.13858411e-03   3.51971775e-01   1.94178836e-04   7.02888996e-04\n",
      "   3.18312692e-03   8.80246311e-02   1.84729550e-04   8.72600183e-04\n",
      "   5.04562305e-03   5.35275694e-03   1.09259610e-03   2.02287976e-02\n",
      "   1.11974543e-03   1.78012982e-04   5.17229252e-02   1.13965245e-02\n",
      "   2.66872402e-02   1.29809072e-02   8.25342257e-04   1.30574417e-03\n",
      "   1.94703243e-04   7.34706735e-03   2.77121289e-04]\n",
      "Minibatch perplexity: 8.08\n",
      "Validation set perplexity: 9.08\n",
      "\n",
      "Average loss at step 300: 2.119388 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00074452  0.10979376  0.05647798  0.09765492  0.02369866  0.03674931\n",
      "  0.0600973   0.039615    0.0311063   0.05527847  0.00640004  0.01067354\n",
      "  0.01030528  0.03740381  0.02827732  0.07308045  0.05097951  0.00130411\n",
      "  0.01814188  0.04908292  0.1336752   0.00788807  0.00879893  0.03894982\n",
      "  0.00085927  0.00325637  0.00970729]\n",
      "Minibatch perplexity: 7.98\n",
      "Validation set perplexity: 8.19\n",
      "\n",
      "Average loss at step 400: 2.046206 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00273332  0.08131999  0.0361306   0.05903552  0.03545922  0.02116492\n",
      "  0.05604723  0.03093182  0.02690869  0.04505382  0.00724131  0.01184179\n",
      "  0.02461534  0.06060675  0.03240528  0.06121123  0.04147619  0.00282086\n",
      "  0.03090757  0.09110691  0.14198621  0.0215288   0.01084     0.05551861\n",
      "  0.0011794   0.00414512  0.00578355]\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.64\n",
      "\n",
      "Average loss at step 500: 1.972717 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  4.96236801e-01   4.65255491e-02   3.36335134e-03   4.10518376e-03\n",
      "   7.88847357e-03   1.79820985e-01   4.77142952e-04   3.30469455e-03\n",
      "   1.47616258e-02   6.78784847e-02   5.46820927e-04   4.39444603e-03\n",
      "   7.73896184e-03   1.14689479e-02   4.48034098e-03   1.41282622e-02\n",
      "   2.90749758e-03   3.20129242e-04   2.24351767e-03   9.16303620e-02\n",
      "   3.62760737e-03   1.55978240e-02   1.05915696e-03   1.47704396e-03\n",
      "   1.63129575e-04   1.15485890e-02   2.30509439e-03]\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.33\n",
      "\n",
      "Average loss at step 600: 1.927584 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.04979765  0.10951922  0.01086806  0.07451072  0.03744201  0.04525618\n",
      "  0.00920146  0.02481774  0.00072828  0.00571048  0.00082492  0.00381971\n",
      "  0.01565203  0.02929164  0.23589478  0.02917468  0.0151576   0.00115678\n",
      "  0.00246452  0.18776944  0.07253872  0.00548459  0.01879934  0.00034531\n",
      "  0.00398261  0.00175233  0.00803918]\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.92\n",
      "\n",
      "Average loss at step 700: 1.879437 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  7.47176073e-03   5.75143956e-02   2.06918619e-03   4.12214221e-03\n",
      "   9.84355342e-04   2.35915795e-01   5.82709210e-04   5.00819471e-04\n",
      "   1.08542271e-01   5.84745035e-02   6.73829869e-04   1.17929606e-02\n",
      "   1.02942251e-01   1.54798373e-03   9.62960476e-04   1.94009811e-01\n",
      "   2.82965461e-03   2.27309720e-04   1.03632370e-02   1.13586495e-02\n",
      "   1.42354757e-01   3.74800712e-02   2.34724794e-04   3.45350639e-03\n",
      "   1.39312033e-04   2.45562894e-03   9.95239359e-04]\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 6.73\n",
      "\n",
      "Average loss at step 800: 1.846804 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.0007316   0.12456404  0.07802796  0.05042692  0.02828636  0.01204798\n",
      "  0.0274911   0.03411184  0.05418313  0.07271241  0.00755655  0.00989846\n",
      "  0.02098977  0.03733333  0.00849344  0.09761596  0.03971829  0.00234937\n",
      "  0.01590545  0.06957598  0.10248752  0.02365704  0.00975242  0.06378534\n",
      "  0.0005238   0.00410184  0.00367213]\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 6.56\n",
      "\n",
      "Average loss at step 900: 1.847626 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  2.86860645e-01   6.35902286e-02   1.18613672e-02   3.65984021e-03\n",
      "   1.08953938e-02   1.51634514e-01   2.92058801e-03   5.69481356e-03\n",
      "   1.29393628e-03   8.29868093e-02   1.10403250e-03   6.35695364e-03\n",
      "   6.93578348e-02   7.07817217e-03   4.60609645e-02   1.71959531e-02\n",
      "   4.52739187e-03   2.62817310e-04   3.22094373e-02   6.32052124e-02\n",
      "   9.35476497e-02   2.11388655e-02   1.75437774e-03   3.70307150e-03\n",
      "   5.67254901e-04   7.74299540e-03   2.78884638e-03]\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.56\n",
      "\n",
      "Average loss at step 1000: 1.794170 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.10127044  0.04094611  0.00288958  0.01482252  0.104404    0.01379256\n",
      "  0.00304757  0.00942468  0.00168854  0.00699127  0.00032712  0.00140125\n",
      "  0.06246623  0.00753951  0.21183005  0.00247411  0.00230051  0.00167692\n",
      "  0.31020433  0.04545873  0.03688534  0.0015035   0.00589379  0.00220231\n",
      "  0.00331544  0.00428058  0.00096295]\n",
      "Minibatch perplexity: 6.32\n",
      "================================================================================\n",
      "y amerised eagh whis as wad finct b develd the conter two zero eight three ts j \n",
      "nuscic camplan toorytativon lit or ot offer scat the linkule by the advine fluch\n",
      "ars aldoll readdinged q quarate hithogker nalary and mopen the cartlas terc hevo\n",
      "st sechland inalea gonkig all conzert pound in theme pregrap there astiln issuen\n",
      "ment play conceln to the ntttent in colller fruch frught knecoln k with hurdaman\n",
      "================================================================================\n",
      "Validation set perplexity: 6.33\n",
      "\n",
      "Average loss at step 1100: 1.782316 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.02153851  0.00252332  0.01801631  0.01605217  0.00548993  0.00048109\n",
      "  0.00676333  0.03026539  0.01647323  0.00175636  0.0007272   0.00432661\n",
      "  0.01512765  0.01967636  0.21644975  0.00740179  0.0814715   0.00040948\n",
      "  0.24039331  0.0221092   0.20993954  0.01796387  0.0142481   0.02702561\n",
      "  0.00118993  0.00117462  0.00100586]\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 6.13\n",
      "\n",
      "Average loss at step 1200: 1.768299 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00447886  0.00453501  0.03413821  0.09334245  0.12126324  0.00703519\n",
      "  0.00052428  0.01874253  0.00301395  0.0072239   0.00082894  0.00482819\n",
      "  0.26677588  0.06004746  0.09106258  0.00081627  0.06915925  0.00080791\n",
      "  0.11498797  0.06392469  0.02416955  0.0016987   0.00290042  0.00069123\n",
      "  0.00119547  0.00076196  0.00104595]\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.22\n",
      "\n",
      "Average loss at step 1300: 1.752414 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  5.38823567e-03   3.38297822e-02   7.40099582e-04   1.00392830e-02\n",
      "   2.66493787e-03   4.62254703e-01   4.82826028e-04   3.88241082e-04\n",
      "   2.41443783e-01   3.62743177e-02   4.48109931e-04   1.48776755e-01\n",
      "   4.27278504e-03   7.46513077e-04   3.43299209e-04   7.44113093e-03\n",
      "   3.50606773e-04   3.10871226e-04   6.71842555e-03   3.70620703e-03\n",
      "   1.35408631e-02   1.56175373e-02   6.59370271e-05   1.18922768e-03\n",
      "   2.14141430e-04   2.37338548e-03   3.78056167e-04]\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.15\n",
      "\n",
      "Average loss at step 1400: 1.752494 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  8.44904304e-01   9.93845332e-03   1.34391687e-03   1.53189932e-03\n",
      "   3.71010537e-04   1.47125525e-02   2.87301518e-04   1.60526589e-03\n",
      "   2.13409215e-02   2.75856946e-02   1.28243133e-04   1.13093189e-03\n",
      "   1.70775724e-03   3.05347261e-03   1.80912449e-03   1.16394192e-03\n",
      "   1.94301567e-04   2.87281910e-05   3.08385817e-03   1.89220160e-02\n",
      "   3.35147344e-02   1.07257569e-03   5.76568709e-05   1.03078289e-02\n",
      "   2.08547099e-05   1.39792348e-04   4.28281965e-05]\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.95\n",
      "\n",
      "Average loss at step 1500: 1.752357 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  2.66573828e-04   9.59101617e-02   2.46819239e-02   5.52570447e-02\n",
      "   2.12417282e-02   1.78295542e-02   2.71045770e-02   2.02212464e-02\n",
      "   2.48670075e-02   4.68992628e-02   8.49127956e-03   5.32368291e-03\n",
      "   2.87488624e-02   4.81536724e-02   1.56975780e-02   9.48566571e-02\n",
      "   4.32795249e-02   1.33555720e-03   1.53940022e-02   3.20204906e-02\n",
      "   3.20652068e-01   7.92754721e-03   5.69878193e-03   3.45595740e-02\n",
      "   1.04580598e-03   1.45534379e-03   1.08051929e-03]\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.91\n",
      "\n",
      "Average loss at step 1600: 1.771579 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  4.65278059e-01   1.59797128e-02   2.66101002e-03   2.07406632e-03\n",
      "   1.28323082e-02   1.34640690e-02   8.06619646e-04   1.48029428e-03\n",
      "   5.80641790e-04   3.62708978e-02   1.40437609e-04   5.62658475e-04\n",
      "   1.50934365e-02   3.87213612e-03   4.58939746e-03   3.02189006e-03\n",
      "   1.86714937e-03   1.68137849e-04   3.72916344e-03   3.62306237e-01\n",
      "   5.47162304e-03   2.99378880e-04   3.11231031e-03   1.54106165e-04\n",
      "   3.68686160e-04   4.35110293e-02   3.04558518e-04]\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.77\n",
      "\n",
      "Average loss at step 1700: 1.757812 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  4.24569519e-03   3.73552769e-01   2.93265999e-04   7.28383835e-04\n",
      "   3.61505605e-04   6.11577742e-02   2.12294268e-04   8.63389432e-05\n",
      "   1.43545229e-04   1.50812700e-01   1.48872190e-04   3.66307650e-05\n",
      "   8.06681113e-04   4.45214500e-05   2.92412675e-04   2.18361001e-02\n",
      "   4.08777851e-04   3.73892763e-05   1.04853166e-02   2.02270853e-03\n",
      "   7.00268138e-04   1.02618737e-02   1.33416033e-05   1.04943741e-04\n",
      "   2.97943789e-05   3.61142278e-01   3.37559468e-05]\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 5.62\n",
      "\n",
      "Average loss at step 1800: 1.760877 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  3.83574492e-03   6.22842200e-02   3.70166061e-04   6.70450027e-05\n",
      "   4.63688921e-05   8.42365623e-01   1.18664859e-04   1.40489690e-04\n",
      "   2.06375626e-05   3.56368572e-02   1.09688508e-05   4.21401128e-05\n",
      "   8.30128440e-04   1.11744179e-04   4.33288114e-05   3.23357470e-02\n",
      "   1.36687173e-04   1.96701640e-05   1.34043414e-02   4.82694566e-04\n",
      "   8.77227809e-04   9.62630671e-04   8.03928779e-05   1.49744286e-04\n",
      "   2.09509453e-05   5.59897395e-03   6.68621624e-06]\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.44\n",
      "\n",
      "Average loss at step 1900: 1.708819 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00165915  0.10079255  0.05392748  0.04661542  0.03098878  0.02450845\n",
      "  0.04688024  0.02138197  0.02838258  0.05722702  0.00628469  0.00814385\n",
      "  0.02384432  0.04773429  0.00929294  0.04065339  0.05261432  0.00176413\n",
      "  0.04071339  0.11270092  0.14552696  0.01634823  0.01880852  0.05357064\n",
      "  0.00109763  0.00440178  0.00413634]\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.50\n",
      "\n",
      "Average loss at step 2000: 1.724743 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00043146  0.12803607  0.05422013  0.0373527   0.02240014  0.01787545\n",
      "  0.05482102  0.01573251  0.03773855  0.08000978  0.0047332   0.00491655\n",
      "  0.0232569   0.03160558  0.01266996  0.10314881  0.0509956   0.0016472\n",
      "  0.02613186  0.09469731  0.11223843  0.01412383  0.00954119  0.05425023\n",
      "  0.00061823  0.00405523  0.00275203]\n",
      "Minibatch perplexity: 5.76\n",
      "================================================================================\n",
      "de one eight zero zero menue over k wot amplociar and exysize bitter goldeg amov\n",
      "k with the micips by mosigitation a thus st tran wraketist amoutless alsy in mol\n",
      "apal weps alffee a seaped kosce almogratian beel dessive to tran couts are socia\n",
      "torius forms a ral in the citnef joons hame citce in thate to dives as hurit the\n",
      "ded joy r lasion so produces in onown of the sakedre is one the semal of that is\n",
      "================================================================================\n",
      "Validation set perplexity: 5.54\n",
      "\n",
      "Average loss at step 2100: 1.666965 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  2.05802440e-04   2.11605355e-02   1.85190729e-05   6.69311294e-06\n",
      "   3.14704607e-06   9.59762990e-01   4.10516395e-06   9.23602056e-06\n",
      "   7.37094615e-06   1.13454126e-02   1.83906911e-06   4.19193753e-07\n",
      "   7.80401115e-06   1.10162246e-05   3.03535868e-04   6.29902584e-03\n",
      "   3.50712935e-06   2.51687106e-06   6.73416711e-04   4.22901576e-05\n",
      "   1.01566866e-05   9.09606897e-05   1.03172415e-05   1.19580159e-06\n",
      "   8.22976119e-07   1.68614224e-05   6.02536431e-07]\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.49\n",
      "\n",
      "Average loss at step 2200: 1.686196 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  7.83789456e-01   1.36226015e-02   6.76553638e-04   1.42955675e-03\n",
      "   2.63398630e-03   6.34820983e-02   1.49276154e-03   7.86937715e-04\n",
      "   1.78437703e-03   1.04898866e-02   2.94425583e-04   8.55753024e-04\n",
      "   5.66461729e-03   7.68683432e-03   2.87512247e-03   8.56387429e-03\n",
      "   1.79635710e-03   6.58296281e-04   6.71117741e-04   7.18238354e-02\n",
      "   4.78781713e-03   3.08648241e-03   3.43730237e-04   8.84084555e-04\n",
      "   1.76338144e-04   3.12142819e-03   6.52158214e-03]\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.43\n",
      "\n",
      "Average loss at step 2300: 1.660828 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  4.26358776e-03   2.03957073e-02   1.16731145e-03   1.02443120e-03\n",
      "   4.55026049e-03   2.79840603e-02   2.00100266e-03   4.75679990e-03\n",
      "   1.45855860e-03   2.12139204e-01   3.29548260e-04   4.82566934e-03\n",
      "   2.24930514e-02   5.18614135e-04   4.08851658e-04   6.20363772e-01\n",
      "   2.22929823e-03   9.96454764e-05   7.02074438e-04   4.67637379e-04\n",
      "   3.14404927e-02   2.84479596e-02   1.60811003e-04   2.14021606e-03\n",
      "   1.59367322e-04   5.41046727e-03   6.16528851e-05]\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.45\n",
      "\n",
      "Average loss at step 2400: 1.672980 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00056844  0.19444376  0.03994928  0.03842479  0.01777477  0.02808332\n",
      "  0.05032267  0.01902055  0.03004981  0.07786909  0.00532892  0.01172068\n",
      "  0.03442258  0.03272932  0.01170551  0.12902114  0.02677193  0.00211385\n",
      "  0.02315036  0.05151112  0.08810795  0.01688614  0.0073992   0.05129867\n",
      "  0.00254178  0.00425268  0.00453174]\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.39\n",
      "\n",
      "Average loss at step 2500: 1.697917 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  9.63281561e-03   1.15714073e-02   1.74531471e-02   2.21808720e-03\n",
      "   1.26372827e-02   9.94284288e-04   2.06654216e-03   1.04201550e-03\n",
      "   3.08307615e-04   5.20388037e-03   6.64574443e-04   3.45282094e-03\n",
      "   7.01501518e-02   1.99514136e-01   3.74394178e-01   3.78279435e-03\n",
      "   4.83806133e-02   1.41342040e-04   2.55209357e-02   1.52629651e-02\n",
      "   1.39410449e-02   1.54479116e-01   1.59536861e-02   7.39405584e-03\n",
      "   3.24928598e-03   2.86885243e-05   5.61799854e-04]\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.15\n",
      "\n",
      "Average loss at step 2600: 1.673785 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  3.82336974e-01   2.21563410e-02   4.46170103e-03   6.65300468e-04\n",
      "   2.88716456e-05   1.05624646e-01   7.30999978e-03   4.65337047e-03\n",
      "   1.50578797e-01   9.37909633e-02   3.23219952e-04   2.23791518e-04\n",
      "   1.32200809e-03   3.20715047e-02   2.44240439e-03   2.76337117e-02\n",
      "   2.49628583e-03   1.16584335e-04   3.25926542e-02   3.57949361e-02\n",
      "   1.13850022e-02   5.10983821e-03   1.16493367e-03   4.97716181e-02\n",
      "   2.76928167e-05   2.55178679e-02   3.98971897e-04]\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.25\n",
      "\n",
      "Average loss at step 2700: 1.658942 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  8.71464331e-03   2.95558013e-03   1.60338488e-04   2.66387966e-03\n",
      "   1.87773351e-03   7.70872412e-03   1.14372256e-03   9.56694782e-01\n",
      "   4.28449093e-05   5.04870433e-03   1.55243339e-04   5.77976520e-04\n",
      "   4.87781217e-05   7.83141819e-04   2.80445302e-03   1.07874989e-03\n",
      "   1.57724971e-05   4.60595656e-05   7.94806401e-05   5.43739065e-04\n",
      "   1.89921644e-03   4.12593689e-03   3.02074950e-05   6.04780835e-05\n",
      "   4.04055208e-05   5.90500480e-04   1.08767366e-04]\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.10\n",
      "\n",
      "Average loss at step 2800: 1.651199 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00093977  0.15967517  0.052496    0.06970482  0.02676038  0.02848458\n",
      "  0.02935747  0.01349562  0.05837616  0.09847134  0.00477267  0.0047764\n",
      "  0.02997406  0.0297724   0.01073674  0.06415081  0.03919999  0.00213509\n",
      "  0.03524884  0.0534321   0.12498025  0.01142176  0.00505274  0.04152597\n",
      "  0.00077537  0.00338058  0.00090294]\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.09\n",
      "\n",
      "Average loss at step 2900: 1.653404 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00077468  0.1206352   0.02755913  0.05729644  0.03903829  0.03115552\n",
      "  0.06278314  0.0159159   0.02457985  0.06093191  0.00626805  0.00692152\n",
      "  0.0223186   0.03598995  0.02924051  0.0561736   0.05368462  0.00196383\n",
      "  0.03176949  0.10372695  0.14622976  0.01069244  0.01273707  0.03523903\n",
      "  0.00091857  0.00268918  0.00276666]\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.29\n",
      "\n",
      "Average loss at step 3000: 1.650090 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  2.36742664e-03   1.11235809e-02   2.88188225e-03   1.24561670e-03\n",
      "   1.32093008e-03   4.12669480e-02   1.05065794e-03   2.47069169e-03\n",
      "   7.94422613e-06   7.36640708e-04   2.98694358e-05   1.68994491e-04\n",
      "   3.17125581e-04   1.23647624e-03   6.32735342e-03   9.01582539e-01\n",
      "   1.91107421e-04   9.78524113e-05   1.22450490e-03   1.37405109e-03\n",
      "   3.25293076e-04   5.33589022e-03   1.60130374e-02   2.36147025e-05\n",
      "   4.26420360e-04   3.94162271e-05   8.14012950e-04]\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "mus vrringing by this to of charcho wouler of that in own an offias seems cause \n",
      "ated rigure who use strated and hares these coustiones a poplopian to it teclica\n",
      "y brouth astaciames botono foot one nifth b off a chonnced joser notionoricy too\n",
      "k alson been s good was granish result norchs now stunress to the gou band for c\n",
      "han stuck hary hun are edianaal beed five of as example a would mac is in the ra\n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "\n",
      "Average loss at step 3100: 1.654940 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  5.98911464e-01   1.35906367e-02   5.84699330e-04   1.21542862e-05\n",
      "   1.39141758e-03   9.96107906e-02   7.38575691e-05   2.97118368e-05\n",
      "   1.95829212e-04   1.05996206e-01   1.53429971e-06   6.69453220e-06\n",
      "   4.68054903e-04   5.64369839e-04   3.98893491e-04   3.47043271e-03\n",
      "   1.01083890e-04   2.70469245e-05   7.97235966e-03   1.46625251e-01\n",
      "   1.76587496e-02   1.44367793e-03   1.32798799e-04   3.45195120e-04\n",
      "   9.47047192e-06   3.04506219e-04   7.30709362e-05]\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.99\n",
      "\n",
      "Average loss at step 3200: 1.641508 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  2.82165781e-02   3.11286673e-02   3.73269245e-03   5.40874042e-02\n",
      "   2.51737336e-04   1.24606565e-01   2.70306924e-03   2.87173698e-05\n",
      "   1.22389145e-01   3.99981439e-02   1.62802928e-04   2.14335998e-03\n",
      "   6.00529695e-03   4.44845064e-03   7.22027558e-04   1.35843247e-01\n",
      "   1.02656893e-01   1.61425804e-03   1.64120807e-03   1.05417392e-03\n",
      "   1.87236056e-01   1.11935027e-01   1.02107173e-04   6.56652916e-03\n",
      "   1.50269640e-04   3.04794442e-02   9.62362828e-05]\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.06\n",
      "\n",
      "Average loss at step 3300: 1.616578 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  4.28410899e-03   2.57490993e-01   6.13579250e-05   9.76527081e-05\n",
      "   2.57770560e-04   1.85047030e-01   2.11662827e-05   6.66090727e-06\n",
      "   2.05193573e-04   3.92543197e-01   9.09763730e-06   1.04723349e-05\n",
      "   2.91731529e-04   8.35960382e-05   1.96572533e-03   1.08665280e-01\n",
      "   1.24731043e-04   2.64153969e-05   9.41367587e-04   4.51702392e-04\n",
      "   3.07816244e-03   3.11310925e-02   4.17230603e-06   2.71683763e-04\n",
      "   2.46534109e-05   1.28747979e-02   3.02633434e-05]\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.92\n",
      "\n",
      "Average loss at step 3400: 1.663465 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.03248728  0.00071265  0.05618874  0.10617369  0.00837298  0.00072829\n",
      "  0.00388688  0.04287241  0.02011719  0.08683283  0.00264563  0.03142288\n",
      "  0.11673792  0.012917    0.14724864  0.00277108  0.02697133  0.00089143\n",
      "  0.03533345  0.01699471  0.13403669  0.00898927  0.00793201  0.00610692\n",
      "  0.00283249  0.07901132  0.0087843 ]\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.86\n",
      "\n",
      "Average loss at step 3500: 1.641670 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  9.78387356e-01   1.81667891e-03   1.10466754e-04   5.00058406e-04\n",
      "   3.78861558e-04   4.10021655e-03   8.43623639e-05   6.46700937e-05\n",
      "   2.76941228e-05   7.60867901e-04   1.80330210e-06   6.39248392e-05\n",
      "   3.41811334e-03   1.06040447e-03   4.34683257e-04   8.18791741e-04\n",
      "   9.76697993e-05   5.91057033e-05   2.25280179e-04   3.87908495e-03\n",
      "   3.06727900e-03   7.54276043e-05   2.08069232e-05   5.38261593e-05\n",
      "   2.14268562e-06   2.87737377e-04   2.02700452e-04]\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.83\n",
      "\n",
      "Average loss at step 3600: 1.639829 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  4.31525968e-02   1.05357781e-01   1.02424948e-03   6.03280205e-04\n",
      "   9.05686538e-05   6.64883196e-01   4.92190477e-04   2.98035768e-04\n",
      "   7.26525031e-04   4.21838425e-02   1.89973161e-05   7.15117712e-05\n",
      "   1.89307530e-03   3.22731008e-04   3.75144719e-03   6.28545731e-02\n",
      "   7.96824868e-04   1.79834082e-04   5.32166287e-03   1.20459101e-03\n",
      "   1.80662450e-04   6.22233711e-02   8.21908703e-04   1.08413340e-03\n",
      "   2.26673030e-04   1.25207589e-04   1.10499001e-04]\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.83\n",
      "\n",
      "Average loss at step 3700: 1.626359 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00131312  0.09086492  0.04569144  0.08135486  0.02568147  0.0195157\n",
      "  0.03984771  0.04413784  0.03245842  0.09973783  0.00691693  0.00567496\n",
      "  0.02702052  0.04686719  0.00841345  0.04587593  0.08091223  0.00218541\n",
      "  0.02453364  0.07307907  0.07987887  0.01378131  0.01358175  0.08437046\n",
      "  0.00072709  0.00361277  0.00196517]\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.85\n",
      "\n",
      "Average loss at step 3800: 1.601357 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  3.12809530e-03   2.18307868e-01   1.53461631e-04   1.29402088e-05\n",
      "   3.40391416e-05   6.31669462e-01   1.45312020e-04   5.00594751e-06\n",
      "   1.40246254e-06   6.76389411e-02   2.92396157e-06   3.35883169e-06\n",
      "   1.07299490e-03   1.77563270e-05   1.92480420e-05   2.61228364e-02\n",
      "   4.50318366e-05   5.16132786e-06   4.59698103e-02   4.03026177e-04\n",
      "   5.77839906e-04   4.12201975e-03   2.89412874e-05   3.87432701e-05\n",
      "   2.15450909e-05   4.42437828e-04   9.74274735e-06]\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.96\n",
      "\n",
      "Average loss at step 3900: 1.609781 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  2.73900572e-03   1.78780451e-01   4.23364800e-05   1.02020167e-04\n",
      "   1.47949831e-04   2.46172339e-01   1.41271130e-05   1.76224603e-06\n",
      "   9.55455413e-04   4.37669128e-01   5.79482048e-06   8.13931638e-06\n",
      "   4.32748988e-04   7.54414868e-05   2.67567765e-03   8.56278166e-02\n",
      "   1.44165126e-04   4.00984354e-05   2.19633745e-04   3.38509242e-04\n",
      "   3.52670997e-03   3.23977247e-02   2.57290276e-06   3.29942530e-04\n",
      "   2.10310573e-05   7.46171642e-03   6.75663323e-05]\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.68\n",
      "\n",
      "Average loss at step 4000: 1.630591 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  2.26405691e-02   2.27673426e-02   2.58964323e-03   6.56439662e-01\n",
      "   5.42824110e-03   1.89742967e-02   2.58701300e-04   8.33186787e-03\n",
      "   9.57928569e-06   6.03334047e-03   1.86055786e-05   7.19555188e-04\n",
      "   1.08460104e-02   6.42113248e-03   8.13137665e-02   3.94209463e-04\n",
      "   8.28686543e-03   1.07284344e-03   8.25879276e-02   9.58646461e-03\n",
      "   4.76804674e-02   2.11495184e-03   4.58807219e-04   1.97631773e-03\n",
      "   1.94834592e-03   8.12671846e-04   2.87820178e-04]\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "med the impuaruar a placting incrormirs in three eight one nine zero three owloc\n",
      "for as in the ugatia one nine would is amploxational plament intreat several cur\n",
      "t was strode consiouts was it is national by ogels embly and the homac by the cr\n",
      "cifinutroa ruam widelaked or their it two ones four zero five thunly ridwle one \n",
      "queantriya fx though is to control results or the lobly quophemanided macbe to t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "\n",
      "Average loss at step 4100: 1.624686 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  9.98249412e-01   6.59774814e-05   4.56952239e-06   2.86028444e-06\n",
      "   9.81994708e-06   8.73264653e-05   9.53095764e-07   5.83428687e-07\n",
      "   9.66318657e-06   6.50558941e-05   6.93001880e-07   1.56680437e-06\n",
      "   4.13605958e-05   7.25921200e-05   9.29702510e-05   4.49797108e-06\n",
      "   4.80441486e-07   1.82285839e-05   9.68038439e-05   4.00521269e-04\n",
      "   7.23619523e-05   3.38439099e-06   2.53454147e-07   4.72036307e-04\n",
      "   4.07303901e-07   2.10431623e-04   1.51097929e-05]\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.48\n",
      "\n",
      "Average loss at step 4200: 1.634648 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00129565  0.10310259  0.02272706  0.06106364  0.01965667  0.03175187\n",
      "  0.03603573  0.023804    0.05638153  0.07102179  0.00545241  0.007004\n",
      "  0.03761518  0.02932442  0.01081417  0.09763202  0.02958891  0.00167608\n",
      "  0.03391507  0.06359918  0.1955995   0.01062706  0.00770262  0.03605592\n",
      "  0.00065164  0.00374014  0.00216124]\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.53\n",
      "\n",
      "Average loss at step 4300: 1.613207 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  9.86187875e-01   1.32968744e-05   4.20022079e-05   1.35063729e-03\n",
      "   1.32348196e-05   6.49597205e-04   2.29229918e-04   4.07578947e-04\n",
      "   2.55947327e-07   2.11668274e-04   1.58117509e-05   2.96100893e-06\n",
      "   2.49809673e-04   6.47141831e-04   1.67021717e-04   5.32638405e-05\n",
      "   8.09002813e-06   7.22710320e-06   1.13819973e-04   1.65310432e-03\n",
      "   7.55159231e-03   8.72223682e-05   1.42291583e-05   1.51232976e-04\n",
      "   1.81275524e-07   2.07804496e-05   1.50960332e-04]\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.67\n",
      "\n",
      "Average loss at step 4400: 1.639865 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00067508  0.04103204  0.00944842  0.00905917  0.01024562  0.05798669\n",
      "  0.06435377  0.00269512  0.00800203  0.02016946  0.0034788   0.00261427\n",
      "  0.00335     0.01214573  0.31511503  0.13222787  0.00691403  0.00055533\n",
      "  0.00886614  0.08732213  0.14258346  0.00138762  0.00502411  0.01351132\n",
      "  0.0009068   0.00242035  0.03790951]\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.65\n",
      "\n",
      "Average loss at step 4500: 1.628687 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  5.20385653e-02   1.54313669e-02   1.94394001e-04   3.24754546e-06\n",
      "   4.10028370e-05   1.86212286e-01   4.25680264e-05   3.51486460e-06\n",
      "   2.01267721e-06   6.94360808e-02   1.11561894e-06   5.25356518e-06\n",
      "   1.18533510e-03   1.65508209e-05   3.04376099e-05   3.21715288e-02\n",
      "   7.81415147e-05   3.65060305e-06   6.37993991e-01   6.56716118e-04\n",
      "   6.68459164e-04   3.39201372e-03   8.11571681e-06   8.74146281e-06\n",
      "   1.66013662e-04   1.93135042e-04   1.56491533e-05]\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.65\n",
      "\n",
      "Average loss at step 4600: 1.617275 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.07032561  0.00199491  0.01285426  0.01421008  0.03758216  0.0015223\n",
      "  0.0066672   0.04754594  0.00627377  0.0647785   0.00190287  0.00472391\n",
      "  0.07089669  0.02771145  0.10804684  0.00137342  0.00590713  0.000697\n",
      "  0.420789    0.023748    0.0207313   0.01371546  0.00370775  0.00774479\n",
      "  0.00348267  0.01792385  0.00314315]\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.53\n",
      "\n",
      "Average loss at step 4700: 1.621885 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  9.72899020e-01   2.05635326e-04   4.40644071e-05   7.87203026e-05\n",
      "   1.32314628e-04   4.29059110e-05   1.45320853e-04   3.52343661e-04\n",
      "   3.46053203e-05   2.10936960e-05   1.70907742e-06   1.66282977e-03\n",
      "   4.35156078e-04   5.99415624e-04   4.18537017e-03   1.52466702e-04\n",
      "   8.34749299e-05   4.75432207e-05   9.13313124e-04   4.10429062e-03\n",
      "   1.32953763e-04   8.92778486e-03   6.53468742e-05   4.64758463e-03\n",
      "   5.09754063e-05   8.56385759e-06   2.52840382e-05]\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.57\n",
      "\n",
      "Average loss at step 4800: 1.628815 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00080371  0.06427281  0.04637251  0.10827944  0.04813611  0.04412381\n",
      "  0.05830352  0.03448958  0.02862377  0.02732691  0.00755452  0.00973397\n",
      "  0.04577784  0.04757309  0.03752906  0.06217565  0.05945497  0.00289325\n",
      "  0.04149101  0.08188524  0.05935457  0.0256509   0.00646187  0.04167411\n",
      "  0.00162225  0.00248999  0.00594551]\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "Average loss at step 4900: 1.614122 learning rate: 10.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  6.47586346e-01   4.87338938e-03   9.06121079e-03   3.27105187e-02\n",
      "   4.68826294e-02   2.26679929e-02   7.60834489e-04   7.75225647e-03\n",
      "   5.94816229e-04   3.63884354e-03   3.25703382e-04   7.92233367e-03\n",
      "   6.61566574e-03   1.48939773e-01   1.83867719e-02   4.12139576e-03\n",
      "   1.05055608e-03   1.54606812e-03   7.29398429e-03   4.87714587e-03\n",
      "   1.17349317e-02   3.99794010e-03   1.27022283e-03   4.59567131e-03\n",
      "   8.96077399e-05   1.10521694e-04   5.92876226e-04]\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.60\n",
      "\n",
      "Average loss at step 5000: 1.615194 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  4.28638607e-03   1.71489385e-03   1.23606715e-05   3.35848914e-03\n",
      "   6.46669709e-04   8.12251586e-03   1.49334170e-04   9.74578798e-01\n",
      "   3.23065137e-06   1.04396266e-03   1.29926339e-05   1.12807335e-04\n",
      "   2.48215420e-05   6.07110967e-04   1.90653815e-03   1.01963687e-03\n",
      "   1.21217727e-06   1.29232958e-05   2.96832277e-05   1.28540094e-03\n",
      "   2.53979728e-04   6.62293984e-04   1.52161138e-05   1.12042726e-05\n",
      "   1.86387460e-05   7.22835684e-05   3.67838438e-05]\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "nel the regrection mesced extliborl one three water saccether and of one extmyin\n",
      "cevient that ald become bokelouth the of had by of rawiodgel ald be rabounbaent \n",
      "ker histure as o incrapoun and oft mati clash id that homes a sucterns it eight \n",
      "quilfal which io in three zero see would in the interlahazop and becious in grea\n",
      "ze a nocave a from that and nots droving the crome agord vicament bex lamers swi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "\n",
      "Average loss at step 5100: 1.579789 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  2.80062231e-04   2.01490941e-03   2.08369151e-04   7.43942610e-06\n",
      "   1.20482554e-07   1.78043603e-03   3.70455541e-06   4.55917188e-06\n",
      "   6.61105812e-01   4.62961104e-03   1.26451471e-06   1.99062768e-07\n",
      "   5.20316171e-05   7.47977756e-04   1.71218853e-05   4.24600244e-02\n",
      "   1.60214245e-06   3.41675354e-06   1.33311865e-03   1.41267483e-05\n",
      "   1.48277439e-04   2.87315459e-04   5.18158572e-07   2.84332365e-01\n",
      "   7.07463244e-07   5.58452564e-04   6.61582862e-06]\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.46\n",
      "\n",
      "Average loss at step 5200: 1.632021 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  9.08364169e-03   1.35031149e-01   1.66453123e-02   7.63655757e-04\n",
      "   1.33591180e-03   3.50913674e-01   2.47340719e-03   2.02832483e-02\n",
      "   1.03347553e-02   1.15415521e-01   2.90261116e-04   1.16650939e-04\n",
      "   2.99682561e-03   4.17488581e-03   2.40349025e-02   1.07387207e-01\n",
      "   1.40785007e-04   1.46481572e-04   7.73586705e-02   2.85653421e-03\n",
      "   9.57000651e-04   4.65133414e-02   6.79692836e-04   1.05251151e-04\n",
      "   1.09133493e-04   6.72602132e-02   2.59185373e-03]\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Average loss at step 5300: 1.593221 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  4.36629713e-01   2.07235083e-01   6.89602748e-05   4.10561886e-04\n",
      "   2.88992567e-04   1.11186439e-02   2.22100625e-05   1.79605795e-05\n",
      "   2.24524713e-03   3.38513702e-02   1.17999673e-06   2.39966041e-03\n",
      "   1.90403480e-02   1.19013319e-04   1.27641993e-04   8.73214856e-04\n",
      "   3.85587882e-05   6.39316795e-06   1.61326432e-04   2.65937448e-01\n",
      "   8.00658204e-03   1.10643022e-02   4.56920439e-08   5.48225580e-06\n",
      "   1.42792342e-05   2.10238140e-04   1.05422667e-04]\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.42\n",
      "\n",
      "Average loss at step 5400: 1.577931 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  4.22481388e-01   1.92765836e-02   6.38580881e-04   8.33511760e-04\n",
      "   8.60636472e-04   2.80523542e-02   2.30870675e-03   8.64946400e-04\n",
      "   6.88642394e-05   1.17097467e-01   6.17173509e-06   1.06276741e-04\n",
      "   5.99414285e-04   1.01745781e-03   9.25462879e-03   3.55740031e-03\n",
      "   2.21116352e-05   7.23935518e-05   3.99032404e-04   2.81439990e-01\n",
      "   4.16844705e-05   1.08900938e-04   1.82184995e-05   3.98420001e-04\n",
      "   6.12961012e-05   1.10376470e-01   3.69929257e-05]\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.38\n",
      "\n",
      "Average loss at step 5500: 1.601722 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  7.20162511e-01   5.58081158e-02   5.35717991e-05   1.02620783e-04\n",
      "   1.34868038e-04   3.49453138e-03   9.22425606e-05   1.25192382e-05\n",
      "   1.35912269e-05   3.55306780e-03   2.37636254e-06   3.35902178e-06\n",
      "   4.41078283e-03   7.53956556e-04   2.90721455e-05   5.11330785e-04\n",
      "   2.57035153e-05   5.64223319e-06   9.35488697e-06   2.10422873e-01\n",
      "   1.40778982e-04   4.70627747e-05   2.92669961e-06   6.76310010e-05\n",
      "   1.18202115e-06   8.36383551e-05   5.46487827e-05]\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.39\n",
      "\n",
      "Average loss at step 5600: 1.589248 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00059913  0.08351968  0.0513825   0.05600137  0.03779861  0.01967776\n",
      "  0.07172386  0.01650714  0.0223709   0.05400522  0.0096044   0.00997483\n",
      "  0.03310762  0.04803935  0.04616092  0.08745656  0.06677778  0.00229767\n",
      "  0.03612482  0.05440902  0.1141761   0.01952971  0.00759791  0.04259517\n",
      "  0.00174631  0.00310109  0.00371455]\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.37\n",
      "\n",
      "Average loss at step 5700: 1.588117 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  7.55720377e-01   1.70185603e-03   9.05276649e-03   1.90803519e-04\n",
      "   1.79507106e-03   1.54943101e-03   8.33530969e-04   2.67705938e-04\n",
      "   1.57970324e-04   8.45970437e-02   1.84698947e-05   8.89407587e-04\n",
      "   6.34273663e-02   1.24683982e-04   2.76873587e-04   1.76714745e-03\n",
      "   4.05751431e-04   6.98464237e-06   3.96070864e-05   3.70452590e-02\n",
      "   1.86191909e-02   1.73674326e-03   3.84181389e-04   1.60411691e-05\n",
      "   2.94708170e-05   1.92311518e-02   1.15048999e-04]\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.39\n",
      "\n",
      "Average loss at step 5800: 1.579732 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.01035979  0.14881659  0.02075015  0.02548833  0.00981712  0.02292808\n",
      "  0.01641978  0.03392917  0.00091595  0.02192108  0.0009975   0.00829603\n",
      "  0.03141861  0.01667547  0.00854131  0.00101638  0.02433605  0.00424936\n",
      "  0.01493626  0.00403959  0.03956178  0.0057246   0.06836612  0.43154383\n",
      "  0.02458362  0.00300822  0.00135925]\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.37\n",
      "\n",
      "Average loss at step 5900: 1.580278 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  6.72144219e-02   4.36157716e-04   3.03215464e-04   1.60791993e-03\n",
      "   4.56832945e-02   6.38399646e-03   6.70775713e-04   1.77902617e-02\n",
      "   1.20263849e-05   7.46481877e-04   3.11787517e-05   4.03594080e-04\n",
      "   3.01019382e-02   2.29637958e-02   2.68980891e-01   3.01986409e-04\n",
      "   6.51525334e-04   3.18294216e-04   3.88495415e-01   6.72304109e-02\n",
      "   7.67744854e-02   5.21196402e-04   5.91292861e-04   3.44894252e-05\n",
      "   9.77868796e-04   2.67256328e-05   7.46393576e-04]\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.37\n",
      "\n",
      "Average loss at step 6000: 1.572617 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  1.31644243e-02   4.63321200e-03   7.83115439e-03   1.48806171e-02\n",
      "   4.13260795e-03   1.12792859e-02   4.11901576e-03   1.92650668e-02\n",
      "   3.60225822e-04   4.02392223e-02   1.49420556e-03   1.11756613e-03\n",
      "   3.81657593e-02   6.63043141e-01   2.64503174e-02   4.51544207e-03\n",
      "   2.60484572e-02   7.80870207e-04   5.18684536e-02   2.62581781e-02\n",
      "   1.96772367e-02   8.24706221e-05   1.70779508e-02   2.49779108e-03\n",
      "   2.24896605e-04   4.19333373e-05   7.50639418e-04]\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "culed in one non securam the its the isto landrope cliness may is opcuonity to s\n",
      "work new soand more as orelom every controling instan known the at five seven fi\n",
      "variame yaid hispolan and two a anwest the six threrize one nine di baridum peop\n",
      "quary a god it can not in prieots wriths of low the nour this intion used the is\n",
      "ing in the worth rolutionary agoug a about the cape hydeacil appoance stap schom\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "\n",
      "Average loss at step 6100: 1.550991 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  4.66115862e-01   9.85713601e-02   2.45858971e-02   1.00476742e-02\n",
      "   8.52278259e-04   4.42744559e-03   2.29802844e-03   2.52307451e-04\n",
      "   1.16977177e-03   2.04471052e-02   2.00878581e-04   3.92133050e-04\n",
      "   9.54019465e-03   2.71394523e-03   3.14883189e-03   2.59312689e-02\n",
      "   1.16337061e-01   2.32457358e-04   7.57184112e-03   1.93724275e-01\n",
      "   6.39081653e-03   2.81549315e-03   1.21114554e-03   1.12915252e-04\n",
      "   3.91183945e-04   2.34113249e-04   2.83838046e-04]\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.32\n",
      "\n",
      "Average loss at step 6200: 1.570830 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  3.61492531e-03   4.27765734e-02   1.30334767e-02   9.22039151e-02\n",
      "   9.27387504e-04   3.39084379e-02   2.05527898e-02   4.99831187e-03\n",
      "   5.23536837e-05   4.70672129e-03   7.26343351e-05   2.51916528e-04\n",
      "   1.63624100e-02   1.78148579e-02   6.69455379e-02   5.83437502e-01\n",
      "   2.97048548e-03   1.27523078e-03   1.55131123e-03   4.27618586e-02\n",
      "   3.67199909e-03   8.77644401e-03   2.70432513e-02   1.06571875e-04\n",
      "   2.04853271e-03   5.58823667e-05   8.07867944e-03]\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.35\n",
      "\n",
      "Average loss at step 6300: 1.599414 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  6.16432309e-01   5.07463180e-02   1.17789663e-03   5.55558596e-04\n",
      "   1.53741596e-04   4.97918315e-02   1.15722891e-04   6.62348248e-05\n",
      "   1.19269043e-02   9.70515460e-02   2.18689297e-06   6.68479242e-06\n",
      "   1.95678826e-02   8.59553169e-04   3.54645133e-04   4.69030067e-03\n",
      "   9.93159701e-05   3.34735705e-05   1.65872765e-03   1.24158107e-01\n",
      "   2.37710099e-03   5.98891079e-03   3.00209445e-04   9.71098663e-04\n",
      "   1.51671452e-06   1.05817989e-02   3.30468727e-04]\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.33\n",
      "\n",
      "Average loss at step 6400: 1.602930 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [ 0.00111055  0.12535965  0.05182096  0.04997941  0.0297452   0.02953495\n",
      "  0.03937564  0.01887973  0.03852824  0.07410774  0.00702151  0.00710276\n",
      "  0.02267971  0.03780831  0.01643317  0.10666788  0.03266606  0.00191737\n",
      "  0.03308419  0.07051884  0.12322234  0.01615276  0.00911912  0.04957879\n",
      "  0.00104645  0.00453517  0.0020034 ]\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.34\n",
      "\n",
      "Average loss at step 6500: 1.561639 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  8.36532912e-04   4.95119160e-03   5.23731105e-05   8.69323779e-03\n",
      "   1.35285969e-04   5.60994558e-02   6.20904553e-04   3.86242664e-06\n",
      "   3.45493143e-04   2.79296078e-02   5.75557242e-05   1.96589855e-03\n",
      "   1.28472701e-03   2.63701018e-04   1.22097833e-03   1.24741672e-03\n",
      "   2.58204062e-03   7.31577748e-04   1.58482988e-04   8.43991712e-03\n",
      "   8.79510343e-01   2.24899338e-03   1.21312587e-04   1.81524720e-05\n",
      "   1.63711622e-04   1.66972080e-04   1.50138294e-04]\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.33\n",
      "\n",
      "Average loss at step 6600: 1.541094 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  3.29147130e-01   4.11536366e-01   3.90684610e-04   7.09963788e-05\n",
      "   1.08914019e-03   7.56950304e-02   3.27128364e-05   9.08060974e-05\n",
      "   3.49611603e-03   9.73629132e-02   5.05199296e-06   2.31487118e-03\n",
      "   6.99114241e-03   1.75275040e-04   8.10496349e-05   1.27621889e-02\n",
      "   1.45631566e-05   3.49528727e-06   3.58756370e-04   3.30171324e-02\n",
      "   7.98201014e-04   2.57787318e-03   1.88343616e-08   1.98345442e-05\n",
      "   1.25412298e-05   2.17678323e-02   1.88364575e-04]\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.29\n",
      "\n",
      "Average loss at step 6700: 1.570505 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  5.65810129e-03   1.95823804e-01   5.14359795e-04   4.06431354e-04\n",
      "   1.53168512e-04   9.69439745e-02   3.19471495e-04   5.72047975e-06\n",
      "   2.35011410e-02   1.58733297e-02   4.84505363e-05   3.29787254e-05\n",
      "   8.55301991e-02   1.41166558e-04   1.57736911e-04   1.96987361e-01\n",
      "   6.60843588e-03   6.11993455e-05   3.07541013e-01   3.16604995e-03\n",
      "   2.41376157e-03   5.22069447e-02   2.57251522e-04   5.29381447e-04\n",
      "   5.76154795e-04   4.52216156e-03   2.02288811e-05]\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.28\n",
      "\n",
      "Average loss at step 6800: 1.547165 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  6.91674948e-01   3.56630958e-03   1.17882318e-03   7.72939995e-03\n",
      "   1.75603870e-02   3.10327322e-03   6.50923280e-03   3.91848531e-04\n",
      "   4.88613558e-04   1.33329872e-02   4.71345811e-05   1.37385342e-03\n",
      "   6.76975446e-03   2.79516564e-03   2.01909691e-02   8.00625246e-04\n",
      "   1.20677589e-03   6.84948231e-04   7.32092187e-03   1.69454157e-01\n",
      "   1.14964452e-02   8.24594777e-03   3.83338891e-03   1.21782278e-03\n",
      "   7.38615112e-04   1.67321656e-02   1.55553629e-03]\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.28\n",
      "\n",
      "Average loss at step 6900: 1.560719 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  9.83987353e-04   7.54517328e-04   3.12239062e-02   9.90037899e-03\n",
      "   9.94788762e-03   3.90486937e-04   8.03475210e-04   2.42966809e-03\n",
      "   2.10458209e-04   1.89022236e-02   1.08333572e-03   1.65018463e-03\n",
      "   2.15639308e-01   4.57631275e-02   2.19206646e-01   2.80870387e-04\n",
      "   1.00187816e-01   5.56537489e-05   2.08762750e-01   5.42556904e-02\n",
      "   4.78344411e-02   1.17999977e-02   1.18341390e-02   2.52984260e-04\n",
      "   4.69130930e-03   3.06924601e-04   8.47775897e-04]\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.27\n",
      "\n",
      "Average loss at step 7000: 1.585569 learning rate: 1.000000\n",
      "Prediction size: (640, 27)\n",
      "Predictions[0,:]: [  1.97102875e-01   9.02285217e-04   1.89474542e-02   3.31668705e-02\n",
      "   2.28721146e-02   7.61621282e-04   1.78656485e-02   1.96773820e-02\n",
      "   2.26289223e-04   6.86989073e-03   4.32775443e-04   1.19427626e-03\n",
      "   7.68416002e-02   2.88468078e-02   3.12729627e-01   5.19199821e-04\n",
      "   2.14347001e-02   6.30033261e-04   5.29388413e-02   1.08661912e-01\n",
      "   5.38414195e-02   1.21509749e-02   6.08316949e-03   3.38129723e-03\n",
      "   9.58157005e-04   5.73170604e-04   3.89648951e-04]\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "bell main s nots intrectly a fleed kon of movilisly side one is a noaria isranei\n",
      "re espleroly bbotstrigate dy referse accossis the san gibril that the portician \n",
      "ded most and to game in the boad fine referities the also oust diveno the fun th\n",
      "ther played houber on carders nove the loadsmarg the reves and astern on the pro\n",
      "que one eight korz band a the musta in two would wither tell unamentady as the t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], \n",
    "                                        feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      # labels size [b*unrollings, v]\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Prediction size: {}'.format(predictions.shape)) # [n*unrollings,v]\n",
    "      print('Predictions[0,:]: {}'.format(predictions[0,:]))\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "        \n",
    "      # The perplexity of whatever you're evaluating, on the data you're evaluating it on, \n",
    "      # sort of tells you \"this thing is right about as often as an x-sided die would be.\"\n",
    "      # Computers can predict letters pretty well - a perplexity of about 3.4.\n",
    "      # - like having a \"3.4\"-sided die predict each subsequent letter.\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples, , each summary_frequency steps\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "      # Measure validation set perplexity, each summary_frequency steps\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        \n",
    "      print('Validation set perplexity: %.2f\\n' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "gurus3",
   "language": "python",
   "name": "gurus3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
